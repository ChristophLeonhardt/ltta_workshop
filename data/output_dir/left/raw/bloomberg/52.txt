1
00:00:00,531 --> 00:00:03,364
(leaves rustling)

2
00:00:07,900 --> 00:00:10,160
As fall breaks out in Canada,

3
00:00:10,160 --> 00:00:14,589
I'm reminded of all the beauty,
innocence and gun-free fun

4
00:00:14,589 --> 00:00:16,933
available from our neighbors to the North.

5
00:00:18,849 --> 00:00:21,599
(majestic music)

6
00:00:26,324 --> 00:00:28,907
There's the majesty of Toronto,

7
00:00:30,670 --> 00:00:31,823
vast hockey rinks,

8
00:00:34,690 --> 00:00:37,103
spectacular batches of poutine,

9
00:00:38,520 --> 00:00:41,950
and gallons of maple syrup
that you can chug openly

10
00:00:41,950 --> 00:00:46,593
and guilt-free for this maple
syrup is pure and nourishing.

11
00:00:51,840 --> 00:00:54,240
The changing of the
seasons also happens to be

12
00:00:54,240 --> 00:00:57,060
the perfect time to
encounter one of Canada's

13
00:00:57,060 --> 00:00:58,733
most prized creatures,

14
00:01:00,470 --> 00:01:02,723
the artificial intelligence nerd.

15
00:01:04,013 --> 00:01:06,764
(resolute music)

16
00:01:09,510 --> 00:01:12,270
Not too long ago, these beings were rare

17
00:01:12,270 --> 00:01:15,840
and hidden away in university dungeons.

18
00:01:15,840 --> 00:01:17,873
But today they flourish.

19
00:01:20,180 --> 00:01:22,563
They primp with instinctual grace.

20
00:01:24,090 --> 00:01:26,270
They wave their hands impressively

21
00:01:26,270 --> 00:01:28,663
to assert their intellectual dominance.

22
00:01:29,930 --> 00:01:33,680
They carb-load like overpaid
professional athletes.

23
00:01:33,680 --> 00:01:36,340
And this makes some sense
because they're among

24
00:01:36,340 --> 00:01:38,803
the best paid professionals in the world.

25
00:01:43,600 --> 00:01:47,260
Together these creatures did
something truly remarkable.

26
00:01:47,260 --> 00:01:49,350
Without anyone paying much notice,

27
00:01:49,350 --> 00:01:52,740
they gave birth to an AI revolution.

28
00:01:52,740 --> 00:01:55,770
They turned Canada, yes, Canada,

29
00:01:55,770 --> 00:01:58,573
into one of the great AI superpowers.

30
00:02:01,520 --> 00:02:03,853
This is the story of
how all this came to be.

31
00:02:04,790 --> 00:02:07,150
It's the story of one nation's quest

32
00:02:07,150 --> 00:02:09,373
to teach computers to think like humans.

33
00:02:10,300 --> 00:02:13,220
It's the story of what this
science experiment will mean

34
00:02:13,220 --> 00:02:18,220
for all our lives and for the
future of the human species.

35
00:02:21,250 --> 00:02:24,783
So if you're a human, or
something trying to imitate one,

36
00:02:25,782 --> 00:02:28,032
you'll wanna pay attention.

37
00:02:44,773 --> 00:02:48,570
Ever since people first came
up with the idea of computers,

38
00:02:48,570 --> 00:02:50,060
they've dreamed of imbuing them

39
00:02:50,060 --> 00:02:51,813
with artificial intelligence.

40
00:02:52,740 --> 00:02:56,090
I am a smart fellow

41
00:02:56,090 --> 00:03:01,050
as I have a very fine brain.

42
00:03:01,050 --> 00:03:03,743
That's the most remarkable
thing I've ever seen.

43
00:03:04,880 --> 00:03:08,530
AI is just a computer
that is able to mimic

44
00:03:08,530 --> 00:03:12,750
or simulate human thought
or human behavior.

45
00:03:12,750 --> 00:03:15,403
Within that there's a subset
called machine learning

46
00:03:15,403 --> 00:03:18,480
that it's now the underpinning

47
00:03:18,480 --> 00:03:20,913
of what is most exciting about AI.

48
00:03:23,340 --> 00:03:25,110
By allowing computers to learn

49
00:03:25,110 --> 00:03:27,110
how to solve problems on their own,

50
00:03:27,110 --> 00:03:29,810
machine learning has made
a series of breakthroughs

51
00:03:29,810 --> 00:03:31,823
that once seemed nearly impossible.

52
00:03:32,670 --> 00:03:36,080
It's the reason computers
can understand your voice,

53
00:03:36,080 --> 00:03:39,223
spot a friend's face in
a photo, and steer a car.

54
00:03:40,550 --> 00:03:42,880
And it's the reason people
are actively talking

55
00:03:42,880 --> 00:03:46,030
about the arrival of human-like AI.

56
00:03:46,030 --> 00:03:48,280
And whether that would be a good thing

57
00:03:48,280 --> 00:03:50,303
or a horrific end of days thing.

58
00:03:53,690 --> 00:03:56,140
Many people made this moment possible,

59
00:03:56,140 --> 00:03:58,683
but one figure towers above the rest.

60
00:03:59,840 --> 00:04:02,170
I've come to the University of Toronto

61
00:04:02,170 --> 00:04:04,400
to see the man they call the godfather

62
00:04:04,400 --> 00:04:06,823
of Modern Artificial Intelligence.

63
00:04:07,760 --> 00:04:08,593
Geoff Hinton.

64
00:04:11,030 --> 00:04:13,447
(calm music)

65
00:04:18,100 --> 00:04:21,170
Because of a back condition,
Geoff Hinton hasn't been able

66
00:04:21,170 --> 00:04:23,573
to sit down for more than 12 years.

67
00:04:24,940 --> 00:04:25,773
I hate standing.

68
00:04:25,773 --> 00:04:27,260
I much rather sit down, but if I sit down

69
00:04:27,260 --> 00:04:28,710
I have a disc that comes out.

70
00:04:29,860 --> 00:04:32,350
Well at least now standing
desks are fashionable.

71
00:04:32,350 --> 00:04:33,870
Yeah, but I was ahead.

72
00:04:33,870 --> 00:04:34,703
(laughter)

73
00:04:34,703 --> 00:04:36,953
I was standing when they
weren't fashionable.

74
00:04:38,510 --> 00:04:41,460
Since he can't sit in a car or on a bus,

75
00:04:41,460 --> 00:04:44,153
Hinton walks everywhere.

76
00:04:50,330 --> 00:04:53,890
The walk says a lot about
Hinton and his resolve.

77
00:04:53,890 --> 00:04:56,357
For nearly 40 years, Hinton has
been trying to get computers

78
00:04:56,357 --> 00:05:00,510
to learn like people do,
a quest almost everyone

79
00:05:00,510 --> 00:05:03,180
thought was crazy or at least hopeless,

80
00:05:03,180 --> 00:05:06,063
right up until the moment
it revolutionized the field.

81
00:05:07,600 --> 00:05:09,650
Google thinks this is the
future of the company,

82
00:05:09,650 --> 00:05:11,440
Amazon thinks this is the
future of the company,

83
00:05:11,440 --> 00:05:13,280
Apple thinks this is the
future of the company,

84
00:05:13,280 --> 00:05:15,690
my own department thinks
it's just probably nonsense

85
00:05:15,690 --> 00:05:17,465
and we shouldn't be doing any more of it.

86
00:05:17,465 --> 00:05:19,890
(laughter)

87
00:05:19,890 --> 00:05:22,640
So I talked everybody into
it except my own department.

88
00:05:26,240 --> 00:05:28,240
You obviously grew up in the UK

89
00:05:28,240 --> 00:05:30,370
and you had this very prestigious family

90
00:05:30,370 --> 00:05:33,590
full of famous
mathematicians and economist,

91
00:05:33,590 --> 00:05:36,450
and I was curious what
it was like for you.

92
00:05:36,450 --> 00:05:38,250
Yeah, there was a lot of pressure.

93
00:05:39,100 --> 00:05:41,220
I think by the time I was about seven

94
00:05:41,220 --> 00:05:43,270
I realized I was gonna have to get a PhD.

95
00:05:45,080 --> 00:05:47,260
Did you rebel against that or you--

96
00:05:47,260 --> 00:05:49,200
I dropped out every so often.

97
00:05:49,200 --> 00:05:50,850
I became a carpenter for a while.

98
00:05:53,120 --> 00:05:55,820
Geoff Hinton, pretty
early on, became obsessed

99
00:05:55,820 --> 00:05:59,793
with this idea of figuring
out how the mind works.

100
00:06:01,881 --> 00:06:03,970
He started off getting into physiology,

101
00:06:03,970 --> 00:06:05,940
the anatomy of how the brain works,

102
00:06:05,940 --> 00:06:10,070
then he got into psychology,
and then finally he settled

103
00:06:10,070 --> 00:06:12,210
on more of a computer science approach

104
00:06:12,210 --> 00:06:16,183
to modeling the brain and got
in to artificial intelligence.

105
00:06:17,420 --> 00:06:20,030
My feeling is if you wanna understand

106
00:06:20,030 --> 00:06:22,710
a really complicated device, like a brain,

107
00:06:22,710 --> 00:06:24,610
you should build one.

108
00:06:24,610 --> 00:06:26,330
I mean you could look at
cars and you could think

109
00:06:26,330 --> 00:06:27,502
you could understand cars.

110
00:06:27,502 --> 00:06:30,122
When you try and build a
car you suddenly discover

111
00:06:30,122 --> 00:06:31,720
this is stuff that has
to go under the hood,

112
00:06:31,720 --> 00:06:33,470
otherwise it doesn't work.
Yeah.

113
00:06:35,010 --> 00:06:37,680
As Geoff was starting to
think about these ideas,

114
00:06:37,680 --> 00:06:41,203
he got inspired by some AI
researchers across the pond.

115
00:06:42,120 --> 00:06:45,283
Specifically this guy, Frank Rosenblatt.

116
00:06:46,240 --> 00:06:49,340
Rosenblatt, in the late 1950s,

117
00:06:49,340 --> 00:06:51,850
developed what he called a Perceptron,

118
00:06:51,850 --> 00:06:55,730
and it was a neural
network, a computing system

119
00:06:55,730 --> 00:06:57,733
that would mimic the brain.

120
00:06:59,430 --> 00:07:02,860
The basic idea is a
collection of small units

121
00:07:02,860 --> 00:07:05,500
called neurons, these are
little computing units

122
00:07:05,500 --> 00:07:07,610
but they're actually modeled on the way

123
00:07:07,610 --> 00:07:09,883
that the human brain does its computation.

124
00:07:10,840 --> 00:07:14,120
They take incoming data
like we do from our senses

125
00:07:14,120 --> 00:07:16,970
and they actually learn so
the neural net can learn

126
00:07:16,970 --> 00:07:18,603
to make decisions over time.

127
00:07:22,100 --> 00:07:24,250
Rosenblatt's hope was that you
could feed a neural network

128
00:07:24,250 --> 00:07:27,550
a bunch of data like
pictures of men and women

129
00:07:27,550 --> 00:07:30,670
and it would eventually
learn how to tell them apart,

130
00:07:30,670 --> 00:07:31,813
just like humans do.

131
00:07:34,980 --> 00:07:37,000
There was just one problem.

132
00:07:37,000 --> 00:07:38,253
It didn't work very well.

133
00:07:39,380 --> 00:07:41,367
Rosenblatt, his neural network

134
00:07:41,367 --> 00:07:44,920
was a single layer of neurons

135
00:07:44,920 --> 00:07:49,130
and it was limiting what it
could do, extremely limited.

136
00:07:49,130 --> 00:07:53,350
And a colleague of his wrote
a book in the late '60s

137
00:07:53,350 --> 00:07:55,313
that show these limitations.

138
00:07:57,680 --> 00:08:00,680
And it kinda put the
whole area of research

139
00:08:00,680 --> 00:08:03,900
into a deep freeze for a good 10 years.

140
00:08:03,900 --> 00:08:05,580
No one wanted to work in this area.

141
00:08:05,580 --> 00:08:07,330
They were sure it would never work.

142
00:08:08,300 --> 00:08:10,433
Well, almost no one.

143
00:08:11,440 --> 00:08:14,450
It was just obvious to me that
it was the right way to go.

144
00:08:14,450 --> 00:08:16,330
The brain's a big neural network

145
00:08:16,330 --> 00:08:19,480
and so it has to be that
stuff like this can work

146
00:08:19,480 --> 00:08:21,460
'cause it works in our brains.

147
00:08:21,460 --> 00:08:23,240
There's just never any doubt about that.

148
00:08:23,240 --> 00:08:25,470
What do you think it was inside of you

149
00:08:25,470 --> 00:08:28,100
that kept you wanting to pursue this

150
00:08:28,100 --> 00:08:29,170
when everyone else was giving up,

151
00:08:29,170 --> 00:08:31,627
just that you thought it was
the right direction to go?

152
00:08:31,627 --> 00:08:33,169
I know that everyone else was wrong.

153
00:08:33,169 --> 00:08:34,002
Okay.

154
00:08:37,950 --> 00:08:40,400
Hinton decides he's got an idea

155
00:08:40,400 --> 00:08:42,480
of how these neural nets might work,

156
00:08:42,480 --> 00:08:44,543
and he's gonna pursue it no matter what.

157
00:08:46,110 --> 00:08:48,240
For a little while, he's bouncing around

158
00:08:48,240 --> 00:08:51,050
research institutions in the US.

159
00:08:51,050 --> 00:08:53,650
He kinda gets fed up that
most of them are funded

160
00:08:53,650 --> 00:08:56,050
by the defense departments
and he starts looking

161
00:08:56,050 --> 00:08:58,390
for somewhere else he can go.

162
00:08:58,390 --> 00:09:01,100
I didn't wanna take
defense department money.

163
00:09:01,100 --> 00:09:02,570
I sort of didn't like the idea

164
00:09:02,570 --> 00:09:04,310
that this stuff was gonna be used

165
00:09:04,310 --> 00:09:06,737
for purposes that I
didn't think were good.

166
00:09:07,650 --> 00:09:09,950
He suddenly hears that
Canada might be interested

167
00:09:09,950 --> 00:09:12,890
in funding artificial intelligence.

168
00:09:12,890 --> 00:09:14,580
And that was very attractive,

169
00:09:14,580 --> 00:09:16,850
that I could go off to this civilized town

170
00:09:16,850 --> 00:09:18,580
and just get on with it.

171
00:09:18,580 --> 00:09:20,523
So I came to the University of Toronto.

172
00:09:21,710 --> 00:09:23,300
And then in the mid '80s, we discovered

173
00:09:23,300 --> 00:09:25,040
I had to make more complicated neural nets

174
00:09:25,040 --> 00:09:26,810
so they could solve those problems

175
00:09:26,810 --> 00:09:29,360
that the simple ones couldn't solve.

176
00:09:29,360 --> 00:09:31,650
He and his collaborators developed

177
00:09:31,650 --> 00:09:35,343
a multi-layered neural
network, a deep neural network.

178
00:09:36,430 --> 00:09:39,223
And this started to work in a lot of ways.

179
00:09:40,400 --> 00:09:43,460
Using a neural network, a
guy named Dean Pomerleau

180
00:09:43,460 --> 00:09:46,260
built a self-driving car in the late '80s,

181
00:09:46,260 --> 00:09:48,750
and it drove on public roads.

182
00:09:48,750 --> 00:09:51,610
Yann LeCun, in the '90s, built a system

183
00:09:51,610 --> 00:09:54,150
that could recognize handwritten digits

184
00:09:54,150 --> 00:09:56,593
and this ended up being used commercially.

185
00:09:57,560 --> 00:09:59,093
But again they hit a ceiling.

186
00:10:02,730 --> 00:10:04,210
They didn't work quite well enough

187
00:10:04,210 --> 00:10:05,450
because we didn't have enough data,

188
00:10:05,450 --> 00:10:07,660
we didn't have enough compute power.

189
00:10:07,660 --> 00:10:11,220
And people in AI, in computer science,

190
00:10:11,220 --> 00:10:14,700
decided neural networks was
wishful thinking basically.

191
00:10:14,700 --> 00:10:16,293
So it was a big disappointment.

192
00:10:18,960 --> 00:10:21,780
Through the '90s into the 2000s,

193
00:10:21,780 --> 00:10:25,070
Geoff was one of only a
handful of people on the planet

194
00:10:25,070 --> 00:10:27,143
who are still pursuing this technology.

195
00:10:29,130 --> 00:10:31,330
He would show up at academic conferences

196
00:10:31,330 --> 00:10:33,560
and being banished to the back rooms.

197
00:10:33,560 --> 00:10:36,263
He was treated as really like a pariah.

198
00:10:37,740 --> 00:10:39,910
Was there like a time when you thought,

199
00:10:39,910 --> 00:10:40,960
this just wasn't gonna work?
No.

200
00:10:40,960 --> 00:10:43,460
And you did have some self-doubt?

201
00:10:43,460 --> 00:10:45,660
I mean there were many
times when I thought,

202
00:10:45,660 --> 00:10:47,110
I'm not gonna make this work.

203
00:10:49,060 --> 00:10:51,683
But Geoff was consumed by
this and couldn't stop.

204
00:10:52,890 --> 00:10:56,970
He just kept pursuing the idea
that computers could learn.

205
00:10:56,970 --> 00:11:00,660
Until about 2006, when
the world catches up

206
00:11:00,660 --> 00:11:02,193
to Hinton's ideas.

207
00:11:07,120 --> 00:11:09,480
Computers were now a lot faster.

208
00:11:09,480 --> 00:11:10,900
And now it's behaving like I thought

209
00:11:10,900 --> 00:11:12,560
it would behave in the mid '80s.

210
00:11:12,560 --> 00:11:14,410
It's solving everything.

211
00:11:14,410 --> 00:11:16,560
The arrival of superfast chips

212
00:11:16,560 --> 00:11:19,690
and the massive amounts of
data produced on the internet

213
00:11:19,690 --> 00:11:22,603
gave Hinton's algorithms a magical boost.

214
00:11:24,220 --> 00:11:28,210
Suddenly computers could
identify what was in an image,

215
00:11:28,210 --> 00:11:30,750
then they could recognize speech

216
00:11:30,750 --> 00:11:33,153
and translate from one
language to another.

217
00:11:34,180 --> 00:11:37,940
By 2012, words like neural
nets and machine learning

218
00:11:37,940 --> 00:11:41,610
were popping up on the front
page of The New York Times.

219
00:11:41,610 --> 00:11:43,140
You have to go all these years

220
00:11:43,140 --> 00:11:47,290
and then all of a sudden,
in the span of a few months,

221
00:11:47,290 --> 00:11:49,950
it just takes off and it
finally feel like, aha,

222
00:11:49,950 --> 00:11:52,950
the world has finally come to my vision.

223
00:11:52,950 --> 00:11:54,600
It's sort of a relief that people

224
00:11:54,600 --> 00:11:55,904
finally came to their senses.

225
00:11:55,904 --> 00:11:58,154
(laughter)

226
00:12:00,270 --> 00:12:02,570
Next up, we have Professor Geoffrey Hinton

227
00:12:02,570 --> 00:12:03,914
of the University of Toronto.

228
00:12:03,914 --> 00:12:06,164
(applause)

229
00:12:10,091 --> 00:12:11,770
Thank you.

230
00:12:11,770 --> 00:12:14,187
(calm music)

231
00:12:16,794 --> 00:12:20,440
For Hinton, this is obviously
a really redemptive moment.

232
00:12:20,440 --> 00:12:23,243
Now he's basically a technology celebrity.

233
00:12:24,240 --> 00:12:26,933
And for Canada, it's the
country's moment as well.

234
00:12:27,950 --> 00:12:29,940
They have more AI researchers

235
00:12:29,940 --> 00:12:32,500
than just about any
other place on the planet

236
00:12:32,500 --> 00:12:35,250
and the quest now is to
see what these guys can do,

237
00:12:35,250 --> 00:12:37,950
starting companies and pushing
the technology forward.

238
00:12:39,480 --> 00:12:41,910
I'm gonna set out on a
journey across Canada

239
00:12:41,910 --> 00:12:45,461
to see the best in Canadian AI technology

240
00:12:45,461 --> 00:12:48,960
and to get a feel for how
far the technology has come

241
00:12:48,960 --> 00:12:50,693
and how far it still has to go.

242
00:13:03,061 --> 00:13:05,510
Here is a city that gets
right at the central tension

243
00:13:05,510 --> 00:13:09,621
of modern life and the
unfolding AI revolution.

244
00:13:09,621 --> 00:13:12,760
(church bell ringing)

245
00:13:12,760 --> 00:13:16,080
It's Montreal, a place filled with beauty

246
00:13:16,080 --> 00:13:19,280
and old world charms that ask you to move

247
00:13:19,280 --> 00:13:24,280
slowly through its streets
and to chill for a while,

248
00:13:24,530 --> 00:13:27,213
reflect, and think deep thoughts.

249
00:13:30,858 --> 00:13:33,275
(calm music)

250
00:13:34,862 --> 00:13:36,680
At the same time, it's one of the world's

251
00:13:36,680 --> 00:13:39,390
top AI research centers.

252
00:13:39,390 --> 00:13:42,040
Students flock here
from all over the globe

253
00:13:42,040 --> 00:13:44,150
to get deep with machine learning

254
00:13:44,150 --> 00:13:47,150
and to take Geoff Hinton's
ideas and figure out

255
00:13:47,150 --> 00:13:49,653
how to turn them into products we all use.

256
00:13:52,860 --> 00:13:55,470
To see just how successful they've been,

257
00:13:55,470 --> 00:13:57,183
look no further than your pocket.

258
00:13:58,540 --> 00:14:01,620
All this stuff started out
as hardcore computer science,

259
00:14:01,620 --> 00:14:04,780
but over the last five
years AI has invaded

260
00:14:04,780 --> 00:14:06,170
our everyday lives.

261
00:14:06,170 --> 00:14:10,030
Your smartphone is packed
full of AI-powered apps

262
00:14:10,030 --> 00:14:11,900
including something like Google Translate

263
00:14:11,900 --> 00:14:14,220
that lets you point
your phone at a magazine

264
00:14:14,220 --> 00:14:17,333
that's written in French and
read it as if you're a local.

265
00:14:18,900 --> 00:14:20,760
Engineers have been
trying to get computers

266
00:14:20,760 --> 00:14:23,490
to translate text like this for decades,

267
00:14:23,490 --> 00:14:24,980
but it was Geoff's neural nets

268
00:14:24,980 --> 00:14:26,623
that finally made it possible.

269
00:14:27,700 --> 00:14:28,603
Thanks, Geoff.

270
00:14:30,129 --> 00:14:32,340
And it's not just your smartphone,

271
00:14:32,340 --> 00:14:35,620
neural networks are
heading for the open road.

272
00:14:35,620 --> 00:14:36,453
Off we go.

273
00:14:43,860 --> 00:14:45,990
Meet my friend Stephane, the head

274
00:14:45,990 --> 00:14:47,853
of Montreal's Tesla Fan Club.

275
00:14:49,720 --> 00:14:51,970
I'm driving a Tesla for a little bit

276
00:14:51,970 --> 00:14:53,899
more than four years and a half.

277
00:14:53,899 --> 00:14:56,070
So do you have people asking
you for rides all the time?

278
00:14:56,070 --> 00:14:58,410
Yes, all the time.

279
00:14:58,410 --> 00:14:59,640
Maybe that's because of his

280
00:14:59,640 --> 00:15:01,880
fancy pants autopilot,

281
00:15:01,880 --> 00:15:04,520
Tesla's semi-autonomous driving system

282
00:15:04,520 --> 00:15:06,863
that kicks in when road
conditions are right.

283
00:15:08,330 --> 00:15:10,350
So that's it, autopilot's on.

284
00:15:10,350 --> 00:15:13,870
Yes, and it's driving by itself.

285
00:15:13,870 --> 00:15:15,220
So we need to pay attention

286
00:15:16,540 --> 00:15:18,917
but we don't have to drive.

287
00:15:18,917 --> 00:15:20,153
That's crazy.

288
00:15:22,415 --> 00:15:24,665
(laughter)

289
00:15:26,740 --> 00:15:29,010
Self-driving cars are
packed full of camera,

290
00:15:29,010 --> 00:15:30,700
sensors and radar.

291
00:15:30,700 --> 00:15:33,240
When teamed with computer
vision neural nets,

292
00:15:33,240 --> 00:15:35,310
it's this technology that lets the cars

293
00:15:35,310 --> 00:15:36,760
build a picture of the world.

294
00:15:38,780 --> 00:15:41,160
The technology has a long way to go,

295
00:15:41,160 --> 00:15:44,370
but this Tesla can monitor
all the cars around it,

296
00:15:44,370 --> 00:15:47,363
switch lanes and park all by itself.

297
00:15:48,380 --> 00:15:49,213
Thanks, Geoff.

298
00:15:50,170 --> 00:15:51,508
So you're living in the future.

299
00:15:51,508 --> 00:15:52,341
Yeah.

300
00:15:52,341 --> 00:15:54,350
You know, when you try it once

301
00:15:54,350 --> 00:15:56,960
it's very difficult to do without it

302
00:15:56,960 --> 00:16:01,960
because I just can be relaxed
and we can drive like this.

303
00:16:02,450 --> 00:16:03,770
Oops.

304
00:16:03,770 --> 00:16:04,980
There's a stop sign.

305
00:16:04,980 --> 00:16:06,926
That's why we still need to pay attention.

306
00:16:06,926 --> 00:16:09,176
(laughter)

307
00:16:12,650 --> 00:16:16,180
Back on the sidewalk, I tap
those neural nets again.

308
00:16:16,180 --> 00:16:19,090
This time in the form
of speech recognition.

309
00:16:19,090 --> 00:16:21,190
Find me some poutine, eh.

310
00:16:21,190 --> 00:16:24,123
I found a few places
within 8.4 kilometers.

311
00:16:25,020 --> 00:16:27,070
Speech recognition used to suck,

312
00:16:27,070 --> 00:16:29,740
but now it's pretty darn good.

313
00:16:29,740 --> 00:16:30,743
Why?

314
00:16:30,743 --> 00:16:32,860
A neural net of course.

315
00:16:32,860 --> 00:16:33,693
Thanks, Geoff.

316
00:16:38,760 --> 00:16:40,980
The Google brain sent me here.

317
00:16:40,980 --> 00:16:43,833
For an artery hardening
affair with poutine.

318
00:16:45,280 --> 00:16:47,650
Once a simple Quebec dish of cheese curds,

319
00:16:47,650 --> 00:16:51,553
French fries and gravy,
it's been disrupted.

320
00:16:52,460 --> 00:16:55,910
The dandan, pepperoni, bacon and onions.

321
00:16:57,354 --> 00:16:58,187
Here we go.

322
00:16:59,830 --> 00:17:04,609
It's gooey, glorious and
blessedly algorithm-free.

323
00:17:04,609 --> 00:17:06,779
Well now the humans are toast

324
00:17:06,780 --> 00:17:08,767
when they could make stuff like this.

325
00:17:11,634 --> 00:17:14,134
(calm music)

326
00:17:19,339 --> 00:17:22,009
A big part of Hinton's legacy lies beyond

327
00:17:22,010 --> 00:17:24,153
these examples of AI in the world.

328
00:17:27,069 --> 00:17:29,910
He's also inspired a legion of disciples

329
00:17:29,910 --> 00:17:32,383
spreading the good word of neural nets.

330
00:17:37,903 --> 00:17:42,320
Yoshua Bengio is a professor
at the University of Montreal,

331
00:17:42,320 --> 00:17:45,750
he's one of the researchers who
gloomed on to Hinton's ideas

332
00:17:45,750 --> 00:17:48,283
when it seemed to make
little sense to do so.

333
00:17:50,610 --> 00:17:53,810
Over the years, he's formed
a mind meld with Hinton,

334
00:17:53,810 --> 00:17:57,030
and together they've come up
with many of the key concepts

335
00:17:57,030 --> 00:17:58,243
behind modern AI.

336
00:17:59,883 --> 00:18:01,575
You guys worked on this
stuff through the '80s,

337
00:18:01,575 --> 00:18:05,180
the '90s, to 2000s and
then it just seemed like

338
00:18:05,180 --> 00:18:09,370
this totally went from
computer science and research

339
00:18:09,370 --> 00:18:11,990
to we see it everywhere in our lives.

340
00:18:11,990 --> 00:18:14,883
Are even you surprised what's happened

341
00:18:14,883 --> 00:18:16,930
in the last five years that it really is

342
00:18:16,930 --> 00:18:18,530
like sitting on all our phones and--

343
00:18:18,530 --> 00:18:22,880
The rate at which the progress
and the industrial products

344
00:18:22,880 --> 00:18:26,123
have been coming up is totally
something we didn't expect.

345
00:18:29,400 --> 00:18:31,140
Even now it's hard to predict,

346
00:18:31,140 --> 00:18:33,793
where are we going, is it gonna slow down,

347
00:18:36,370 --> 00:18:39,343
or are we gonna continue with
this exponential increase.

348
00:18:43,030 --> 00:18:45,690
It's thanks to Yoshua
that Montreal is full

349
00:18:45,690 --> 00:18:48,133
of top notch AI graduate talent.

350
00:18:49,160 --> 00:18:50,960
This in turn has brought tech giants

351
00:18:50,960 --> 00:18:53,310
like Google and Facebook to town,

352
00:18:53,310 --> 00:18:55,413
along with their ample checkbooks.

353
00:18:59,600 --> 00:19:02,790
To me, it seems like if you're good at AI

354
00:19:02,790 --> 00:19:06,365
you can make $200,000, $300,000 a year.

355
00:19:06,365 --> 00:19:09,490
It is crazy to see how much
these guys get paid now.

356
00:19:09,490 --> 00:19:13,540
A million dollars is something
quite common as a salary.

357
00:19:13,540 --> 00:19:15,230
Have you ever had a country offer you

358
00:19:15,230 --> 00:19:19,200
an incredible money to
come set up a lab there?

359
00:19:19,200 --> 00:19:21,350
Not a country but, yeah, companies, yeah.

360
00:19:25,624 --> 00:19:28,520
But Yoshua has rejected
the lucrative offers

361
00:19:28,520 --> 00:19:30,380
of big neural net.

362
00:19:30,380 --> 00:19:33,730
He remains committed to the
ivory towers of academia,

363
00:19:33,730 --> 00:19:38,020
which is a better fit for his
philosophical approach to AI.

364
00:19:38,020 --> 00:19:41,140
You've got guys like Elon
Musk and Stephen Hawking

365
00:19:41,140 --> 00:19:42,870
that sometimes paint this technology

366
00:19:42,870 --> 00:19:45,780
in a very, very dark light
that it could run amok

367
00:19:45,780 --> 00:19:47,810
and start doing things on its own.

368
00:19:47,810 --> 00:19:51,570
What do you feel when you hear
people say things like that?

369
00:19:51,570 --> 00:19:54,340
I'm not concerned about
technology running amok.

370
00:19:54,340 --> 00:19:57,130
The Terminator scenario I
think is not very credible.

371
00:19:57,130 --> 00:20:00,630
And I also believe that if
we're able to build machines

372
00:20:00,630 --> 00:20:04,810
that are as smart as us,
they're also smart enough

373
00:20:04,810 --> 00:20:07,810
to understand our values and
to understand our moral system

374
00:20:07,810 --> 00:20:11,240
and so act in a way that's good for us.

375
00:20:11,240 --> 00:20:13,820
Now I think there are real concerns

376
00:20:13,820 --> 00:20:15,800
which is essentially misuse of AI

377
00:20:15,800 --> 00:20:16,970
to influence people's minds.

378
00:20:16,970 --> 00:20:18,730
It's already happening
with political advertising.

379
00:20:18,730 --> 00:20:21,150
Yeah, we've already seen
like the stuff from Facebook.

380
00:20:21,150 --> 00:20:23,400
So I think we should
be careful about this.

381
00:20:25,430 --> 00:20:27,620
And try to regulate the use of AI

382
00:20:27,620 --> 00:20:30,660
in places where it's morally
wrong or ethically wrong,

383
00:20:30,660 --> 00:20:33,743
I think we just, we should just
ban it and make it illegal.

384
00:20:37,520 --> 00:20:40,203
It's comforting that
Yoshua has these concerns.

385
00:20:41,760 --> 00:20:44,290
But hop down the road from the university,

386
00:20:44,290 --> 00:20:47,723
in reality, or what's left
of it, becomes messier.

387
00:20:50,180 --> 00:20:55,090
This tiny room is the home
to a startup called Lyrebird.

388
00:20:55,090 --> 00:20:57,960
It was founded by Yoshua's former students

389
00:20:57,960 --> 00:21:00,983
and has built an app that
can clone your voice.

390
00:21:01,830 --> 00:21:03,970
We were speaking about this new algorithm

391
00:21:03,970 --> 00:21:05,410
to copy voices.

392
00:21:05,410 --> 00:21:06,243
This is huge.

393
00:21:06,243 --> 00:21:08,433
It can make or say
anything, really anything.

394
00:21:09,810 --> 00:21:11,850
One of its founders is this guy,

395
00:21:11,850 --> 00:21:13,443
Mexican expat Jose.

396
00:21:15,180 --> 00:21:18,070
He taught me the art of the clone.

397
00:21:18,070 --> 00:21:19,920
So you'll need to record yourself

398
00:21:19,920 --> 00:21:21,593
for a few minutes of audio.

399
00:21:24,380 --> 00:21:26,320
Thousands of letters danced across

400
00:21:26,320 --> 00:21:28,670
the amateur author screen.

401
00:21:28,670 --> 00:21:32,230
When you start to eat like
this, something is the matter.

402
00:21:32,230 --> 00:21:35,483
You guys better quit
politics and take in washing.

403
00:21:37,420 --> 00:21:39,770
I don't know where that
one came from (laughs).

404
00:21:40,810 --> 00:21:44,203
Okay, so create my digital voice now.

405
00:21:45,120 --> 00:21:46,480
Creating your digital voice.

406
00:21:46,480 --> 00:21:47,580
Takes at least one minute.

407
00:21:47,580 --> 00:21:49,700
One minute, my God.

408
00:21:49,700 --> 00:21:54,150
Yeah, so before to create some
artificial voice of someone

409
00:21:54,150 --> 00:21:58,130
you would need to record yourself
for at least eight hours.

410
00:21:58,130 --> 00:21:58,963
Test your voice.

411
00:21:58,963 --> 00:22:00,630
All right, so now I get to type something.

412
00:22:00,630 --> 00:22:02,768
Yeah, so the moment of the truth.

413
00:22:02,768 --> 00:22:03,601
Okay.

414
00:22:06,410 --> 00:22:09,200
Once Lyrebird's AI has worked its magic,

415
00:22:09,200 --> 00:22:11,840
after I'm done typing.

416
00:22:11,840 --> 00:22:13,610
Better spell that out.

417
00:22:13,610 --> 00:22:15,990
Any words I put into the
app can be played back

418
00:22:15,990 --> 00:22:17,880
in my digital voice.

419
00:22:17,880 --> 00:22:19,930
And here's the crazy thing.

420
00:22:19,930 --> 00:22:23,400
Even words I never actually
said in the first place.

421
00:22:23,400 --> 00:22:25,320
Artificial intelligence technology

422
00:22:25,320 --> 00:22:27,810
seems to be advancing very quickly.

423
00:22:27,810 --> 00:22:28,973
Should we be afraid?

424
00:22:29,850 --> 00:22:32,570
I mean I can definitely
hear my voice in there.

425
00:22:32,570 --> 00:22:35,030
That's really interesting.

426
00:22:35,030 --> 00:22:36,760
I just picked those words at random

427
00:22:36,760 --> 00:22:38,910
and I definitely did not say some of them

428
00:22:38,910 --> 00:22:42,210
and it's like flawless and
being able to sort of pick

429
00:22:42,210 --> 00:22:44,633
from just about any
word and manufacture it.

430
00:22:48,030 --> 00:22:48,863
Hello, world.

431
00:22:48,863 --> 00:22:50,833
This is the best show I have ever seen.

432
00:22:53,790 --> 00:22:55,653
This technology seems sweet,

433
00:22:57,870 --> 00:23:01,003
but lends itself to
all manner of trickery.

434
00:23:05,310 --> 00:23:07,710
I've popped back to my hotel to test out

435
00:23:07,710 --> 00:23:10,080
the Lyrebird technology a little bit.

436
00:23:10,080 --> 00:23:12,550
And you could see some really obvious ways

437
00:23:12,550 --> 00:23:14,330
that this could be abused.

438
00:23:14,330 --> 00:23:17,980
This is fake Donald Trump talking.

439
00:23:17,980 --> 00:23:19,990
The United States is considering,

440
00:23:19,990 --> 00:23:21,430
in addition to other options,

441
00:23:21,430 --> 00:23:23,020
stopping all trade with any country

442
00:23:23,020 --> 00:23:25,360
doing business with North Korea.

443
00:23:25,360 --> 00:23:29,420
And then you could picture
somebody taking over your voice

444
00:23:29,420 --> 00:23:32,570
and creating some mayhem
in your personal life.

445
00:23:32,570 --> 00:23:35,740
Now to really put my
computer voice to the test,

446
00:23:35,740 --> 00:23:38,690
I'm going to call my dear, sweet mother

447
00:23:38,690 --> 00:23:40,373
and see if she recognizes me.

448
00:23:42,351 --> 00:23:45,018
(phone ringing)

449
00:23:48,190 --> 00:23:49,130
Hey, mom.
Hi.

450
00:23:49,130 --> 00:23:51,280
What are you guys up to today?

451
00:23:57,580 --> 00:23:59,390
I'm just finishing up work and waiting

452
00:23:59,390 --> 00:24:00,640
for the boys to get home.

453
00:24:01,540 --> 00:24:03,010
Okay.

454
00:24:03,010 --> 00:24:05,253
I think I'm coming down with a virus.

455
00:24:07,767 --> 00:24:09,770
(laughter)

456
00:24:09,770 --> 00:24:10,810
I was messing around with you.

457
00:24:10,810 --> 00:24:12,460
You were talking to the computer.

458
00:24:16,868 --> 00:24:18,518
(laughs) Is that scary or good?

459
00:24:24,400 --> 00:24:25,233
I don't know.

460
00:24:26,386 --> 00:24:28,636
(laughter)

461
00:24:29,490 --> 00:24:30,323
Is it?

462
00:24:34,333 --> 00:24:37,640
(calm music)

463
00:24:37,640 --> 00:24:40,010
After realizing that anyone with the time

464
00:24:40,010 --> 00:24:42,570
and inclination could mess with my life,

465
00:24:42,570 --> 00:24:45,450
there was only one thing left to do.

466
00:24:45,450 --> 00:24:48,180
I joined Jose and a few other Lyrebirds

467
00:24:48,180 --> 00:24:51,000
to chat more about the evils of AI

468
00:24:51,000 --> 00:24:53,950
while dulling my fear with booze.

469
00:24:53,950 --> 00:24:56,600
Obviously some people are
freaked out by this technology

470
00:24:56,600 --> 00:24:59,800
because we're already
like blurring the line

471
00:24:59,800 --> 00:25:02,140
about truth and reality.

472
00:25:02,140 --> 00:25:05,500
Of course there is some risk in people

473
00:25:05,500 --> 00:25:08,630
using this kind of technology
for bad applications.

474
00:25:08,630 --> 00:25:13,200
Unfortunately, technologies
it's not possible to stop it.

475
00:25:13,200 --> 00:25:16,420
So the ethical path that we have decided

476
00:25:16,420 --> 00:25:19,710
is to show these to
people, to make them know

477
00:25:19,710 --> 00:25:22,377
that this kind of technology is available

478
00:25:22,377 --> 00:25:26,570
and to make them more cautious
from this kind of subject.

479
00:25:26,570 --> 00:25:28,280
We really believe that right now

480
00:25:28,280 --> 00:25:30,530
that the technology is not perfect

481
00:25:30,530 --> 00:25:34,090
is the right time to let
people kind of play with it,

482
00:25:34,090 --> 00:25:36,101
get used to it slowly.

483
00:25:36,101 --> 00:25:39,911
So you guys think that the
idea is just sort of new

484
00:25:39,911 --> 00:25:41,410
and that's why it scares people,

485
00:25:41,410 --> 00:25:43,238
but if you get used to it it's just,

486
00:25:43,238 --> 00:25:44,490
that's just the way it is.

487
00:25:44,490 --> 00:25:48,310
We want our technology to
be used for positive things.

488
00:25:48,310 --> 00:25:51,900
It's not something that we
should be really afraid of,

489
00:25:51,900 --> 00:25:54,490
it's something that we
should be careful about

490
00:25:54,490 --> 00:25:56,483
but I feel enthusiastic about.

491
00:25:57,490 --> 00:25:59,990
(calm music)

492
00:26:01,610 --> 00:26:03,160
It's nice to be enthusiastic.

493
00:26:04,160 --> 00:26:06,760
It's also nice to meditate
on the consequences

494
00:26:06,760 --> 00:26:09,570
of your inventions, instead
of turning our souls

495
00:26:09,570 --> 00:26:11,503
over to chance and blind luck.

496
00:26:14,991 --> 00:26:17,860
But it is kinda cool
to be a cynical bastard

497
00:26:17,860 --> 00:26:20,593
in my new artisanal computer voice.

498
00:26:22,210 --> 00:26:24,340
Welcome Russian friends to our huge,

499
00:26:24,340 --> 00:26:26,563
wonderful and very pure elections.

500
00:26:38,190 --> 00:26:40,430
The real artificial intelligence weirdos

501
00:26:40,430 --> 00:26:43,753
in Canada live here in Edmonton.

502
00:26:51,210 --> 00:26:53,860
This is a large but very, very cold

503
00:26:53,860 --> 00:26:56,110
and very, very flat city.

504
00:26:56,110 --> 00:26:58,443
It is more or less in
the middle of nowhere.

505
00:26:59,830 --> 00:27:02,940
It's the kind of place that
has a giant butter vault

506
00:27:02,940 --> 00:27:06,473
to help people survive
the lean winter months.

507
00:27:08,110 --> 00:27:10,660
Canadians like to put
the best possible spin

508
00:27:10,660 --> 00:27:12,230
on how these conditions bring out

509
00:27:12,230 --> 00:27:13,813
interesting traits in people.

510
00:27:15,120 --> 00:27:16,410
Ask anyone.

511
00:27:16,410 --> 00:27:19,350
Like this guy from the
Edmonton Tourist Center.

512
00:27:19,350 --> 00:27:21,610
Well Edmonton's one of those cities

513
00:27:21,610 --> 00:27:25,570
that isn't automatically listed
in the top cities in Canada

514
00:27:25,570 --> 00:27:29,230
in terms of size or scale or notice even.

515
00:27:29,230 --> 00:27:33,230
But it's always had a
really neat quality to it

516
00:27:33,230 --> 00:27:35,820
of that Western independent spirit

517
00:27:35,820 --> 00:27:38,910
that you see very much
in Alberta in general,

518
00:27:38,910 --> 00:27:43,453
combined with a conscience
and thoughtfulness.

519
00:27:45,560 --> 00:27:47,650
Over at the University of Alberta,

520
00:27:47,650 --> 00:27:50,670
some of the most far
out AI research in world

521
00:27:50,670 --> 00:27:51,803
is taking place.

522
00:27:53,230 --> 00:27:56,060
The man I'm here to see is
the university's very own

523
00:27:56,060 --> 00:27:58,443
AI godfather, Rich Sutton.

524
00:27:59,370 --> 00:28:01,100
Rich is considered one of the great

525
00:28:01,100 --> 00:28:03,203
revolutionary thinkers in AI.

526
00:28:04,390 --> 00:28:06,240
You are not Canadian.

527
00:28:06,240 --> 00:28:07,073
I am Canadian.

528
00:28:07,073 --> 00:28:09,120
You are (laughs), but not by birth.

529
00:28:09,120 --> 00:28:10,650
No, I was born in the US.

530
00:28:10,650 --> 00:28:12,230
But now I'm just Canadian.

531
00:28:12,230 --> 00:28:13,063
Okay.

532
00:28:13,063 --> 00:28:15,030
And what brought you to Canada?

533
00:28:15,030 --> 00:28:15,863
The politics.

534
00:28:15,863 --> 00:28:18,050
I wanted to get away from difficult times

535
00:28:18,050 --> 00:28:19,570
in the United States.

536
00:28:19,570 --> 00:28:22,850
United States was invading
other countries in 2003

537
00:28:22,850 --> 00:28:26,313
when I came here and I
didn't care for all that.

538
00:28:29,530 --> 00:28:32,480
Sutton entered the field
of AI in the mid '80s.

539
00:28:32,480 --> 00:28:35,110
And like Geoff Hinton and Yoshua Bengio,

540
00:28:35,110 --> 00:28:37,403
he was a big believer in neural networks.

541
00:28:38,470 --> 00:28:40,220
But Sutton has a different idea

542
00:28:40,220 --> 00:28:42,313
about how to further the technology.

543
00:28:43,618 --> 00:28:46,080
Unlike Hinton's method of
feeding neural networks

544
00:28:46,080 --> 00:28:49,560
reams and reams of data and
telling them what to do.

545
00:28:49,560 --> 00:28:53,500
Sutton wants them to learn
more naturally from experience,

546
00:28:53,500 --> 00:28:56,203
an approach called reinforcement learning.

547
00:28:57,530 --> 00:28:59,760
Reinforcement learning,
it's like what animals do

548
00:28:59,760 --> 00:29:01,700
and what people do, try several things,

549
00:29:01,700 --> 00:29:03,690
the things that work
best you keep doing those

550
00:29:03,690 --> 00:29:06,110
and things that don't work out
so well you stop doing them.

551
00:29:06,110 --> 00:29:09,896
And how do you teach a computer that idea?

552
00:29:09,896 --> 00:29:11,356
The computer has to have a sense

553
00:29:11,356 --> 00:29:13,900
of what's good and what's bad.

554
00:29:13,900 --> 00:29:16,180
And so you give a special
signal called a reward.

555
00:29:16,180 --> 00:29:18,257
If the reward is high
that means it's good,

556
00:29:18,257 --> 00:29:20,113
if the reward is low that means it's bad.

557
00:29:21,990 --> 00:29:24,440
To see reinforcement learning in action,

558
00:29:24,440 --> 00:29:27,820
I found Marlos] an
industrious young Brazilian

559
00:29:27,820 --> 00:29:31,523
whose created an AI to play
his video games for him.

560
00:29:32,500 --> 00:29:35,660
His algorithm plays the
game thousands of times

561
00:29:35,660 --> 00:29:38,623
and gradually learns from
experience how to do better.

562
00:29:39,700 --> 00:29:43,320
So the goal of this game is
that you are this yellow block

563
00:29:43,320 --> 00:29:45,030
and what you have to do
is that you have to get

564
00:29:45,030 --> 00:29:47,733
as many potions as you can
while avoiding harpies.

565
00:29:48,930 --> 00:29:52,070
And this is like the AI going
at this for the first time.

566
00:29:52,070 --> 00:29:53,920
It's the AI running for the first time.

567
00:29:53,920 --> 00:29:55,220
So it just bumps into things.

568
00:29:55,220 --> 00:29:58,660
If it gets points it's happy,
if it dies it's unhappy.

569
00:29:58,660 --> 00:29:59,920
Yes.

570
00:29:59,920 --> 00:30:03,440
And the AI starts to figure
out that maybe what I wanna do

571
00:30:03,440 --> 00:30:06,030
is to collect the potions
and avoid the harpies.

572
00:30:06,030 --> 00:30:10,278
And now we can look at AI
that has ran for 5,000 games.

573
00:30:10,278 --> 00:30:11,111
Okay.

574
00:30:11,111 --> 00:30:12,603
And this is what it looks like.

575
00:30:15,680 --> 00:30:16,910
You can tell that it's smarter

576
00:30:16,910 --> 00:30:19,938
about its strategy.
Yes.

577
00:30:19,938 --> 00:30:23,230
And then what happens if
you run it 500,000 times?

578
00:30:23,230 --> 00:30:26,083
Oh, we got you this
superhuman performance level.

579
00:30:28,940 --> 00:30:30,380
Though notching a high score

580
00:30:30,380 --> 00:30:32,420
is the noblest of pursuits,

581
00:30:32,420 --> 00:30:34,610
reinforcement learning
has turned out to have

582
00:30:34,610 --> 00:30:36,210
all kinds of other applications.

583
00:30:38,120 --> 00:30:40,610
It's behind the algorithm
that recommends movies

584
00:30:40,610 --> 00:30:43,900
and TV shows on Netflix and Amazon.

585
00:30:43,900 --> 00:30:46,340
It beat the world champion Go player,

586
00:30:46,340 --> 00:30:49,483
a feat previously thought
impossible for a computer.

587
00:30:50,740 --> 00:30:53,200
Soon, it could read your brain waves

588
00:30:53,200 --> 00:30:55,783
and determine whether you
have a mental disorder.

589
00:30:57,420 --> 00:31:00,960
But for Sutton, all that
is just the beginning.

590
00:31:00,960 --> 00:31:03,230
We are trying to make real intelligence.

591
00:31:03,230 --> 00:31:05,590
We're trying to recreate
human intelligence.

592
00:31:05,590 --> 00:31:07,103
Humans are our examples.

593
00:31:08,120 --> 00:31:10,550
He sees reinforcement learning as the path

594
00:31:10,550 --> 00:31:13,770
to what futurists call the singularity.

595
00:31:13,770 --> 00:31:16,610
The moment when our AI creations light up

596
00:31:16,610 --> 00:31:19,363
and surge past human level intelligence.

597
00:31:20,734 --> 00:31:24,006
Do you have dates for the singularity or?

598
00:31:24,006 --> 00:31:27,480
It's a quite broad
probability distribution

599
00:31:27,480 --> 00:31:29,966
and the median is at 2040.

600
00:31:29,966 --> 00:31:34,070
So that means equal chance
being before or after 2040.

601
00:31:34,070 --> 00:31:36,120
The rationale goes like this.

602
00:31:36,120 --> 00:31:38,670
By 2030, we'll have the hardware.

603
00:31:38,670 --> 00:31:43,670
So give guys like me another
10 years to figure out

604
00:31:43,730 --> 00:31:44,800
the algorithms,

605
00:31:44,800 --> 00:31:47,440
the software to go with
the hardware to do it

606
00:31:48,692 --> 00:31:50,942
and it's gonna be exciting
where we're going.

607
00:31:52,680 --> 00:31:54,660
If 2040 seems like a long time

608
00:31:54,660 --> 00:31:57,693
to wait to meet a smart
robot, do not fret.

609
00:31:58,840 --> 00:32:01,740
Over in the experimental
wing of the university,

610
00:32:01,740 --> 00:32:04,410
there are coeds hard at
work learning the line

611
00:32:04,410 --> 00:32:06,073
between humans and machines.

612
00:32:07,260 --> 00:32:08,113
Are you human?

613
00:32:09,011 --> 00:32:10,410
Of course not,

614
00:32:10,410 --> 00:32:12,460
but that shouldn't keep us from chatting.

615
00:32:13,340 --> 00:32:16,910
Case in point, homegrown
Edmontonian genius,

616
00:32:16,910 --> 00:32:18,800
Kory Mathewson.

617
00:32:18,800 --> 00:32:20,570
Tell me about this guy a little bit or--

618
00:32:20,570 --> 00:32:21,900
Yeah, sure.

619
00:32:21,900 --> 00:32:22,950
Sure.

620
00:32:22,950 --> 00:32:25,010
So this is Blueberry.

621
00:32:25,010 --> 00:32:28,680
On Blueberry, I've
deployed the Improv System

622
00:32:28,680 --> 00:32:30,440
so there's an artificial Improv System

623
00:32:30,440 --> 00:32:31,990
running on Blueberry right now.

624
00:32:33,490 --> 00:32:38,490
Yes, that's right, Kory does
Improv comedy with a robot.

625
00:32:39,120 --> 00:32:40,570
I've been doing Improv longer

626
00:32:40,570 --> 00:32:42,020
than I've been doing computing science.

627
00:32:42,020 --> 00:32:44,440
I've been doing it for
12 years and I thought

628
00:32:44,440 --> 00:32:46,540
there's no more natural convergence

629
00:32:46,540 --> 00:32:48,890
than taking some of these
state-of-the-art systems

630
00:32:48,890 --> 00:32:50,720
and putting them up on stage.

631
00:32:50,720 --> 00:32:53,480
One day, we'll take it to
the moon if this planet

632
00:32:54,410 --> 00:32:56,113
is not to be our last.

633
00:33:00,810 --> 00:33:02,188
(laughter)

634
00:33:02,188 --> 00:33:04,660
The sky, the moon, and the universe.

635
00:33:04,660 --> 00:33:06,077
The sun, the sun.

636
00:33:07,028 --> 00:33:08,090
The sky, the moon, and the universe.

637
00:33:08,090 --> 00:33:09,600
I keep thinking it's like ventriloquist

638
00:33:09,600 --> 00:33:11,203
or it is like a new edge.

639
00:33:12,240 --> 00:33:14,260
That's really good way to put it, yeah.

640
00:33:14,260 --> 00:33:15,160
Strange too as I thought it.

641
00:33:15,160 --> 00:33:17,050
The piece that's different is that

642
00:33:17,050 --> 00:33:19,730
I don't know what it will say.

643
00:33:19,730 --> 00:33:20,940
Anything that comes from the system,

644
00:33:20,940 --> 00:33:22,690
it's generating live in the moment.

645
00:33:23,800 --> 00:33:25,410
Blueberry, I created you.

646
00:33:25,410 --> 00:33:27,810
I downloaded a voice into your brain

647
00:33:27,810 --> 00:33:30,670
so that you could perform
in front of these people.

648
00:33:30,670 --> 00:33:33,330
But I do not know what I'm going to say.

649
00:33:33,330 --> 00:33:35,530
I don't know what you're gonna say either.

650
00:33:37,240 --> 00:33:39,540
To give Blueberry the power of surreal

651
00:33:39,540 --> 00:33:42,530
Canadian Improv, Kory
made use of some tech

652
00:33:42,530 --> 00:33:45,993
that should be familiar
by now, a neural network.

653
00:33:49,370 --> 00:33:52,330
Step one, he feeds the
network the dialogue

654
00:33:52,330 --> 00:33:54,400
from a bunch of movies.

655
00:33:54,400 --> 00:33:56,923
102,000 movies to be exact.

656
00:33:57,980 --> 00:33:58,813
All the movies.

657
00:33:58,813 --> 00:34:02,230
Every movie for 100 years.

658
00:34:02,230 --> 00:34:04,640
And that's just so it can learn language

659
00:34:04,640 --> 00:34:06,990
to see how somebody
responds to somebody else.

660
00:34:06,990 --> 00:34:07,823
That's exactly right.

661
00:34:07,823 --> 00:34:10,203
Yeah, it builds kind of language model.

662
00:34:11,100 --> 00:34:13,960
Step two, he uses reinforcement learning

663
00:34:13,960 --> 00:34:15,429
to train the network.

664
00:34:15,429 --> 00:34:17,609
Rewarding it when it makes sense

665
00:34:17,610 --> 00:34:20,353
and the punishing it when
it spits out gibberish.

666
00:34:21,520 --> 00:34:24,783
Time to put this wannabe
kid in the hall to the test.

667
00:34:25,650 --> 00:34:26,840
There we go.

668
00:34:26,840 --> 00:34:28,083
Start improvising.

669
00:34:30,110 --> 00:34:32,650
Okay, campers, we're gonna get ready

670
00:34:32,650 --> 00:34:34,960
for a real baseball game.

671
00:34:34,960 --> 00:34:37,179
Grab your gloves and
grab your baseball bats,

672
00:34:37,179 --> 00:34:40,142
let's get out there,
especially you, Franklin.

673
00:34:43,239 --> 00:34:44,159
Okay, okay.

674
00:34:44,159 --> 00:34:46,792
Well, why aren't you ready for the match?

675
00:34:51,440 --> 00:34:52,510
Okay.

676
00:34:52,510 --> 00:34:55,350
Come on, Franklin, you
know how I feel about you,

677
00:34:55,350 --> 00:34:58,163
but you gotta keep your
head in the game right now.

678
00:35:03,309 --> 00:35:04,876
It's threatening you?

679
00:35:04,876 --> 00:35:05,709
I know.

680
00:35:08,570 --> 00:35:11,490
Oh, Jesus, put down the bat, Franklin.

681
00:35:11,490 --> 00:35:12,513
What are you doing?

682
00:35:15,390 --> 00:35:16,640
I've got nothing to hide.

683
00:35:16,640 --> 00:35:19,060
Look, this is all I am.

684
00:35:19,060 --> 00:35:21,266
Okay, I'll end it there.

685
00:35:21,266 --> 00:35:22,099
That's what how it work.
That's great.

686
00:35:23,030 --> 00:35:25,410
Obviously, some of the responses
are a little bit weird,

687
00:35:25,410 --> 00:35:28,120
but that it's really funny
'cause then as you go along,

688
00:35:28,120 --> 00:35:30,400
it did hid a couple of things perfectly

689
00:35:30,400 --> 00:35:33,510
and then it's like, I
mean, it's extra hilarious

690
00:35:33,510 --> 00:35:35,423
because, yeah, that's going.

691
00:35:38,180 --> 00:35:39,500
Blueberry may not be ready

692
00:35:39,500 --> 00:35:41,783
for its second city audition just yet,

693
00:35:42,820 --> 00:35:46,863
but Kory has a higher
purpose: making AI relatable.

694
00:35:48,860 --> 00:35:49,693
Oh, it's gonna move.

695
00:35:49,693 --> 00:35:50,526
It's gonna move.

696
00:35:50,526 --> 00:35:51,359
It's gonna move.

697
00:35:51,359 --> 00:35:52,192
Holy crap.

698
00:35:52,192 --> 00:35:53,820
(laughter)

699
00:35:53,820 --> 00:35:57,423
There is fear in society of AI.

700
00:35:58,350 --> 00:36:01,060
So we are kind of humanizing this AI

701
00:36:01,060 --> 00:36:02,550
where we're taking it down a peg.

702
00:36:02,550 --> 00:36:04,690
We're saying don't be afraid of this tech.

703
00:36:04,690 --> 00:36:05,730
Look at how cute it is.

704
00:36:05,730 --> 00:36:07,610
Look at how kind of naive it is.

705
00:36:07,610 --> 00:36:09,664
Yeah, yeah, yeah that sounds cool.

706
00:36:09,664 --> 00:36:11,163
You've done it again, Blueberry.

707
00:36:12,140 --> 00:36:13,360
Isn't there a flip side to that though

708
00:36:13,360 --> 00:36:17,514
that you make it cute and
then people start to accept it

709
00:36:17,514 --> 00:36:20,360
then we wake up?

710
00:36:20,360 --> 00:36:23,653
I mean, I don't think that
will happen in my time.

711
00:36:25,920 --> 00:36:30,513
The singularity may be
near or maybe not so near.

712
00:36:31,620 --> 00:36:34,440
But if the inhabitants of
this oddly beautiful place

713
00:36:34,440 --> 00:36:37,590
keep pushing the technology,
they just might create

714
00:36:37,590 --> 00:36:40,143
something alarmingly human-like one day.

715
00:36:42,480 --> 00:36:43,810
For Rich Sutton,

716
00:36:43,810 --> 00:36:46,300
it's not a question of
whether we'll get there,

717
00:36:46,300 --> 00:36:49,693
but whether we'll be able to
accept our mechanized brethren.

718
00:36:50,540 --> 00:36:52,914
Our society will be challenged.

719
00:36:52,914 --> 00:36:56,580
It's just like every time
are lack people people,

720
00:36:56,580 --> 00:36:58,210
are women people,

721
00:36:58,210 --> 00:37:00,913
we will do the same thing
with robots eventually.

722
00:37:02,320 --> 00:37:04,550
Are they allowed to own property?

723
00:37:04,550 --> 00:37:06,220
Are they allowed to earn an income

724
00:37:06,220 --> 00:37:08,290
or do they have to be owned by somebody?

725
00:37:08,290 --> 00:37:10,393
But a robot is obviously not a person.

726
00:37:12,780 --> 00:37:13,613
Right?

727
00:37:15,140 --> 00:37:16,150
No.

728
00:37:16,150 --> 00:37:18,567
(calm music)

729
00:37:28,892 --> 00:37:31,475
(upbeat music)

730
00:37:36,080 --> 00:37:39,390
For my last stop, I returned to Toronto.

731
00:37:39,390 --> 00:37:44,390
Home to 2.8 million people,
one very tall tower,

732
00:37:44,510 --> 00:37:47,680
and of course, the godfather himself.

733
00:37:47,680 --> 00:37:50,840
Inside the system, there's
also little processes

734
00:37:50,840 --> 00:37:53,490
which are a little bit like brain cells.

735
00:37:53,490 --> 00:37:55,090
He may be an import,

736
00:37:55,090 --> 00:37:56,950
but Geoff Hinton has done something

737
00:37:56,950 --> 00:37:59,430
truly exceptional for Toronto.

738
00:37:59,430 --> 00:38:02,250
He's turned this city into an AI Mecca

739
00:38:02,250 --> 00:38:06,675
where AI conferences like this
one seem to take place daily

740
00:38:06,675 --> 00:38:09,092
(applauding)

741
00:38:10,640 --> 00:38:14,023
and where young minds come
to show off their ideas.

742
00:38:15,580 --> 00:38:17,890
Canada, if we're being honest,

743
00:38:17,890 --> 00:38:20,163
doesn't usually seem that intimidating,

744
00:38:21,030 --> 00:38:23,510
but thanks to Geoff, it's got nothing

745
00:38:23,510 --> 00:38:26,210
less than world domination in mind.

746
00:38:26,210 --> 00:38:29,010
We are enormously thankful to Canadians

747
00:38:29,010 --> 00:38:32,040
for inventing all these
stuff 'cause we now use it

748
00:38:32,040 --> 00:38:33,370
throughout our entire business--

749
00:38:33,370 --> 00:38:35,090
And we have it on record that he owns

750
00:38:35,090 --> 00:38:36,230
that Google owns Canada.

751
00:38:36,230 --> 00:38:39,483
We absolutely own Canada,
so that was a mistake.

752
00:38:42,740 --> 00:38:45,763
The tech industry is full
of people who adore AI.

753
00:38:48,810 --> 00:38:51,410
And then also some famous
types like Elon Musk

754
00:38:51,410 --> 00:38:53,570
and Stephen Hawking who said,

755
00:38:53,570 --> 00:38:56,183
well, that AI might be the end of us.

756
00:38:58,770 --> 00:39:01,643
To consider such dystopia
in the proper light,

757
00:39:04,020 --> 00:39:06,300
I've come to Toronto's geekiest bar

758
00:39:08,580 --> 00:39:11,390
to encase myself in this steel container

759
00:39:11,390 --> 00:39:13,300
with George Dvorsky.

760
00:39:13,300 --> 00:39:16,953
He's a writer for Gizmodo
and an AI philosopher.

761
00:39:19,080 --> 00:39:22,160
Since we're in a Apocalyptic bar,

762
00:39:22,160 --> 00:39:25,730
what is the con case around AI?

763
00:39:25,730 --> 00:39:27,810
What's the nastiest scenario

764
00:39:27,810 --> 00:39:29,500
that everybody is worried about?

765
00:39:29,500 --> 00:39:32,360
Unfortunately, there is no
shortage of nasty scenarios

766
00:39:32,360 --> 00:39:35,110
and I think this is what
makes artificial intelligence

767
00:39:35,110 --> 00:39:37,080
such a scary thing is
all the different ways

768
00:39:37,080 --> 00:39:38,400
that it can go wrong.

769
00:39:38,400 --> 00:39:40,490
It can be everything from an accident

770
00:39:40,490 --> 00:39:42,300
where we just didn't think it through.

771
00:39:42,300 --> 00:39:45,760
We gave a very powerful
computer instructions

772
00:39:45,760 --> 00:39:46,760
to do something.

773
00:39:46,760 --> 00:39:48,500
We thought we explained it articulately,

774
00:39:48,500 --> 00:39:50,230
we thought we gave it a concrete goal

775
00:39:50,230 --> 00:39:52,560
and it completely took a different path

776
00:39:52,560 --> 00:39:53,830
than we thought it would in such a way

777
00:39:53,830 --> 00:39:56,423
that it actually caused some great damage.

778
00:39:57,620 --> 00:39:59,870
And I'm sure you've heard
the old paper clip example

779
00:39:59,870 --> 00:40:02,630
where you're a paper clip
manufacturer and you say, hey,

780
00:40:02,630 --> 00:40:04,146
we need lots of paper clips,

781
00:40:04,146 --> 00:40:05,520
and because the artificial intelligence

782
00:40:05,520 --> 00:40:07,890
has so much reach and so much power,

783
00:40:07,890 --> 00:40:10,660
it actually starts to go about
converting all the matter

784
00:40:10,660 --> 00:40:13,110
and all the molecules on
the planet into paper clips.

785
00:40:13,110 --> 00:40:15,700
Before you know it, we've now
converted the entire cosmos

786
00:40:15,700 --> 00:40:16,550
into paper clips.

787
00:40:18,170 --> 00:40:21,030
It's a crazy scenario, but
it's an illustrative scenario.

788
00:40:21,030 --> 00:40:23,220
We can't be dismissive of the perils.

789
00:40:23,220 --> 00:40:25,870
I think that's exceptionally dangerous

790
00:40:25,870 --> 00:40:28,050
and I don't think it's too early

791
00:40:28,050 --> 00:40:29,950
to start raising alarm bells about it.

792
00:40:32,990 --> 00:40:35,490
Being turned into clippie sounds awful.

793
00:40:38,450 --> 00:40:40,770
But fear not, we'll have years

794
00:40:40,770 --> 00:40:42,940
to ease into that sort of suffering

795
00:40:45,280 --> 00:40:49,093
as AI steadily plucks off
one job after another.

796
00:40:52,460 --> 00:40:54,870
The first to go, of course, will likely be

797
00:40:54,870 --> 00:40:58,030
the always screwed factory workers

798
00:40:58,030 --> 00:41:01,060
which brings us to Suzanne Gilbert,

799
00:41:01,060 --> 00:41:03,360
a budding AI overlord

800
00:41:03,360 --> 00:41:06,663
and founder of robotic
startup, Kindred AI.

801
00:41:07,571 --> 00:41:09,988
(calm music)

802
00:41:15,790 --> 00:41:17,270
Tell me about these guys.

803
00:41:17,270 --> 00:41:20,410
So these are research prototypes.

804
00:41:20,410 --> 00:41:23,360
So they're some of the first
robots we've built at Kindred.

805
00:41:25,020 --> 00:41:26,870
We tend to work with small robots.

806
00:41:26,870 --> 00:41:30,390
It's a bit like if you
imagine a child growing up

807
00:41:30,390 --> 00:41:31,570
and it breaks a lot of things.

808
00:41:31,570 --> 00:41:33,810
Now, imagine if the
child was six feet tall

809
00:41:33,810 --> 00:41:36,110
when it have the brain of a six-month-old,

810
00:41:36,110 --> 00:41:39,240
it would be terribly dangerous.

811
00:41:39,240 --> 00:41:42,030
How many of these robots
have ever slapped you?

812
00:41:42,030 --> 00:41:45,633
I have been hit in the face
by robots a couple of times.

813
00:41:47,330 --> 00:41:48,973
Suzanne seems nice enough.

814
00:41:53,410 --> 00:41:55,093
She makes exotic digital art.

815
00:41:57,840 --> 00:42:00,030
And she loves cats to the
point where she has built

816
00:42:00,030 --> 00:42:02,103
a robotic fleet of them for the office.

817
00:42:03,240 --> 00:42:05,090
This one, I usually call pink foot.

818
00:42:07,480 --> 00:42:11,330
It's a quadruped robot
loosely based on cat anatomy,

819
00:42:11,330 --> 00:42:14,903
although it's not a very highly
faithful representation yet.

820
00:42:16,920 --> 00:42:18,100
And then when you were growing up,

821
00:42:18,100 --> 00:42:19,760
you would build things as well?

822
00:42:19,760 --> 00:42:21,210
Yeah, that's correct, yeah.

823
00:42:23,120 --> 00:42:27,033
So I was really enthralled by
electronics at an early age.

824
00:42:28,230 --> 00:42:29,830
I guess most little
girls will be looking at

825
00:42:29,830 --> 00:42:32,320
trays of beads and things
and I was looking at trays

826
00:42:32,320 --> 00:42:34,940
of like resistors and capacitors
and little components,

827
00:42:34,940 --> 00:42:37,190
but having the same kind
of reaction to them.

828
00:42:39,890 --> 00:42:42,030
But don't be fooled by
the hobby electronics

829
00:42:42,030 --> 00:42:43,453
and the cute cat bots.

830
00:42:45,340 --> 00:42:47,543
Suzanne is a keen business woman.

831
00:42:49,770 --> 00:42:51,790
And Kindred has recently embarked

832
00:42:51,790 --> 00:42:53,653
on its first commercial venture.

833
00:42:56,050 --> 00:42:58,950
What's going on here is that
we have a bank of robots

834
00:42:58,950 --> 00:43:00,170
that are learning,

835
00:43:00,170 --> 00:43:04,300
so they are continuously
running picking up objects.

836
00:43:04,300 --> 00:43:05,800
These would run all day?

837
00:43:05,800 --> 00:43:06,850
All day, all night.

838
00:43:13,070 --> 00:43:14,530
Powered by a neural network,

839
00:43:14,530 --> 00:43:17,970
these arms can do something
that's very easy for a human,

840
00:43:17,970 --> 00:43:20,280
but very hard for a bot.

841
00:43:20,280 --> 00:43:24,133
Pick up objects of different
shapes and put them down.

842
00:43:25,660 --> 00:43:28,900
Most factories still use people
to do that sort of thing,

843
00:43:28,900 --> 00:43:31,300
lots and lots of people.

844
00:43:31,300 --> 00:43:33,415
Today, everyone's shopping on ecommerce.

845
00:43:33,415 --> 00:43:35,540
Thousands and thousands of
different types of objects,

846
00:43:35,540 --> 00:43:39,940
shapes, textures, weights,
how do you pick that up?

847
00:43:39,940 --> 00:43:41,860
Right now as humans, we
have millions of humans

848
00:43:41,860 --> 00:43:43,360
in warehouses just like picking up things

849
00:43:43,360 --> 00:43:45,490
and putting it into another location,

850
00:43:45,490 --> 00:43:47,690
so we're teaching our
robots how to do that.

851
00:43:51,100 --> 00:43:52,770
What's the hard part is figuring out

852
00:43:52,770 --> 00:43:55,890
what's a belt, what's a shirt
or it's just how to grasp it.

853
00:43:55,890 --> 00:43:56,723
Yeah, exactly.

854
00:43:56,723 --> 00:43:57,556
It's very hard to pick it up, right?

855
00:43:57,556 --> 00:43:59,260
So things will show up in any shape

856
00:43:59,260 --> 00:44:01,920
and you gotta figure out how
to pick up without dropping it,

857
00:44:01,920 --> 00:44:03,050
put in the location.

858
00:44:03,050 --> 00:44:04,583
So it takes a lot of training.

859
00:44:06,590 --> 00:44:08,330
Part of the training involves,

860
00:44:08,330 --> 00:44:10,850
of all things, humans.

861
00:44:10,850 --> 00:44:14,070
Robot pilots who manually control the arms

862
00:44:14,070 --> 00:44:15,740
while the AI watches

863
00:44:15,740 --> 00:44:17,963
and learns the finer points of grabbing.

864
00:44:19,860 --> 00:44:21,013
All right, man, teach me
how to use this thing.

865
00:44:21,013 --> 00:44:21,903
Have a seat.

866
00:44:24,880 --> 00:44:26,430
So you see a 3D mouse here,

867
00:44:26,430 --> 00:44:29,460
this lets you navigate the
arm through dimensional space.

868
00:44:29,460 --> 00:44:32,450
So imagine you're holding
the arm in that left hand

869
00:44:32,450 --> 00:44:33,950
and you're just moving around.

870
00:44:35,520 --> 00:44:37,153
Move it slowly and gently.

871
00:44:38,360 --> 00:44:39,193
There you go.

872
00:44:40,867 --> 00:44:43,070
I'm trying to get the Oreos.

873
00:44:43,070 --> 00:44:44,150
I gotta go up.

874
00:44:44,150 --> 00:44:46,043
Oh, shoot, I went too far up.

875
00:44:47,337 --> 00:44:48,287
I want these Oreos.

876
00:44:50,015 --> 00:44:52,415
(grunts)

877
00:44:52,415 --> 00:44:54,015
You killed the can.
You lose.

878
00:44:55,920 --> 00:44:57,793
Come back to me arm.

879
00:44:59,690 --> 00:45:01,773
There, success.

880
00:45:04,260 --> 00:45:06,960
It's like being in an arcade.
Basically.

881
00:45:06,960 --> 00:45:09,260
It's like you actually
get to win something.

882
00:45:12,230 --> 00:45:14,470
Just down the hall, Kindred keeps a room

883
00:45:14,470 --> 00:45:17,600
full of pilots doing the same thing as me.

884
00:45:17,600 --> 00:45:19,843
Only these guys are actually competent.

885
00:45:21,000 --> 00:45:24,010
They're remotely overseeing
some arms in a Gap factory,

886
00:45:24,010 --> 00:45:26,293
a thousand miles away in Tennessee.

887
00:45:29,770 --> 00:45:31,910
How long have you been a robot pilot?

888
00:45:31,910 --> 00:45:33,260
Just over a month actually.

889
00:45:33,260 --> 00:45:34,990
I've only been here five weeks.

890
00:45:34,990 --> 00:45:37,641
What was the training process like?

891
00:45:37,641 --> 00:45:39,203
Almost like playing a video game.

892
00:45:41,100 --> 00:45:42,370
It's a like shirt, done.

893
00:45:42,370 --> 00:45:44,770
That's a backpack.
That's a backpack, okay.

894
00:45:46,030 --> 00:45:47,420
Somebody's undies.

895
00:45:47,420 --> 00:45:49,620
Oh, there it goes.

896
00:45:49,620 --> 00:45:50,720
One shirt at a time.

897
00:45:52,639 --> 00:45:55,139
(calm music)

898
00:46:00,257 --> 00:46:02,360
As the arms observed their human guides,

899
00:46:02,360 --> 00:46:04,190
they gradually learn how to do better

900
00:46:04,190 --> 00:46:06,563
at picking up T-shirts and shoe boxes.

901
00:46:08,500 --> 00:46:11,400
Eventually, they'll be fully autonomous

902
00:46:11,400 --> 00:46:15,400
and size services will
no longer be required.

903
00:46:15,400 --> 00:46:17,130
One day, this is just gonna light up

904
00:46:17,130 --> 00:46:19,110
and it's gonna be picking the objects--

905
00:46:19,110 --> 00:46:19,943
Pretty much.

906
00:46:19,943 --> 00:46:23,280
Pretty much that's the ultimate
end goal at least for these

907
00:46:23,280 --> 00:46:25,633
to have it just constantly
wearing and going.

908
00:46:27,867 --> 00:46:29,240
And the people will be free.

909
00:46:29,240 --> 00:46:32,240
The people will be free to do
other more important things.

910
00:46:35,440 --> 00:46:37,870
So he seemed kind of
happy about the prospect

911
00:46:37,870 --> 00:46:42,133
of unemployment, but I was
concerned for his future.

912
00:46:47,620 --> 00:46:51,053
Isn't there something grim
about the human training there?

913
00:46:52,150 --> 00:46:55,690
Yeah, it's not good to
take people's jobs away,

914
00:46:55,690 --> 00:46:58,480
but this kind of technology
coming into the workforce

915
00:46:58,480 --> 00:47:00,230
should make us stop thinking

916
00:47:00,230 --> 00:47:03,180
about how we're going to
pay people in the future

917
00:47:03,180 --> 00:47:07,780
because AI is not just going
to automate manual labor jobs,

918
00:47:07,780 --> 00:47:10,680
it's gonna automate things
like doctors, and lawyers,

919
00:47:10,680 --> 00:47:12,280
and accountants very soon,

920
00:47:12,280 --> 00:47:13,790
so I think there's gonna be issues,

921
00:47:13,790 --> 00:47:15,480
there's gonna be a lot of disruption

922
00:47:15,480 --> 00:47:17,013
when these things come online.

923
00:47:19,500 --> 00:47:22,963
Suzanne is a realist, but
she's also an optimist.

924
00:47:27,140 --> 00:47:29,000
In her vision of the future,

925
00:47:29,000 --> 00:47:32,130
robots won't be mindless
competitors to humanity.

926
00:47:32,130 --> 00:47:35,610
They'll be full-fledged
citizens like the rest of us.

927
00:47:35,610 --> 00:47:38,280
One of the crazy ideas
that you talked about was

928
00:47:38,280 --> 00:47:41,460
you've got a robot and
it's working at a factory

929
00:47:41,460 --> 00:47:43,940
and then it's gotta go,
maybe it gets paid a wage

930
00:47:43,940 --> 00:47:47,730
and it goes to buy lithium-ion
batteries to keep it going.

931
00:47:47,730 --> 00:47:49,940
Why would that have to happen?

932
00:47:49,940 --> 00:47:51,560
I mean, if you're having a physical body

933
00:47:51,560 --> 00:47:53,330
then you will have a lot of physical needs

934
00:47:53,330 --> 00:47:54,383
just like we have.

935
00:47:56,720 --> 00:47:59,200
You might have to go to the repair shop

936
00:47:59,200 --> 00:48:01,430
to get like motor looked
at or something like that

937
00:48:01,430 --> 00:48:03,190
and they'll have to
pay someone to do that.

938
00:48:03,190 --> 00:48:05,290
I think they'll just be
contributing to our economy

939
00:48:05,290 --> 00:48:06,583
in the same way we do.

940
00:48:08,890 --> 00:48:12,120
And if they have brains
like us, they'll want to

941
00:48:12,120 --> 00:48:14,100
explore new things
they've never seen before,

942
00:48:14,100 --> 00:48:17,850
they'll want to learn things,
they'll want to perhaps rest

943
00:48:17,850 --> 00:48:20,040
so that their mind has time to consolidate

944
00:48:20,040 --> 00:48:21,758
all this new information.

945
00:48:21,758 --> 00:48:23,200
I'm trying to picture it in my head

946
00:48:23,200 --> 00:48:24,980
this little robot worker.

947
00:48:24,980 --> 00:48:29,450
Does he go home and sit on the
couch, watch TV after work?

948
00:48:29,450 --> 00:48:31,100
I don't see why not.

949
00:48:31,100 --> 00:48:33,803
They probably watch cat
videos like the rest of us.

950
00:48:34,820 --> 00:48:36,740
It's hard to tell sometimes if Suzanne

951
00:48:36,740 --> 00:48:38,933
is laughing with us or at us.

952
00:48:41,650 --> 00:48:45,350
But she's not alone in her
cautious optimism for the future.

953
00:48:45,350 --> 00:48:48,930
I think there's always a
sense that technology can be

954
00:48:48,930 --> 00:48:52,930
either used for good or
used for bad unreassured

955
00:48:52,930 --> 00:48:54,980
that Canada is part of it

956
00:48:54,980 --> 00:48:57,673
in terms of trying to
set us on the right path.

957
00:49:01,450 --> 00:49:04,780
On the whole being
responsible and thoughtful

958
00:49:04,780 --> 00:49:09,610
about the power we're gaining
by research and learning

959
00:49:09,610 --> 00:49:11,360
is the right trend line,

960
00:49:11,360 --> 00:49:13,970
and I don't think AI
is automatically doomed

961
00:49:13,970 --> 00:49:15,823
to some dystopian outcome.

962
00:49:21,280 --> 00:49:23,180
We're told that politicians will come up

963
00:49:23,180 --> 00:49:26,180
with policies that
address massive job loss

964
00:49:26,180 --> 00:49:29,403
and prevent horrific
inequality between the classes.

965
00:49:32,790 --> 00:49:35,300
And we're told that these
guys will take so long

966
00:49:35,300 --> 00:49:38,993
to become human-like that we
need not be afraid for a while.

967
00:49:42,940 --> 00:49:45,610
The truth though is that
we're turning ourselves

968
00:49:45,610 --> 00:49:47,540
over to the unknown here.

969
00:49:47,540 --> 00:49:50,543
So, you know, fingers crossed.

970
00:49:52,680 --> 00:49:57,290
Eventually, I think we
will become the AIs.

971
00:49:57,290 --> 00:50:00,830
We will become the intelligent machines.

972
00:50:00,830 --> 00:50:03,010
We will understand how things can be smart

973
00:50:03,010 --> 00:50:04,810
and we can deliberately create them.

974
00:50:05,890 --> 00:50:08,760
So it's you might think of it
as making a new generation,

975
00:50:08,760 --> 00:50:10,063
new kinds of people.

976
00:50:13,753 --> 00:50:16,380
Humanity is continuing to evolve,

977
00:50:16,380 --> 00:50:21,380
and why wouldn't enhanced
people or even design people

978
00:50:21,650 --> 00:50:23,333
be the next step in humanity?

979
00:50:26,070 --> 00:50:28,263
It's really hard to predict the future.

980
00:50:30,160 --> 00:50:32,170
I think there's gonna be all
sorts of things happening

981
00:50:32,170 --> 00:50:36,983
we didn't expect, but there's
one thing that we can predict.

982
00:50:38,000 --> 00:50:40,193
This technology is
gonna change everything.

983
00:50:45,340 --> 00:50:46,980
Good bye.

984
00:50:46,980 --> 00:50:48,250
Good bye.

985
00:50:48,250 --> 00:50:51,250
Good bye.
Good bye.

986
00:50:51,250 --> 00:50:53,643
Once I power you down, that's it.

987
00:50:58,072 --> 00:50:59,822
Yeah, that's right.

988
00:51:02,740 --> 00:51:04,530
We'll end it right there.

989
00:51:04,530 --> 00:51:05,433
That was getting deep.

990
00:51:05,433 --> 00:00:00,000
That was getting really deep.

