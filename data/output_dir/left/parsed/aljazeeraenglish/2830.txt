I can progress and your industry
hi Anthony okay welcome to the stream
everybody I'm Malika be loud if you're
watching us on YouTube it's chat us your
comments for questions we'll try to get
them into the show today killer robot
technology should it be banned we'll
look at the debate over letting weapons
decide when to pull the trigger using
artificial intelligence or AI weapons of
war are being programmed to be smarter
and more independent but should machines
be given the ability to decide when and
who to kill
representatives are meeting this week in
Geneva to consider a pre-emptive ban on
autonomous weapons technology that may
someday power what activists call killer
robots the world's high tech militaries
including the US Russia and Israel are
all eagerly pursuing AI applications but
is there a line developers shouldn't
cross who in creating weapons designed
to think for themselves well with us to
talk about this in London Alka Schwartz
she's a member of the International
Committee for robot arms control or I
crack she lectures at the University of
Leicester and is the author of a soon to
be released book death machines the
ethics of violent technologies in Geneva
Paul Shari he has senior fellow and
director of the technology and national
security program at the Center for a new
American security and also author of a
forthcoming book army of none autonomous
weapons and the future of war and in San
Francisco Alex Sal Kiba he is a writer
and futurist and the co-author of the
driver in the driverless car welcome to
all of our guests we could have a book
club with just you three so good to have
him here so we asked our community their
thoughts on laws which is lethal
autonomous weapons systems and what some
are calling killer robots this is what
one person tweeted us back and that's
because this is what's in the public
imagination it's a gift and it's from
the Netflix drama black mirror and it
depicts a robot designed to kill this
robot and
fact is taking a knife and is going to
pursue a woman but Paul how far from
reality is this what's your definition
of a lethal autonomous weapon the kinds
of intelligent robots that are in that
black mirror episode which is a great
episode all are quite a ways away still
they're still in science fiction it
would be possible to build weapons today
that hunted military targets things like
radars that are emitting in the
electromagnetic spectrum would be easier
to distinguish particularly if you're
flying through the air or at sea where
there's not a lot of other objects or
civilians in a way I guess I'm just
curious about this phrase killer robots
when I say it I don't know if it's a cry
or to laugh I have to exact opposite
emotions Alex this term is it helpful to
us to understand exactly what we're
talking about that's all right are you
feeling good shape I think it's a good
term because it's very easy for the
general public to get the concept of
what we're talking about we just robots
to kill people I'm just gonna give you a
beat catch a breath okay list this
phrase killer robots yeah I think it's a
useful term against sanitizing what the
technology could potentially do which is
kill so it might be slightly over drawn
or it might appear to be slightly over
drawn and of course it in books always
an idea the image of terminator but I
think it's useful to have a bucket of
terminology in order to also bring it
home to the general public what the
signal adji could potentially be all
about so I think it's quite a useful
term is it technically accurate it's a
different story but I think it's a
useful term i'm just spelling out so
everybody understands for the rest of
this conversation a killer robot or a
lethal autonomous weapon system is a
weapon that can make a decision about a
kill without human necessarily being
involved in that way
that weapon Cahill by itself right so
the controversial technology that is
being debated as lethal autonomous
weapon systems and the function that is
most controversial are the critical
functions which means engaging selecting
a target and engaging a cut a target
meaning finding somebody and killing
somebody just be good bluntly and also
just interject there could be situations
where humans may be killed because the
system is doing its job recognizing an
image or a footprint so for example what
Paul's talking about if there's a human
in the radar station they would die if
it's struck or if a AI is programmed to
recognize a tank as a Russian style tank
in a field of battle and that is a
target there are humans in the tanked
and they could die so there's sort of
these gradations along the way it's not
just find you know human identify them
kill them when you're talking about the
gradiation Zion here you Paul I'll give
this to you
Alex mentions the gradations I get what
you're saying but I want to make sure
our audience is on the same page so one
person sent us an example and I don't
want to hear what you think about it t V
Gorgon says when that man in Dallas
killed four cops they blew him up with a
robot so this is clearly real and
happening now and what he is referencing
is something that happened in 2016 this
is from The Guardian use a police robot
to kill Dallas shooting suspect believed
to be first in US history and it was the
lethal use of a bomb disposal robot is
that what we're talking about her is it
slightly different because the robot
itself didn't decide to deliver that
bomb precise yeah well but did not did
not make the kill decision all that Paul
talk on that mm-hmm it's a very
different I mean there are 16 countries
today that already have weaponized
robots and a number of non-state groups
so the debate going on internationally
about autonomous weapons is really
looking forward into the future and
saying as LK described what happens when
the robots themselves are making the
decisions and so it's not people killing
other people with robots it's people
launching robots and then the robots are
making these decisions there is some
debate over whether we're talking about
robots that are targeting people or
military objects and depending on who
you talk to
very different views about you know
what's acceptable I want to share with
people because we're not in science
fiction world right now
there's a story that you put out in the
Wall Street Journal poor and you say
meet the new robot army intelligent
machines that could usher in an era of
autonomous warfare are already here I
want to give people a couple of examples
these are illustrations but these are
are pieces of weaponry that are being
used right now the you ran nine what's
the you ran nine what does that do Paul
so it's the euro nine is a Russian
ground combat vehicle that is entirely
robotic it is equipped with a machine
gun and anti-tank rockets
so it's you know designed to go
toe-to-toe with other tanks and then
blow them up and there'd be no one on at
all inside it now what's not clear is
who's pulling the trigger is there a
human remotely deciding and that is
really the essence of the question with
an autonomous weapon or is the robot
deciding on its own we don't know you
can look at it but you can't see from
the outside what software has and how
its thinking okay let me do another one
here this is interesting this takes us
into the oceans here scroll down a
little bit this is a hunter who's the
sea hunter yeah this is a US Navy ship
that's totally unmanned on the surface
that's designed to hunt for enemy
submarines right and it's currently
unarmed there's no weapons but the US
has talked about putting missiles on it
in the future so this is actually out
there why are we getting illustrations
pull and not the real thing I think if
it's so secret I could show you but then
I'd have to kill you look one more here
this is a long-range anti-ship missile
yes so this is the long-range missile
it's also called el resum it's a sort of
on the cutting edge of intelligent
missiles today yeah a human still
decides the target right there you see
okay yeah a human said I'm gonna take
out that ship yeah but the missile has a
lot of autonomy in how it gets to the
ship and to navigate all on its own so
what I understand Paul because I read
your article very carefully
is that if it sees obstacles on its way
it's not gonna do a little detour and
blow up something else it's going to
make a decision and then keep going on
to its target and the missile was gonna
do that it's gonna be thinking that's
right
and then missile has the ability all on
its own to avoid other threats that
might be in its way so if it sees a ship
between on their way to a target that it
uses a threat it'll is view around it to
stay on course to its target if it works
as designed which is question a couple
of people are bringing up what happens
when they don't and who then takes
responsibility for that so I want to
play a video comment from Thompson Chang
Guetta and he talks about some of the
legal implications behind this and ok
I'll direct this to you have listened
where killer robots are used we are
likely to have an accountability gap
there will be no human responsibility
for use of weapons this is an affront to
the recognized principle under
international human rights law that
victims have a right to remedy a right
to remedy includes prosecution of the
perpetrator in the National Human
Terrain legal norms are timeless in this
case it is not the law that has to keep
up with the technology it is the
technology that should keep up or
conform with the law so ok who is
responsible if the robot makes a mistake
yes well that's the million-dollar
question really and I wholeheartedly
agree with Thompson's comment in the in
the video clip here the problem really
is that when you have technology where
you don't exactly know where the
decision is made or how that isn't it is
made who can be accountable and who can
feel responsible about it so it is not
as easy as it would be with a ok I'm
using a tool and I'm actually actively
pulling the trigger I am therefore
responsible there's a whole decision
mechanism which cannot necessarily be
attributed to a single person or a unit
but rather could be happening or the
decision could be happening
by programming through an operator and a
whole system of participants so putting
your finger on accountability or
responsibility in the first place and
then legal accountability is really
difficult with those systems and that is
obviously a huge problem for existing
frameworks I like to nodding I'd also
add that countermeasures are something
that we've seen in military systems
forever they will be deployed against
the AI systems as well to potentially to
really tragic results so I mean where he
already sees systems or situations where
non-state actors use human shields to
hide military targets behind them you
could see that as the reverse for
political means meaning that they make a
school bus look like a tank to draw AI
fire additionally there's a whole field
of computer science now in a I called
adversarial that adversarial adversarial
attacks where you make things appear to
be something that they're not in order
to fool the AI so I mean as we strip
humans out of the decision loop and the
time between essentially the decision
being made and the impact happening or
the trigger being pulled the bomb
blowing up goes to nearly zero the time
to correct these kind of errors shrinks
and becomes negligible so the problem
here is the accountability gap is is a
risk it could happen but there's
actually no principle in the laws of war
that says you have to be able to hold an
individual person accountable that's
actually not a thing it's appealing
personally I think to many people to say
well who's responsible for this but
there's nothing in the laws of war that
says you have to have that and there are
accident that that happened today with
people with humans sure but there is
something really profoundly morally
troubling about not having anybody be
accountable specifically when it comes
to kill decisions specifically when it
comes to decisions to eliminate a human
being or an entire group of human beings
so saying that okay well this may happen
this may not happen it happens in other
contexts it's not really helpful to
unpack or perhaps even limit what is
quite clearly an accountability gap with
systems in which a decision may be
pre-programmed in which
might be taken sorry their accident no
I'm actually describing countermeasure
it's like if a terrorist group decided
to pull an AI and instead ended up
blowing up a school and again those
things happen today they do in with the
AI it's going to be much more difficult
and much more challenging and mean that
most of what's happening with AI we make
it sound like it's some proprietary
magic technology it's actually all
open-source software and the compute
necessary to run AI is shrinking and
shrinking AI will be something that
every actor can use that is able to
here's another problem
we're just algorithmic bias or bias in
artificial intelligence systems and this
is something that the entire artificial
intelligence community knows do you want
to just stop do you want to just give an
example for us a very quick vivid
example for what you mean by bias okay
so my prime example always for
algorithmic bias is something that we're
all familiar with but if you go into
your Google search engine and you type
in CEO and this is an example of Kate
Crawford uses when she talks about
algorithmic bias and you type and see
what you will find is usually usually
largely white middle-aged men there's
the odd woman there's Barbie as a CEO
but that's sort of scattered if you take
a beautiful woman it's all blonde women
too so we won't stop and ask how does
Alex know that I've written I've
actually written about algorithmic fire
all right yes your studies have saved
you okay continue yes so the way things
are labeled in order to be able to be
processed through algorithms is by no
means clear-cut right the data that you
feed in the training set that you have
these are all decisions that I made
beforehand and they may not necessarily
represent the entire diversity of the
population you're dealing with and so
once that is in the system it's very
difficult to untangle or disentangle you
know what is already data that is coming
in as biased and what comes out so you
say okay you're basically saying we
could end up with racist weaponry
I hear you there Alex but okay I'm glad
that you brought that up because we got
this question specifically on that case
it's what human biases will be
programmed into the robot she's assuming
that they already will be but of course
we're all biased so that is a good
assumption but I want to shift just a
little bit here to bring up this tweet
from Anthony who says the list of
countries who wants to ban them is the
same as the list of countries that can't
make them be careful what you're
cheering for it may not exactly be
accurate but Paul his point there I
think is well-taken and you being
someone who actually has been in the
arena of war you were in the military
what do you make of being on a
battlefield when automated weapons are
either alongside you on your side or on
the opposing side when we look at and I
think it's a great point when you look
at the list of countries that have said
they support a ban
none of them are leading military
developers and what's driving for many
of them their desire to support a ban is
not humanitarian concerns it's politics
now the NGOs like Iraq and others
they're motivated by the humanitarian
concern but for many of these countries
they don't have to know what autonomous
weapons are to know that they're against
them because they know they're not the
ones building them and when you look at
some of the lists a lot of them are not
leaders in global Human Rights and so
you know internationally ultimately what
drives the conversation is politics
among nations I'm just curious about
this list is boil this down to basics
the fact that you go to war using a
machine or using some weaponry that's
making its own decisions okay why is
that worse than going to war without
that because I think when we go to war
we have to think about how to achieve
peace so there should be a reluctance
towards violence or use the use of
violence it should not but we're ready
okay that's not even really that's not a
realistic I mean that's a historians
perspective know at the end of this war
that were already in that there's going
to be peace negotiations but we're in it
right now right and we're killing people
why why would you be more upset about
how
you kill somebody because certain
technologies I fear lower the threshold
for the use of violence so there's more
violence and there's plenty of
scholarship that shows that more
violence is not likely to lead to a more
peaceful context down the road in fact
there's an escalatory dimension to that
so as you lower the threshold for the
use of violence and as you feel or as
the as the current thinking becomes or
thinking becomes okay we can have this
technology where we can perhaps engage
more risk free in the application of
violence I think a distortion takes
place in terms of how or the dimensions
that are necessary to solve the
conflicts so okay there's there's an
important difference there someone who
agrees to the L key here that I just
want to get them and this is live on
YouTube Tom sighs wouldn't killer robots
desensitize humans do the already
atrocious acts that they would be
committing so picking up on your point
there but on the other side Alex this
person says having killer robots do the
fighting would keep soldiers and police
officers out of harm's way
Alex so what I was going to point out is
that emotionally I kind of want to agree
with what else you saying but if you
look historically as we've injected more
and more technology into warfare the
levels of casualties and violence and
incidents of warfare has actually gone
down I mean at least that's according to
what Steven Pinker writes about and you
know angels in the greater nature and
you know we don't know enough
necessarily about AI to think to be sure
that it will be worse to have ai ai than
better and I'll give a specific example
I mean if there was a way to codify an
AI not to kill civilians or to recognize
children very clearly AI it does a much
better job of this kind of stuff over
time in the heat of battle and it also
doesn't get tired or snappy or angry it
doesn't take drugs it isn't drunk you
know it won't make the kind of human
errors so I'm kind of on the fence both
ways but that's like the counter it also
can't have mercy it can't it can't
surprise you with with you know
spontaneous humanitarian acts and
helping could you program it to have
mercy could you do an algorithm
that was a merciful algorithm and what
about the sexual violence no sexual
violence in the field either
sure absolutely I mean there's
horrendous things human soldiers do
there's absolutely no doubt about that
humans are entirely naturally fallible
and there's terrible things being done
um and I wouldn't by any stretch of the
imagination try to mitigate that but we
also have to consider the fact that
humans can also do very positive things
very good things this there's empathy
this compassion
there are ways of acting human
relationships human relations that I
think are important too mmm not do away
with for the sake of technology I don't
think you can you can code you can code
laws or rules or ethical guidelines into
a machinery in some way in some form or
another but that relies on some sort of
quantification of what you think is good
and what is bad and everything that you
cannot mathematically quantify ie
program into a system or code then kind
of falls by the wayside poor 19 I think
there are ways that you know I do you
think that there are ways that we can
find where's the balance both of these
concerns to use the technology in ways
that reduce civilian casualties and
reduce the risk to soldiers but also
hold on to our humanity you know one of
the things that people mix up a lot of
times is the value of physical robots
and giving soldiers or police officers
more stand off from threats and then
autonomous weapons that would make their
own decisions and I don't like the term
killer robots and I think it mixes up
all of these important differences among
the technology if you could increase the
physical distance so people aren't at
harm's way and they don't have to make a
decision in the moment do I shoot right
now to defend myself that could reduce
harm in the warfare and would be a good
thing there might be other things we
could use AI to make factual decisions
is this person holding a rifle or are
they holding a rake and we can do that
we can do that with AI today I agree
with okay we probably still want to find
ways to use this that we we hold on to
our humanity we don't get to a place
where humans no longer care about what
engine war I think that would be
terrible so Paul it sounds like you're
saying there is the middle ground here
there is a way to make this work but I
bring John sweet in here
he says lethal automated weapon systems
are problematic because it's uncertain
that their automated intelligence
algorithms are capable of meeting the
international humanitarian law
requirements and so Alex actually give
this one to you because right now we
have state actors who don't meet
international human humanitarian law
requirements what is it about these
robots that means they might be
different is there a way to program them
so that they do in theory you could
program to meet any requirement as lq
said if it can be reduced to mathematics
of some sort a code of conduct then you
can program to meet that code of conduct
there'll always be edge cases because
the world is very messy but at the same
time it certainly is easier to encode
that type of thing in in you know in the
computer code than it is to figure out
how it works in our brains at this point
but one other point I'd sort of make
around this is that whether we like it
or not we're going to have to fund a lot
of research around this even if we
abandoned because we're gonna have to
have the capability for countermeasures
should someone else use it or just to be
a ability to deal with it in case it
gets out of Pandora's box I just want to
do a lightning round in the 60 seconds
we have left in ten years time will we
have killer robots
Alex someone's going to build them on
someone won't care about international
humanitarian law whether it's terrorists
or what you knew rogue actors like
Bashar al-assad who's terrorizing
civilians like someone will be
understanding it someone's already doing
it right now okay when we have them and
we probably does somebody will probably
build them but I would really wish that
we have some sort of robust
international legal framework get a plan
or some sort of Norma stablished that
they use them okay for cool functions
Alex it's been a pleasure wrestling with
this ethical conversation Malika
Sarah Rasheed's here says she doesn't
think there's a willingness by
governments to ban the use of AI robots
and it would be the equivalent of
governments turning in their guns and
promoting world peace thanks for
watching the stream you'll find Malika
myself online at hashtag AJ stream see
you next time
you
