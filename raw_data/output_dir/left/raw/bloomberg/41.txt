1
00:00:00,371 --> 00:00:03,038
(playful music)

2
00:00:05,020 --> 00:00:07,330
This is Geoff Hinton.

3
00:00:07,330 --> 00:00:09,910
Because of a back condition,
he hasn't been able

4
00:00:09,910 --> 00:00:12,923
to sit down for more than 12 years.

5
00:00:14,160 --> 00:00:15,940
I hate standing, I would
much rather sit down,

6
00:00:15,940 --> 00:00:18,070
but if I sit down I have
a disc that comes out.

7
00:00:18,070 --> 00:00:19,080
So.
Okay.

8
00:00:19,080 --> 00:00:21,590
Well, at least now standing
desks are fashionable and--

9
00:00:21,590 --> 00:00:22,882
Yeah, but I was ahead.

10
00:00:22,882 --> 00:00:23,880
(laughs)

11
00:00:23,880 --> 00:00:26,130
I was standing when they
weren't fashionable.

12
00:00:27,395 --> 00:00:30,093
Since he can't sit in a car or on a bus,

13
00:00:30,093 --> 00:00:33,910
Hinton walks everywhere.

14
00:00:33,910 --> 00:00:36,577
(playful music)

15
00:00:39,560 --> 00:00:43,100
The walk says a lot about
Hinton and his resolve.

16
00:00:43,100 --> 00:00:45,520
For nearly 40 years,
Hinton has been trying

17
00:00:45,520 --> 00:00:48,350
to get computers to learn like people do.

18
00:00:48,350 --> 00:00:50,780
A quest almost everyone thought was crazy,

19
00:00:50,780 --> 00:00:52,330
or at least hopeless.

20
00:00:52,330 --> 00:00:55,463
Right up until the moment
it revolutionized the field.

21
00:00:56,820 --> 00:00:58,860
Google thinks this is the
future of the company.

22
00:00:58,860 --> 00:01:00,922
Amazon thinks this is the
future of the company.

23
00:01:00,922 --> 00:01:02,480
Apple thinks it's future of the company.

24
00:01:02,480 --> 00:01:04,900
My own department thinks this
stuff's probably nonsense

25
00:01:04,900 --> 00:01:06,609
and we shouldn't be doing any more of it.

26
00:01:06,609 --> 00:01:07,825
(laughs)

27
00:01:07,825 --> 00:01:11,572
So, I talked everybody into
it except my own department.

28
00:01:11,572 --> 00:01:14,239
(playful music)

29
00:01:19,750 --> 00:01:21,750
You obviously grew up in the UK,

30
00:01:21,750 --> 00:01:23,870
and you had this very prestigious family

31
00:01:23,870 --> 00:01:27,080
full of famous
mathematicians and economists

32
00:01:27,080 --> 00:01:29,382
and, I was curious what
that was like for you.

33
00:01:29,382 --> 00:01:31,755
Yeah, there was a lot of pressure.

34
00:01:31,755 --> 00:01:34,740
I think by the time I was about seven,

35
00:01:34,740 --> 00:01:36,978
I realized I was gonna have to get a Ph.D.

36
00:01:36,978 --> 00:01:38,570
(laughing)

37
00:01:38,570 --> 00:01:40,000
Did you rebel against that?

38
00:01:40,000 --> 00:01:41,410
Or you went along with it?

39
00:01:41,410 --> 00:01:42,710
I dropped out every so often.

40
00:01:42,710 --> 00:01:44,360
I became a carpenter for a while.

41
00:01:46,620 --> 00:01:48,959
Geoff Hinton pretty
early on became obsessed

42
00:01:48,959 --> 00:01:53,303
with this idea of figuring
out how the mind works.

43
00:01:54,878 --> 00:01:57,470
He started off getting into physiology,

44
00:01:57,470 --> 00:01:59,440
the anatomy of how the brain works,

45
00:01:59,440 --> 00:02:02,505
then he got into psychology,
and then finally,

46
00:02:02,505 --> 00:02:05,720
he settled on more of a
computer science approach

47
00:02:05,720 --> 00:02:09,683
to modeling the brain, and got
into artificial intelligence.

48
00:02:10,919 --> 00:02:13,530
My feeling is, if you want to understand

49
00:02:13,530 --> 00:02:16,230
a really complicated device like a brain,

50
00:02:16,230 --> 00:02:18,120
you should build one.

51
00:02:18,120 --> 00:02:19,240
I mean, you can look at cars,

52
00:02:19,240 --> 00:02:20,870
and you could think you
could understand cars.

53
00:02:20,870 --> 00:02:22,961
When you try to build a
car, you suddenly discover

54
00:02:22,961 --> 00:02:25,230
then there's this stuff that
has to go under the hood,

55
00:02:25,230 --> 00:02:26,190
otherwise it doesn't work.

56
00:02:26,190 --> 00:02:28,490
Yeah. (laughs)

57
00:02:28,490 --> 00:02:31,190
As Geoff was starting to
think about these ideas,

58
00:02:31,190 --> 00:02:35,192
he got inspired by some AI
researchers across the pond.

59
00:02:35,192 --> 00:02:38,633
Specifically, this guy: Frank Rosenblatt.

60
00:02:39,730 --> 00:02:42,860
Rosenblatt, in the the late 1950s,

61
00:02:42,860 --> 00:02:45,470
developed what he called a perceptron,

62
00:02:45,470 --> 00:02:49,240
and it was a neural
network, a computing system

63
00:02:49,240 --> 00:02:51,303
that would mimic the brain.

64
00:02:52,940 --> 00:02:55,330
The basic idea is a collection

65
00:02:55,330 --> 00:02:57,480
of small units, called neurons.

66
00:02:57,480 --> 00:02:59,010
These are little computing units,

67
00:02:59,010 --> 00:03:01,110
but they're actually modeled on the way

68
00:03:01,110 --> 00:03:03,383
that the human brain
does it's computation.

69
00:03:04,350 --> 00:03:07,620
They take their incoming data
like we do from our senses,

70
00:03:07,620 --> 00:03:09,810
and they actually learn, so the neural net

71
00:03:09,810 --> 00:03:12,123
can learn to make decisions over time.

72
00:03:14,873 --> 00:03:16,980
Rosenblatts's hope was that you could feed

73
00:03:16,980 --> 00:03:19,040
a neural network a bunch of data,

74
00:03:19,040 --> 00:03:21,020
like pictures of men and women,

75
00:03:21,020 --> 00:03:23,756
and it would eventually
learn how to tell them apart.

76
00:03:23,756 --> 00:03:25,333
Just like humans do.

77
00:03:28,083 --> 00:03:32,445
There was just one problem:
it didn't work very well.

78
00:03:32,445 --> 00:03:36,226
Rosenblatt, his neural
network was the single layer

79
00:03:36,226 --> 00:03:40,710
of neurons, and it was
limited in what it could do.

80
00:03:40,710 --> 00:03:42,630
Extremely limited.

81
00:03:42,630 --> 00:03:46,870
And a colleague of his
wrote a book in the late 60s

82
00:03:46,870 --> 00:03:48,823
that showed these limitations.

83
00:03:50,682 --> 00:03:54,200
And, it kind of put the
whole area of research

84
00:03:54,200 --> 00:03:56,943
into a deep freeze for a good 10 years.

85
00:03:56,943 --> 00:03:59,090
No one wanted to work in this area.

86
00:03:59,090 --> 00:04:00,840
They were sure it would never work.

87
00:04:01,810 --> 00:04:03,933
Well, almost no one.

88
00:04:04,930 --> 00:04:06,538
It was just obvious to me that everything

89
00:04:06,538 --> 00:04:07,970
was about ready to go.

90
00:04:07,970 --> 00:04:09,820
The brain's a big neural network,

91
00:04:09,820 --> 00:04:12,629
and so, it has to be that
stuff like this can work,

92
00:04:12,629 --> 00:04:14,291
because it works in our brains.

93
00:04:14,291 --> 00:04:16,427
There's just never any doubt about that.

94
00:04:16,427 --> 00:04:19,250
And what do you think
that it was inside of you

95
00:04:19,250 --> 00:04:21,610
that kept you wanting to pursue this

96
00:04:21,610 --> 00:04:22,680
when everyone else was giving up?

97
00:04:22,680 --> 00:04:25,118
Just, that you thought it was
the right direction to go?

98
00:04:25,118 --> 00:04:26,590
No, that everyone else was wrong.

99
00:04:26,590 --> 00:04:27,777
Okay.

100
00:04:27,777 --> 00:04:29,982
(laughs)

101
00:04:29,982 --> 00:04:31,460
(upbeat music)

102
00:04:31,460 --> 00:04:33,910
Hinton decides he's got an idea

103
00:04:33,910 --> 00:04:35,990
of how these neural nets might work,

104
00:04:35,990 --> 00:04:38,140
and he's going to pursue
it no matter what.

105
00:04:39,610 --> 00:04:41,730
For a little while, he's bouncing around

106
00:04:41,730 --> 00:04:43,852
research institutions in the US.

107
00:04:43,852 --> 00:04:46,600
He kind of gets fed up that most of them

108
00:04:46,600 --> 00:04:48,520
were funded by the Defense Department,

109
00:04:48,520 --> 00:04:51,890
and he starts looking for
somewhere else he can go.

110
00:04:51,890 --> 00:04:54,600
I didn't want to take
Defense Department money.

111
00:04:54,600 --> 00:04:56,530
I sort of didn't like
the idea that this stuff

112
00:04:56,530 --> 00:04:58,470
was going to be used for purposes

113
00:04:58,470 --> 00:05:00,480
that I didn't think were good.

114
00:05:00,480 --> 00:05:03,460
He suddenly hears that
Canada might be interested

115
00:05:03,460 --> 00:05:05,914
in funding artificial intelligence.

116
00:05:05,914 --> 00:05:08,080
And that was very attractive,

117
00:05:08,080 --> 00:05:10,340
that I could go off to
this civilized town,

118
00:05:10,340 --> 00:05:12,090
and just get on with it.

119
00:05:12,090 --> 00:05:14,033
So I came to the University of Toronto.

120
00:05:15,210 --> 00:05:16,810
And then in the mid-80s, we discovered

121
00:05:16,810 --> 00:05:18,530
how to make more complicated neural nets

122
00:05:18,530 --> 00:05:20,410
so they could solve those problems

123
00:05:20,410 --> 00:05:22,412
that the simple ones couldn't solve.

124
00:05:22,412 --> 00:05:24,700
He and his collaborators developed

125
00:05:24,700 --> 00:05:29,152
a multi-layered neural
network, a deep neural network.

126
00:05:29,152 --> 00:05:33,390
And this started to work in a lot of ways.

127
00:05:33,390 --> 00:05:35,610
Using a neural network, a guy named

128
00:05:35,610 --> 00:05:39,770
Dean Pomerleau built a
self-driving car in the late 80s.

129
00:05:39,770 --> 00:05:42,240
And it drove on public roads.

130
00:05:42,240 --> 00:05:45,100
Yann LeCun, in the 90s, built a system

131
00:05:45,100 --> 00:05:47,670
that could recognize handwritten digits,

132
00:05:47,670 --> 00:05:50,083
and this ended up being used commercially.

133
00:05:51,060 --> 00:05:52,593
But again, they hit a ceiling.

134
00:05:53,979 --> 00:05:56,260
(upbeat music)

135
00:05:56,260 --> 00:05:57,720
It didn't work quite well enough,

136
00:05:57,720 --> 00:05:58,950
because we didn't have enough data,

137
00:05:58,950 --> 00:06:01,170
we didn't have enough compute power.

138
00:06:01,170 --> 00:06:04,730
And people in AI and computer science,

139
00:06:04,730 --> 00:06:05,870
decided that neural networks

140
00:06:05,870 --> 00:06:08,210
were wishful thinking, basically.

141
00:06:08,210 --> 00:06:09,810
So, it was a big disappointment.

142
00:06:12,118 --> 00:06:14,399
Through the 90s, into the 2000s,

143
00:06:14,399 --> 00:06:18,144
Geoff was one of only a
handful of people on the planet

144
00:06:18,144 --> 00:06:20,653
who were still pursuing this technology.

145
00:06:22,630 --> 00:06:24,830
He would show up at academic conferences

146
00:06:24,830 --> 00:06:27,040
and be banished to the back rooms,

147
00:06:27,040 --> 00:06:29,763
he was treated as, really like a pariah.

148
00:06:30,845 --> 00:06:32,922
Was there like a time when you thought

149
00:06:32,922 --> 00:06:34,470
this just wasn't going to work?

150
00:06:34,470 --> 00:06:36,960
And you had some self-doubt?

151
00:06:36,960 --> 00:06:39,087
I mean there were many
times when I thought,

152
00:06:39,087 --> 00:06:40,953
"I'm not going to make this work."

153
00:06:40,953 --> 00:06:42,560
(laughs)

154
00:06:42,560 --> 00:06:45,183
But Geoff was consumed by
this and couldn't stop.

155
00:06:46,390 --> 00:06:48,700
He just kept pursuing the idea

156
00:06:48,700 --> 00:06:50,480
that computers could learn.

157
00:06:50,480 --> 00:06:54,150
Until about 2006, when
the world catches up

158
00:06:54,150 --> 00:06:56,220
to Hinton's ideas.

159
00:06:56,220 --> 00:06:58,803
(upbeat music)

160
00:07:00,620 --> 00:07:02,960
Computers are now a lot faster.

161
00:07:02,960 --> 00:07:04,600
And now, it's behaving like I thought

162
00:07:04,600 --> 00:07:06,050
it would behave in the mid-80s.

163
00:07:06,050 --> 00:07:07,910
It's solving everything.

164
00:07:07,910 --> 00:07:10,070
The arrival of super-fast chips,

165
00:07:10,070 --> 00:07:13,190
and the massive amounts of
data produced on the internet

166
00:07:13,190 --> 00:07:16,829
gave Hinton's algorithms a magical boost.

167
00:07:16,829 --> 00:07:21,700
Suddenly, computers could
identify what was in an image.

168
00:07:21,700 --> 00:07:23,735
Then, they could recognize speech

169
00:07:23,735 --> 00:07:26,633
and translate from one
language to another.

170
00:07:27,700 --> 00:07:31,450
By 2012, words like neural
nets and machine learning

171
00:07:31,450 --> 00:07:33,180
were popping up on the front page

172
00:07:33,180 --> 00:07:35,120
of the New York Times.

173
00:07:35,120 --> 00:07:36,650
You have to go all these years,

174
00:07:36,650 --> 00:07:40,520
and then all of a sudden, in
a the span of a few months,

175
00:07:40,520 --> 00:07:41,560
it just takes off.

176
00:07:41,560 --> 00:07:43,106
Did it finally feel like aha,

177
00:07:43,106 --> 00:07:46,510
the world has finally come to my vision?

178
00:07:46,510 --> 00:07:48,090
It was sort of a relief that people

179
00:07:48,090 --> 00:07:49,482
finally came to their senses.

180
00:07:49,482 --> 00:07:50,960
(laughs)

181
00:07:50,960 --> 00:07:53,970
(gentle music)

182
00:07:53,970 --> 00:07:57,157
For Hinton, this was
clearly a redemptive moment

183
00:07:57,157 --> 00:07:59,023
after decades of toil.

184
00:08:00,780 --> 00:08:04,063
And for Canada, it meant
something even bigger.

185
00:08:05,810 --> 00:08:08,241
Hinton and his students
put the country on the map

186
00:08:08,241 --> 00:08:10,163
as an AI superpower,

187
00:08:11,150 --> 00:08:14,370
something no one, and no computer,

188
00:08:14,370 --> 00:00:00,000
could ever have predicted.

