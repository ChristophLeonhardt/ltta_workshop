in a new effort to win back trust of
users Facebook is giving a peek behind
the curtain revealing many of the rules
moderators use to review content on
their platform the company says the
guidelines are a living document being
edited as needed but the shadow of
high-profile missteps still looms large
something founder Mark Zuckerberg noted
in a hearing before Senators earlier
this month in retrospect I think we
clearly view it as a mistake that we
didn't inform people and we did that
based on false information that we
thought that the case was closed and
that the data had been deleted and
knowing what we know now we should have
handled a lot of things here differently
privacy issues are nothing new for
Facebook the social media giant faced
major backlash from users since its
newsfeed debuted in 2006 and users trust
in the company it's complicated in 2011
Facebook faced an eight-count complaint
from the Federal Trade Commission the
FTC said the company mishandled private
data and engaged in deceptive practices
the company entered an agreement with
the FTC that included financial
penalties if it violated the decree in
2014 researcher Alexandre Cogan
unleashed his now infamous personality
testing app this is your digital life on
Facebook
two hundred and seventy thousand users
took the quiz but data was captured from
their friends lists as well the data was
allegedly sold to political consulting
firm Cambridge analytics giving it
micro-targeting information on millions
of Americans that same year Facebook
revealed it conducted a study on the
emotions of almost 700 thousand users
the study looked into happiness levels
depending on if they were shown positive
or negative content users did not know
they were part of the emotion study and
did not give consent almost a full year
after the 2016 presidential election
Facebook admitted ready for this as many
as 126 million users may have been
exposed to Russian political ads and
disinformation during the election cycle
Facebook later released the ads in
identified as Russians linked to
Congress
last month now new reporting found that
up to 87 million users had their data
scraped by Alexander Cogan's app nearly
71 million of them were from Americans
the vast majority of them never gave
consent for the app to collect their
information since only 1/3 of 1% even
took the quiz to capture their data the
company hopes that its new transparency
push helps rebuild the trust of its two
billion active monthly users earlier
today I spoke with Monika bicker at the
company's vice president for product
policy and counterterrorism Monica we've
just gone through sort of the string of
what Facebook has been through so now
today how do we trust you you say you're
protecting our information how do we
know that that's true
well the standards that we have put out
are designed to keep our community safe
and I want to be clear that we've had
these standards for years but what we're
doing now is we are publicizing and
making public the details on how we
enforce those standards so for years
you've seen that we don't allow hate
speech and we don't allow harassment now
you can come to the site you can click
on those sections in our standards and
you can see exactly how we define those
things and the content decisions that we
make once something is up there it's up
there and it might not be hateful but it
could be a lie and that's where we get
to fake news and standards and practices
so someone might not flag it there might
not be anything offensive written but
it's dangerous if it's looks like a news
story if there's pictures of kids there
if it's nice moms who wrote it when in
fact it's not well there's two different
issues there and and one is what we do
around fake news and the other is how do
we become aware of content if our
community doesn't report it to us on the
second question technology is getting
better at helping us identify posts that
might violate our standards for instance
now when it comes to terrorism
propaganda more than 99% of the content
that we remove from Facebook for
violating our terrorism standards we
identify ourselves
and that's primarily through the use of
technology we're removing the fake
accounts that tend to share fake news
and we're also removing content that we
see that is shared for economic
incentives like like most fake news is
this is a sort of content that leads off
site to ad farms for instance and we can
remove it the other thing that we're
trying to do is give people context if
we have indications that a news story on
Facebook might be false we're providing
related articles from around the
internet and we're sure who's put that
story on the site where do you get those
indication will can report stories
people can report stories to us as fake
news through the site and we work with
third party fact-checking organizations
as far as sharing our information now we
have the option to we can opt out
why wouldn't opt-in be the option we go
on it it's we've already opted in the
default is that we've opted in to share
our private it is opt-in it is opt-in so
if you've chosen to use an application a
third party application through Facebook
that's a decision that you make and then
what you'll see let's say that you
decide to install a third party app and
authorize that through Facebook it will
say this developer would like and then
it will tell you the information the
developer once so it might say this
developer wants or this this app wants
your hometown or pages you have liked or
your photos and then you can choose
whether or not you want to share that
information or not do you opt in on your
page I do use the maps on Facebook so
you you allow Outsiders to get your
private information I do have some
applications that I have authorized
using Facebook which means I get a
notification that says this developer
would like to have your hometown and I
click whether or not I want them to have
that information and then I could use
the app just as if I've logged into it
creating my own email for the
application would you categorize
Facebook as a media company
it's certainly a place where people come
to get news that's why that section is
in the community standards but it's a
lot more than that I mean a lot of
people that come to Facebook are coming
because they want to connect with their
family and their friends and if you look
at the standards you'll see that the
topics we cover there like hate speech
like harassment like bullying have a lot
to do with those types of personal
interactions should you have the same
type of regulation as traditional media
companies I mean when you think about
advertising that's how you make your
money that's how we make our money and
we're competing for the same advertising
dollars so if one out of every five
advertising dollar goes to Facebook if
we're directly in competition shouldn't
you face the same regulation I do well
there's certainly regulations around the
world that do govern our site and how
about my entrance regulations also but
sometimes those regulations also cover
the type of speech that were allowed to
have on the site you'll see some of that
reflected in the standards for the
Cambridge analytical breach I know you
want to do better but should there be
ramifications a punishment of fine we
want to do what's right for our
community we're waiting to see what the
results of the investigation in the UK
will be but what's important to us is
making sure that we get it right
hey MSNBC fans thanks for checking out
our YouTube channel subscribe by
clicking on that button down there and
click on any of the videos here to watch
the latest interviews and highlights you
can get more MSNBC for free every day
with our newsletters just visit
msnbc.com slash newsletters to sign up
now
