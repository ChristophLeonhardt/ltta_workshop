1
00:00:04,899 --> 00:00:08,799
thank you for coming and<font color="#E5E5E5"> I'm</font><font color="#CCCCCC"> shakir and</font>

2
00:00:08,799 --> 00:00:11,240
<font color="#E5E5E5">really excited to talk</font><font color="#CCCCCC"> to</font><font color="#E5E5E5"> you today</font>

3
00:00:11,240 --> 00:00:13,820
<font color="#E5E5E5">about unsupervised learning and</font>

4
00:00:13,820 --> 00:00:15,799
generative<font color="#CCCCCC"> model</font><font color="#E5E5E5"> so I wanted to maybe</font>

5
00:00:15,799 --> 00:00:18,260
just<font color="#E5E5E5"> start with a quick question to all</font>

6
00:00:18,260 --> 00:00:21,260
of<font color="#CCCCCC"> you few</font><font color="#E5E5E5"> people want to shout out</font>

7
00:00:21,260 --> 00:00:24,130
maybe the reasons<font color="#E5E5E5"> they think</font>

8
00:00:24,130 --> 00:00:27,140
<font color="#E5E5E5">unsupervised learning is important in</font>

9
00:00:27,140 --> 00:00:29,480
machine learning<font color="#CCCCCC"> generative models may</font>

10
00:00:29,480 --> 00:00:31,550
be important<font color="#E5E5E5"> or maybe if we have</font><font color="#CCCCCC"> someone</font>

11
00:00:31,550 --> 00:00:33,590
contrarian who thinks they aren't

12
00:00:33,590 --> 00:00:36,560
important<font color="#CCCCCC"> everyone wants to throw up at</font>

13
00:00:36,560 --> 00:00:41,030
<font color="#E5E5E5">the back so if you don't have enough</font>

14
00:00:41,030 --> 00:00:42,950
label data to do supervised learning

15
00:00:42,950 --> 00:00:46,010
<font color="#E5E5E5">that's a great one another any other</font>

16
00:00:46,010 --> 00:00:48,470
reasons or thoughts around<font color="#E5E5E5"> what</font><font color="#CCCCCC"> a</font>

17
00:00:48,470 --> 00:00:51,500
generative<font color="#E5E5E5"> model</font><font color="#CCCCCC"> is the applications</font>

18
00:00:51,500 --> 00:00:53,899
while<font color="#E5E5E5"> we let people settle in anyone</font>

19
00:00:53,899 --> 00:00:54,850
else

20
00:00:54,850 --> 00:01:00,860
<font color="#CCCCCC">ok so pardon</font><font color="#E5E5E5"> using</font><font color="#CCCCCC"> NLP used in NLP yes</font>

21
00:01:00,860 --> 00:01:02,629
<font color="#E5E5E5">to do natural language generation of</font>

22
00:01:02,629 --> 00:01:05,030
text or<font color="#E5E5E5"> even</font><font color="#CCCCCC"> audio or other kinds of</font>

23
00:01:05,030 --> 00:01:07,579
<font color="#E5E5E5">things so well experiment and explore</font><font color="#CCCCCC"> a</font>

24
00:01:07,579 --> 00:01:09,830
lot of the different ways of generative

25
00:01:09,830 --> 00:01:11,570
<font color="#E5E5E5">models and unsupervised learning more</font>

26
00:01:11,570 --> 00:01:14,049
generally I think<font color="#CCCCCC"> you</font><font color="#E5E5E5"> haven't seen</font>

27
00:01:14,049 --> 00:01:16,100
anything around unsupervised learning

28
00:01:16,100 --> 00:01:18,200
thus far in<font color="#E5E5E5"> this course but you've</font>

29
00:01:18,200 --> 00:01:20,840
<font color="#E5E5E5">probably seen them we know this and so</font><font color="#CCCCCC"> I</font>

30
00:01:20,840 --> 00:01:23,390
thought we'd start<font color="#CCCCCC"> with that so</font><font color="#E5E5E5"> what the</font>

31
00:01:23,390 --> 00:01:24,950
person said at the back why are<font color="#CCCCCC"> we</font>

32
00:01:24,950 --> 00:01:27,290
interested<font color="#E5E5E5"> in unsupervised learning and</font>

33
00:01:27,290 --> 00:01:29,630
the first reason<font color="#CCCCCC"> that many people</font><font color="#E5E5E5"> give</font>

34
00:01:29,630 --> 00:01:32,210
is<font color="#CCCCCC"> to move beyond associating simply</font>

35
00:01:32,210 --> 00:01:37,130
inputs to outputs<font color="#E5E5E5"> of images or features</font>

36
00:01:37,130 --> 00:01:40,640
<font color="#E5E5E5">to labels or to targets</font><font color="#CCCCCC"> but</font><font color="#E5E5E5"> then there</font>

37
00:01:40,640 --> 00:01:42,189
<font color="#E5E5E5">are other kinds</font><font color="#CCCCCC"> of reasons like the</font>

38
00:01:42,189 --> 00:01:44,240
example<font color="#CCCCCC"> that was just given</font><font color="#E5E5E5"> around in</font>

39
00:01:44,240 --> 00:01:46,790
<font color="#CCCCCC">elpida we want to understand</font><font color="#E5E5E5"> how the</font>

40
00:01:46,790 --> 00:01:49,100
world is<font color="#E5E5E5"> evolving how it's going to</font>

41
00:01:49,100 --> 00:01:51,229
change<font color="#CCCCCC"> over time and</font><font color="#E5E5E5"> we want to be able</font>

42
00:01:51,229 --> 00:01:53,750
<font color="#CCCCCC">to imagine</font><font color="#E5E5E5"> and simulate these kind of</font>

43
00:01:53,750 --> 00:01:55,670
potential evolutions of<font color="#E5E5E5"> the</font><font color="#CCCCCC"> world</font><font color="#E5E5E5"> going</font>

44
00:01:55,670 --> 00:01:58,430
<font color="#CCCCCC">forward in time</font><font color="#E5E5E5"> there are these things</font>

45
00:01:58,430 --> 00:02:00,560
<font color="#E5E5E5">which we recognize as people that are</font>

46
00:02:00,560 --> 00:02:02,810
objects in<font color="#E5E5E5"> the world they are objects</font>

47
00:02:02,810 --> 00:02:05,360
like your computer<font color="#CCCCCC"> like paper like a</font>

48
00:02:05,360 --> 00:02:07,369
screen they have certain ways in which

49
00:02:07,369 --> 00:02:09,799
they<font color="#CCCCCC"> are</font><font color="#E5E5E5"> move they behave</font><font color="#CCCCCC"> their factors</font>

50
00:02:09,799 --> 00:02:11,689
<font color="#CCCCCC">are variation as they call them which we</font>

51
00:02:11,689 --> 00:02:14,239
want<font color="#E5E5E5"> to know related to that is we want</font>

52
00:02:14,239 --> 00:02:16,519
<font color="#E5E5E5">to be able</font><font color="#CCCCCC"> to do a more abstract kind of</font>

53
00:02:16,519 --> 00:02:17,630
reasoning and

54
00:02:17,630 --> 00:02:19,700
so in<font color="#E5E5E5"> this kind of abstract reasoning we</font>

55
00:02:19,700 --> 00:02:21,740
want to be able to establish<font color="#CCCCCC"> certain</font>

56
00:02:21,740 --> 00:02:25,100
types of concepts<font color="#CCCCCC"> a conceptual basis for</font>

57
00:02:25,100 --> 00:02:26,270
the world and do reasoning and

58
00:02:26,270 --> 00:02:28,730
decision-making in that space we want to

59
00:02:28,730 --> 00:02:31,370
be able to know when something is new is

60
00:02:31,370 --> 00:02:34,760
interesting is surprising and we want to

61
00:02:34,760 --> 00:02:37,430
be able<font color="#CCCCCC"> to generate plans</font><font color="#E5E5E5"> about the</font><font color="#CCCCCC"> way</font>

62
00:02:37,430 --> 00:02:39,260
we will behave in<font color="#CCCCCC"> the future</font><font color="#E5E5E5"> the way</font>

63
00:02:39,260 --> 00:02:40,940
<font color="#CCCCCC">you're doing</font><font color="#E5E5E5"> reinforcement learning</font><font color="#CCCCCC"> so</font>

64
00:02:40,940 --> 00:02:43,100
all<font color="#E5E5E5"> of these are</font><font color="#CCCCCC"> the reasons</font><font color="#E5E5E5"> why we</font><font color="#CCCCCC"> are</font>

65
00:02:43,100 --> 00:02:45,790
<font color="#CCCCCC">interested in unsupervised learning and</font>

66
00:02:45,790 --> 00:02:48,950
<font color="#E5E5E5">I hope maybe we'll try and explore some</font>

67
00:02:48,950 --> 00:02:50,960
<font color="#CCCCCC">of</font><font color="#E5E5E5"> all of these kinds of things</font><font color="#CCCCCC"> as we go</font>

68
00:02:50,960 --> 00:02:52,790
through so but unsupervised learning

69
00:02:52,790 --> 00:02:56,060
will just be one part of far more bigger

70
00:02:56,060 --> 00:02:57,920
suite of things<font color="#CCCCCC"> that really</font><font color="#E5E5E5"> imagine and</font>

71
00:02:57,920 --> 00:02:59,930
under various different names<font color="#E5E5E5"> but one</font>

72
00:02:59,930 --> 00:03:01,510
way<font color="#E5E5E5"> to think of them are as</font>

73
00:03:01,510 --> 00:03:03,590
<font color="#E5E5E5">complementary learning systems we have</font>

74
00:03:03,590 --> 00:03:05,570
several<font color="#CCCCCC"> different types of</font><font color="#E5E5E5"> learning</font>

75
00:03:05,570 --> 00:03:07,480
systems unsupervised supervised

76
00:03:07,480 --> 00:03:09,560
reinforcement learning<font color="#E5E5E5"> semi-supervised</font>

77
00:03:09,560 --> 00:03:12,050
other kinds of transductive learning

78
00:03:12,050 --> 00:03:13,460
<font color="#E5E5E5">then</font><font color="#CCCCCC"> will all work</font><font color="#E5E5E5"> together to</font>

79
00:03:13,460 --> 00:03:15,530
complement<font color="#E5E5E5"> each other for us to build</font>

80
00:03:15,530 --> 00:03:17,960
these<font color="#E5E5E5"> general-purpose learning systems</font>

81
00:03:17,960 --> 00:03:20,540
<font color="#E5E5E5">right and so when we come to generative</font>

82
00:03:20,540 --> 00:03:22,580
models are<font color="#E5E5E5"> usually group their</font>

83
00:03:22,580 --> 00:03:24,500
applications<font color="#E5E5E5"> in their roles in three</font>

84
00:03:24,500 --> 00:03:25,910
different<font color="#CCCCCC"> areas we</font><font color="#E5E5E5"> want to have</font>

85
00:03:25,910 --> 00:03:27,740
generative models because<font color="#CCCCCC"> we want</font><font color="#E5E5E5"> to</font>

86
00:03:27,740 --> 00:03:29,660
build products<font color="#CCCCCC"> we want</font><font color="#E5E5E5"> to build things</font>

87
00:03:29,660 --> 00:03:31,310
which are useful which help our everyday

88
00:03:31,310 --> 00:03:33,830
<font color="#E5E5E5">lives</font><font color="#CCCCCC"> so things like super resolution of</font>

89
00:03:33,830 --> 00:03:35,570
<font color="#E5E5E5">images and video as we want to</font><font color="#CCCCCC"> do</font>

90
00:03:35,570 --> 00:03:37,580
transmission and compression of high

91
00:03:37,580 --> 00:03:40,670
bandwidth<font color="#E5E5E5"> data streams</font><font color="#CCCCCC"> we want to do</font>

92
00:03:40,670 --> 00:03:43,360
<font color="#E5E5E5">text-to-speech for example to create</font>

93
00:03:43,360 --> 00:03:46,400
<font color="#E5E5E5">accessibility tools</font><font color="#CCCCCC"> then we want</font><font color="#E5E5E5"> to move</font>

94
00:03:46,400 --> 00:03:48,650
to areas<font color="#CCCCCC"> of</font><font color="#E5E5E5"> science we want to continue</font>

95
00:03:48,650 --> 00:03:51,170
<font color="#E5E5E5">to understand how it is</font><font color="#CCCCCC"> that</font><font color="#E5E5E5"> the</font><font color="#CCCCCC"> natural</font>

96
00:03:51,170 --> 00:03:53,270
and<font color="#E5E5E5"> physical world observes and then to</font>

97
00:03:53,270 --> 00:03:56,600
<font color="#E5E5E5">do use those scientific</font><font color="#CCCCCC"> basis to again</font>

98
00:03:56,600 --> 00:03:58,640
<font color="#E5E5E5">inform other kinds of areas</font><font color="#CCCCCC"> like</font>

99
00:03:58,640 --> 00:04:01,010
<font color="#CCCCCC">healthcare and the social social world</font>

100
00:04:01,010 --> 00:04:04,030
so things like proteomics<font color="#CCCCCC"> drug discovery</font>

101
00:04:04,030 --> 00:04:06,620
understanding celestial objects using

102
00:04:06,620 --> 00:04:08,810
<font color="#CCCCCC">high-energy physics</font><font color="#E5E5E5"> where the big</font>

103
00:04:08,810 --> 00:04:10,580
question in<font color="#CCCCCC"> high-energy</font><font color="#E5E5E5"> physics is how</font>

104
00:04:10,580 --> 00:04:13,010
we move beyond<font color="#E5E5E5"> the standard model that's</font>

105
00:04:13,010 --> 00:04:15,590
what they call the current question<font color="#E5E5E5"> now</font>

106
00:04:15,590 --> 00:04:17,600
that<font color="#CCCCCC"> we've found the</font><font color="#E5E5E5"> Higgs boson what is</font>

107
00:04:17,600 --> 00:04:20,120
beyond that standard model and then<font color="#CCCCCC"> we</font>

108
00:04:20,120 --> 00:04:22,610
have the tasks<font color="#E5E5E5"> of reasoning and AI so</font>

109
00:04:22,610 --> 00:04:24,770
can we do planning<font color="#CCCCCC"> how do we do</font>

110
00:04:24,770 --> 00:04:26,960
exploration<font color="#CCCCCC"> as agents in the world to</font>

111
00:04:26,960 --> 00:04:28,010
discover new

112
00:04:28,010 --> 00:04:31,130
<font color="#E5E5E5">here is how do we build self motivation</font>

113
00:04:31,130 --> 00:04:33,080
<font color="#E5E5E5">and keep ourselves discovering new</font>

114
00:04:33,080 --> 00:04:35,120
<font color="#CCCCCC">things and</font><font color="#E5E5E5"> maybe part of what you've</font>

115
00:04:35,120 --> 00:04:37,010
seen in the<font color="#E5E5E5"> other half is around</font><font color="#CCCCCC"> doing</font>

116
00:04:37,010 --> 00:04:39,020
model-based reinforcement learning so

117
00:04:39,020 --> 00:04:40,310
these are all some<font color="#CCCCCC"> of the different</font>

118
00:04:40,310 --> 00:04:43,730
<font color="#CCCCCC">areas that I have</font><font color="#E5E5E5"> in mind</font><font color="#CCCCCC"> and</font><font color="#E5E5E5"> different</font>

119
00:04:43,730 --> 00:04:45,560
products so I'm gonna do a little

120
00:04:45,560 --> 00:04:47,060
<font color="#E5E5E5">experiment</font><font color="#CCCCCC"> with you but I thought we're</font>

121
00:04:47,060 --> 00:04:48,890
gonna do two halves<font color="#E5E5E5"> and in this first</font>

122
00:04:48,890 --> 00:04:49,400
half

123
00:04:49,400 --> 00:04:51,950
I thought we'd look<font color="#E5E5E5"> at five tricks for</font>

124
00:04:51,950 --> 00:04:53,720
manipulating probabilities<font color="#E5E5E5"> and I'm gonna</font>

125
00:04:53,720 --> 00:04:55,330
just<font color="#E5E5E5"> discuss with you five</font>

126
00:04:55,330 --> 00:04:58,640
self-contained<font color="#E5E5E5"> tools mathematical and</font>

127
00:04:58,640 --> 00:05:00,230
probabilistic<font color="#E5E5E5"> tools that you can use in</font>

128
00:05:00,230 --> 00:05:01,700
<font color="#E5E5E5">almost every part of machine learning</font>

129
00:05:01,700 --> 00:05:04,580
<font color="#CCCCCC">and they're typically</font><font color="#E5E5E5"> 4 4 into</font><font color="#CCCCCC"> 3</font><font color="#E5E5E5"> parts</font>

130
00:05:04,580 --> 00:05:06,590
<font color="#E5E5E5">one part will be about manipulating</font>

131
00:05:06,590 --> 00:05:08,510
integrals<font color="#E5E5E5"> another part will be about</font>

132
00:05:08,510 --> 00:05:10,910
<font color="#E5E5E5">manipulating densities and another part</font>

133
00:05:10,910 --> 00:05:13,220
of<font color="#CCCCCC"> up</font><font color="#E5E5E5"> manipulating gradients and these</font>

134
00:05:13,220 --> 00:05:15,140
five tricks<font color="#E5E5E5"> together you can use them</font>

135
00:05:15,140 --> 00:05:17,480
certainly<font color="#E5E5E5"> for generative models what</font>

136
00:05:17,480 --> 00:05:18,920
we're going<font color="#CCCCCC"> to discuss in the next</font><font color="#E5E5E5"> part</font>

137
00:05:18,920 --> 00:05:21,050
<font color="#CCCCCC">but</font><font color="#E5E5E5"> you have already used some</font><font color="#CCCCCC"> of them</font>

138
00:05:21,050 --> 00:05:22,940
in reinforcement learning<font color="#E5E5E5"> and in other</font>

139
00:05:22,940 --> 00:05:24,410
<font color="#E5E5E5">areas of machine</font><font color="#CCCCCC"> learning that</font><font color="#E5E5E5"> you've</font>

140
00:05:24,410 --> 00:05:26,030
covered<font color="#CCCCCC"> before you've used them</font><font color="#E5E5E5"> and I</font>

141
00:05:26,030 --> 00:05:28,160
want to just<font color="#E5E5E5"> represent them to you in a</font>

142
00:05:28,160 --> 00:05:30,050
<font color="#E5E5E5">different light and then in the second</font>

143
00:05:30,050 --> 00:05:32,240
part<font color="#E5E5E5"> we'll do a brief introduction to</font>

144
00:05:32,240 --> 00:05:33,980
generative models<font color="#CCCCCC"> we'll look at the</font>

145
00:05:33,980 --> 00:05:35,540
types of generative<font color="#E5E5E5"> models that we have</font>

146
00:05:35,540 --> 00:05:37,220
<font color="#E5E5E5">available and then we'll break</font>

147
00:05:37,220 --> 00:05:39,020
generative models up into two parts

148
00:05:39,020 --> 00:05:41,030
<font color="#E5E5E5">prescribed generative models and</font>

149
00:05:41,030 --> 00:05:43,370
implicit generative models<font color="#E5E5E5"> and we'll</font>

150
00:05:43,370 --> 00:05:44,810
look at the different<font color="#CCCCCC"> kinds of</font><font color="#E5E5E5"> ways of</font>

151
00:05:44,810 --> 00:05:47,240
doing<font color="#E5E5E5"> learning in these two types of</font>

152
00:05:47,240 --> 00:05:51,320
<font color="#E5E5E5">models so</font><font color="#CCCCCC"> ok here my five tricks and</font>

153
00:05:51,320 --> 00:05:53,300
again I want you to try<font color="#E5E5E5"> and experiment</font>

154
00:05:53,300 --> 00:05:54,830
with you because I want to try<font color="#CCCCCC"> and do</font>

155
00:05:54,830 --> 00:05:56,300
this<font color="#CCCCCC"> again later</font><font color="#E5E5E5"> on at</font><font color="#CCCCCC"> the machine</font>

156
00:05:56,300 --> 00:05:57,740
learning<font color="#CCCCCC"> summer school so please give me</font>

157
00:05:57,740 --> 00:05:59,450
feedback<font color="#CCCCCC"> at</font><font color="#E5E5E5"> the end as</font><font color="#CCCCCC"> to how you think</font>

158
00:05:59,450 --> 00:06:03,470
<font color="#CCCCCC">it worked out so the thing that I want</font>

159
00:06:03,470 --> 00:06:05,030
<font color="#E5E5E5">to leave</font><font color="#CCCCCC"> you in this</font><font color="#E5E5E5"> first half of this</font>

160
00:06:05,030 --> 00:06:07,460
<font color="#E5E5E5">next 50</font><font color="#CCCCCC"> minutes is that I want each and</font>

161
00:06:07,460 --> 00:06:09,650
every<font color="#CCCCCC"> one of you to be able</font><font color="#E5E5E5"> to build a</font>

162
00:06:09,650 --> 00:06:12,380
probabilistic<font color="#E5E5E5"> dexterity when you see</font>

163
00:06:12,380 --> 00:06:14,150
probabilities and integrals<font color="#E5E5E5"> and</font>

164
00:06:14,150 --> 00:06:16,280
probabilities you can<font color="#E5E5E5"> manipulate and</font>

165
00:06:16,280 --> 00:06:18,020
<font color="#CCCCCC">mould them to do the things that you</font>

166
00:06:18,020 --> 00:06:20,000
need<font color="#CCCCCC"> to do and this is effectively the</font>

167
00:06:20,000 --> 00:06:22,430
<font color="#E5E5E5">problem of machine learning</font><font color="#CCCCCC"> so if we</font>

168
00:06:22,430 --> 00:06:24,620
have this probabilistic dexterity if we

169
00:06:24,620 --> 00:06:26,510
have a set of<font color="#E5E5E5"> tools which we can</font>

170
00:06:26,510 --> 00:06:28,310
<font color="#E5E5E5">manipulate integrals probability</font>

171
00:06:28,310 --> 00:06:30,080
distributions and their gradients then

172
00:06:30,080 --> 00:06:31,640
we will have the tools<font color="#E5E5E5"> to solve the</font>

173
00:06:31,640 --> 00:06:33,050
fundamental<font color="#E5E5E5"> problems of machine learning</font>

174
00:06:33,050 --> 00:06:35,810
<font color="#CCCCCC">in AR and some of these questions come</font>

175
00:06:35,810 --> 00:06:38,750
<font color="#E5E5E5">up one of the most fundamental</font><font color="#CCCCCC"> questions</font>

176
00:06:38,750 --> 00:06:40,640
is always<font color="#E5E5E5"> to compute the evidence of a</font>

177
00:06:40,640 --> 00:06:41,690
set of<font color="#E5E5E5"> data so if</font>

178
00:06:41,690 --> 00:06:44,300
<font color="#E5E5E5">data X you want to know what is the</font>

179
00:06:44,300 --> 00:06:47,000
<font color="#E5E5E5">probability that that</font><font color="#CCCCCC"> dataset</font><font color="#E5E5E5"> appears in</font>

180
00:06:47,000 --> 00:06:48,710
the world<font color="#E5E5E5"> and knowing this quantity is</font>

181
00:06:48,710 --> 00:06:50,240
in fact one<font color="#CCCCCC"> of</font><font color="#E5E5E5"> the most fundamental</font>

182
00:06:50,240 --> 00:06:52,160
quantities you<font color="#CCCCCC"> can know in machine</font>

183
00:06:52,160 --> 00:06:53,570
<font color="#CCCCCC">learning</font><font color="#E5E5E5"> because</font><font color="#CCCCCC"> if you know this</font>

184
00:06:53,570 --> 00:06:55,580
<font color="#E5E5E5">probability X you basically know</font>

185
00:06:55,580 --> 00:06:57,260
everything you know when it's surprising

186
00:06:57,260 --> 00:06:58,850
what is probable<font color="#CCCCCC"> you know all</font><font color="#E5E5E5"> its</font>

187
00:06:58,850 --> 00:07:01,640
moments<font color="#E5E5E5"> you know you can do everything</font>

188
00:07:01,640 --> 00:07:04,700
<font color="#CCCCCC">from there rewriting this evidence</font>

189
00:07:04,700 --> 00:07:06,440
estimation question in a<font color="#CCCCCC"> different</font><font color="#E5E5E5"> way</font>

190
00:07:06,440 --> 00:07:08,420
<font color="#E5E5E5">so this first part is sometimes called</font>

191
00:07:08,420 --> 00:07:10,430
the<font color="#E5E5E5"> marginalized probability they</font>

192
00:07:10,430 --> 00:07:12,320
integrated likelihood the partition

193
00:07:12,320 --> 00:07:14,000
function<font color="#CCCCCC"> that has several different</font>

194
00:07:14,000 --> 00:07:15,650
kinds of names<font color="#CCCCCC"> you may want to</font><font color="#E5E5E5"> do a</font>

195
00:07:15,650 --> 00:07:17,570
simpler task than this<font color="#E5E5E5"> you may just want</font>

196
00:07:17,570 --> 00:07:19,490
to compute<font color="#E5E5E5"> a set of moments this</font><font color="#CCCCCC"> is</font>

197
00:07:19,490 --> 00:07:21,230
because you want to<font color="#E5E5E5"> summarize</font><font color="#CCCCCC"> data in</font>

198
00:07:21,230 --> 00:07:22,940
some way<font color="#E5E5E5"> you want to compute</font><font color="#CCCCCC"> this</font>

199
00:07:22,940 --> 00:07:25,700
quantiles or certain expect<font color="#CCCCCC"> Isles</font><font color="#E5E5E5"> and</font>

200
00:07:25,700 --> 00:07:27,830
knowing<font color="#E5E5E5"> those quantities and reporting</font>

201
00:07:27,830 --> 00:07:30,320
them is useful<font color="#E5E5E5"> and so for</font><font color="#CCCCCC"> example</font>

202
00:07:30,320 --> 00:07:31,640
physics you want to do<font color="#CCCCCC"> Six Sigma</font>

203
00:07:31,640 --> 00:07:33,470
<font color="#E5E5E5">computations of whether something is</font>

204
00:07:33,470 --> 00:07:35,570
<font color="#CCCCCC">actually there or</font><font color="#E5E5E5"> not so</font><font color="#CCCCCC"> then you'll</font>

205
00:07:35,570 --> 00:07:37,190
want<font color="#E5E5E5"> to do these moment computations</font>

206
00:07:37,190 --> 00:07:38,720
where<font color="#E5E5E5"> you'll compute an expectation of</font>

207
00:07:38,720 --> 00:07:42,500
some function<font color="#E5E5E5"> of a random variable</font><font color="#CCCCCC"> the</font>

208
00:07:42,500 --> 00:07:44,510
the notation<font color="#E5E5E5"> is swapping</font><font color="#CCCCCC"> around but just</font>

209
00:07:44,510 --> 00:07:46,490
use it as a<font color="#E5E5E5"> general idea of course the</font>

210
00:07:46,490 --> 00:07:48,080
<font color="#E5E5E5">most fundamental problem in machine</font>

211
00:07:48,080 --> 00:07:49,760
learning<font color="#CCCCCC"> statistics is to do parameter</font>

212
00:07:49,760 --> 00:07:51,500
estimation<font color="#E5E5E5"> you want to actually know</font>

213
00:07:51,500 --> 00:07:54,470
<font color="#CCCCCC">what the value of parameter theta</font><font color="#E5E5E5"> of a</font>

214
00:07:54,470 --> 00:07:56,900
model that<font color="#CCCCCC"> you have is given some data X</font>

215
00:07:56,900 --> 00:07:59,270
and if you want it to be<font color="#E5E5E5"> frequentist</font>

216
00:07:59,270 --> 00:08:01,310
then you will find the point estimate

217
00:08:01,310 --> 00:08:03,140
and a confidence interval<font color="#E5E5E5"> around theta</font>

218
00:08:03,140 --> 00:08:04,940
<font color="#E5E5E5">or if you are being Bayesian you would</font>

219
00:08:04,940 --> 00:08:06,380
find the entire<font color="#CCCCCC"> probability distribution</font>

220
00:08:06,380 --> 00:08:10,490
and report<font color="#CCCCCC"> either its central central</font>

221
00:08:10,490 --> 00:08:13,400
area and<font color="#CCCCCC"> then it's a confidence highest</font>

222
00:08:13,400 --> 00:08:15,770
<font color="#CCCCCC">probability density region so this is</font>

223
00:08:15,770 --> 00:08:17,300
probably the<font color="#E5E5E5"> most common what you've</font>

224
00:08:17,300 --> 00:08:19,340
done<font color="#CCCCCC"> almost in every course in machine</font>

225
00:08:19,340 --> 00:08:21,290
learning<font color="#E5E5E5"> so far and of course we want to</font>

226
00:08:21,290 --> 00:08:22,970
do prediction because once we have<font color="#E5E5E5"> these</font>

227
00:08:22,970 --> 00:08:25,550
models<font color="#E5E5E5"> we want to use it to do something</font>

228
00:08:25,550 --> 00:08:28,130
useful in the world the thing<font color="#CCCCCC"> you've</font>

229
00:08:28,130 --> 00:08:30,020
seen in the other part is<font color="#E5E5E5"> to do planning</font>

230
00:08:30,020 --> 00:08:32,000
so<font color="#CCCCCC"> that if you have a</font><font color="#E5E5E5"> certain cost</font>

231
00:08:32,000 --> 00:08:34,760
function C and an action<font color="#E5E5E5"> u that you are</font>

232
00:08:34,760 --> 00:08:36,650
trying<font color="#E5E5E5"> to take how is it</font><font color="#CCCCCC"> that you can</font>

233
00:08:36,650 --> 00:08:39,200
choose<font color="#CCCCCC"> the set of</font><font color="#E5E5E5"> actions</font><font color="#CCCCCC"> you to</font>

234
00:08:39,200 --> 00:08:41,570
<font color="#E5E5E5">maximize the cost</font><font color="#CCCCCC"> under certain types of</font>

235
00:08:41,570 --> 00:08:43,370
<font color="#E5E5E5">probability distribution so this was the</font>

236
00:08:43,370 --> 00:08:45,500
fundamental equation that<font color="#E5E5E5"> you</font><font color="#CCCCCC"> were all</font>

237
00:08:45,500 --> 00:08:48,860
solving to talk about<font color="#CCCCCC"> man's</font><font color="#E5E5E5"> equation and</font>

238
00:08:48,860 --> 00:08:50,960
to learn the policy gradient theorem and

239
00:08:50,960 --> 00:08:52,480
to do value<font color="#E5E5E5"> estimation as well</font>

240
00:08:52,480 --> 00:08:55,000
then another<font color="#E5E5E5"> kind of important one is</font>

241
00:08:55,000 --> 00:08:56,440
for<font color="#CCCCCC"> example if we are dealing with</font>

242
00:08:56,440 --> 00:08:59,019
<font color="#E5E5E5">medical domain</font><font color="#CCCCCC"> you will have</font><font color="#E5E5E5"> multiple</font>

243
00:08:59,019 --> 00:09:01,120
hypotheses<font color="#CCCCCC"> that we are</font><font color="#E5E5E5"> testing and you</font>

244
00:09:01,120 --> 00:09:02,740
need to be able to compare those two

245
00:09:02,740 --> 00:09:05,260
hypotheses<font color="#E5E5E5"> which one is better or which</font>

246
00:09:05,260 --> 00:09:07,480
one has the<font color="#E5E5E5"> effects that you wanted it</font>

247
00:09:07,480 --> 00:09:09,519
<font color="#E5E5E5">to have and then the last one relate to</font>

248
00:09:09,519 --> 00:09:11,139
the hypothesis testing is to do an

249
00:09:11,139 --> 00:09:13,600
experimental<font color="#E5E5E5"> design</font><font color="#CCCCCC"> how is</font><font color="#E5E5E5"> it</font><font color="#CCCCCC"> that I can</font>

250
00:09:13,600 --> 00:09:15,820
take an<font color="#E5E5E5"> action observe</font><font color="#CCCCCC"> what happens</font><font color="#E5E5E5"> in</font>

251
00:09:15,820 --> 00:09:17,410
the world and then choose the actions of

252
00:09:17,410 --> 00:09:19,420
the next set of<font color="#CCCCCC"> experiments that I will</font>

253
00:09:19,420 --> 00:09:21,639
do so that I actually<font color="#E5E5E5"> learn and so</font>

254
00:09:21,639 --> 00:09:23,560
hidden in all of these things<font color="#CCCCCC"> there are</font>

255
00:09:23,560 --> 00:09:26,260
<font color="#E5E5E5">a probabilities and these probabilities</font>

256
00:09:26,260 --> 00:09:28,690
are<font color="#E5E5E5"> sometimes difficult you don't know</font>

257
00:09:28,690 --> 00:09:31,060
what they<font color="#E5E5E5"> are</font><font color="#CCCCCC"> sometimes you are known to</font>

258
00:09:31,060 --> 00:09:32,500
you partially sometimes you can<font color="#E5E5E5"> only</font>

259
00:09:32,500 --> 00:09:33,970
simulate<font color="#E5E5E5"> you don't know them</font>

260
00:09:33,970 --> 00:09:36,250
<font color="#CCCCCC">analytically in</font><font color="#E5E5E5"> almost</font><font color="#CCCCCC"> all of them there</font>

261
00:09:36,250 --> 00:09:38,139
is<font color="#E5E5E5"> a horrible integral it's just written</font>

262
00:09:38,139 --> 00:09:39,459
one here<font color="#E5E5E5"> but</font><font color="#CCCCCC"> usually it's a</font>

263
00:09:39,459 --> 00:09:41,139
<font color="#E5E5E5">d-dimensional integral over the</font>

264
00:09:41,139 --> 00:09:42,579
dimension<font color="#E5E5E5"> of things that you are dealing</font>

265
00:09:42,579 --> 00:09:44,579
<font color="#E5E5E5">with so you can't solve it numerically</font>

266
00:09:44,579 --> 00:09:47,380
<font color="#CCCCCC">in other cases they are gradients which</font>

267
00:09:47,380 --> 00:09:49,029
need<font color="#CCCCCC"> to be</font><font color="#E5E5E5"> done through inverse</font>

268
00:09:49,029 --> 00:09:50,320
probabilities and<font color="#E5E5E5"> there are other</font>

269
00:09:50,320 --> 00:09:52,540
normalizing constants<font color="#CCCCCC"> that appear or in</font>

270
00:09:52,540 --> 00:09:55,839
<font color="#E5E5E5">some cases that external factors which</font>

271
00:09:55,839 --> 00:09:57,790
are affecting your model<font color="#E5E5E5"> and so in all</font>

272
00:09:57,790 --> 00:09:59,410
of<font color="#CCCCCC"> these</font><font color="#E5E5E5"> cases you will need to do some</font>

273
00:09:59,410 --> 00:10:01,750
kind<font color="#CCCCCC"> of</font><font color="#E5E5E5"> manipulation</font><font color="#CCCCCC"> and this is where</font>

274
00:10:01,750 --> 00:10:03,579
this first<font color="#E5E5E5"> part is and if we</font><font color="#CCCCCC"> have those</font>

275
00:10:03,579 --> 00:10:05,769
kind of tricks we<font color="#CCCCCC"> can manipulate all</font>

276
00:10:05,769 --> 00:10:07,360
these<font color="#E5E5E5"> integrals all these gradients all</font>

277
00:10:07,360 --> 00:10:08,949
these probabilities<font color="#CCCCCC"> and</font><font color="#E5E5E5"> then we can</font>

278
00:10:08,949 --> 00:10:11,980
actually<font color="#E5E5E5"> build a rich very rich</font><font color="#CCCCCC"> and deep</font>

279
00:10:11,980 --> 00:10:14,769
<font color="#CCCCCC">understanding of</font><font color="#E5E5E5"> machine</font><font color="#CCCCCC"> on so the first</font>

280
00:10:14,769 --> 00:10:16,209
trick<font color="#E5E5E5"> I wanted to present you you've all</font>

281
00:10:16,209 --> 00:10:17,319
seen this before<font color="#CCCCCC"> it's</font><font color="#E5E5E5"> called the</font>

282
00:10:17,319 --> 00:10:19,540
identity trick so whenever you have<font color="#E5E5E5"> a</font>

283
00:10:19,540 --> 00:10:21,370
<font color="#CCCCCC">problem</font><font color="#E5E5E5"> that you have</font><font color="#CCCCCC"> an integral or</font><font color="#E5E5E5"> an</font>

284
00:10:21,370 --> 00:10:24,370
expectation<font color="#E5E5E5"> in some</font><font color="#CCCCCC"> distribution P you</font>

285
00:10:24,370 --> 00:10:26,350
may<font color="#E5E5E5"> not like that integral you may not</font>

286
00:10:26,350 --> 00:10:28,779
like P P<font color="#CCCCCC"> is not your</font><font color="#E5E5E5"> friend</font><font color="#CCCCCC"> you can</font>

287
00:10:28,779 --> 00:10:30,699
instead change the integral into an

288
00:10:30,699 --> 00:10:32,019
expectation<font color="#CCCCCC"> under a different</font>

289
00:10:32,019 --> 00:10:34,120
distribution<font color="#CCCCCC"> q</font><font color="#E5E5E5"> here is something you get</font>

290
00:10:34,120 --> 00:10:36,639
<font color="#CCCCCC">to</font><font color="#E5E5E5"> choose</font><font color="#CCCCCC"> q is your friend so this</font><font color="#E5E5E5"> is</font>

291
00:10:36,639 --> 00:10:38,350
<font color="#E5E5E5">where you will use this identity trick</font>

292
00:10:38,350 --> 00:10:40,389
and so here's the expectation you<font color="#CCCCCC"> have</font>

293
00:10:40,389 --> 00:10:42,730
an integral<font color="#E5E5E5"> of some probability</font>

294
00:10:42,730 --> 00:10:45,220
distribution<font color="#E5E5E5"> x over a function f of X</font><font color="#CCCCCC"> Y</font>

295
00:10:45,220 --> 00:10:47,290
<font color="#E5E5E5">it's the expectation of X under the</font>

296
00:10:47,290 --> 00:10:49,600
distribution<font color="#E5E5E5"> P as I said you may</font><font color="#CCCCCC"> not</font>

297
00:10:49,600 --> 00:10:51,699
like this integral for<font color="#E5E5E5"> various reasons</font>

298
00:10:51,699 --> 00:10:54,310
<font color="#E5E5E5">because P is difficult to compute</font>

299
00:10:54,310 --> 00:10:56,470
because<font color="#E5E5E5"> F may be not differentiable for</font>

300
00:10:56,470 --> 00:10:58,209
various reasons<font color="#E5E5E5"> and you instead want to</font>

301
00:10:58,209 --> 00:11:00,069
rewrite it<font color="#E5E5E5"> as an expectation like this</font>

302
00:11:00,069 --> 00:11:02,439
an expectation<font color="#E5E5E5"> under</font><font color="#CCCCCC"> a new distribution</font>

303
00:11:02,439 --> 00:11:04,839
<font color="#CCCCCC">Q with some transformation of the</font>

304
00:11:04,839 --> 00:11:05,760
function f so

305
00:11:05,760 --> 00:11:07,949
<font color="#CCCCCC">writing this G here and writing</font><font color="#E5E5E5"> out the</font>

306
00:11:07,949 --> 00:11:10,079
integral and<font color="#E5E5E5"> so how you move between</font>

307
00:11:10,079 --> 00:11:12,360
<font color="#E5E5E5">these two is to multiply your</font>

308
00:11:12,360 --> 00:11:14,670
<font color="#E5E5E5">probabilities by one right so we're</font>

309
00:11:14,670 --> 00:11:16,500
going<font color="#E5E5E5"> to introduce a probabilistic one</font>

310
00:11:16,500 --> 00:11:18,420
and I think<font color="#CCCCCC"> you've seen this already at</font>

311
00:11:18,420 --> 00:11:20,639
<font color="#E5E5E5">least in two different places so let's</font>

312
00:11:20,639 --> 00:11:23,519
do let's apply this identity trick so

313
00:11:23,519 --> 00:11:25,320
here's an integral problem I want to

314
00:11:25,320 --> 00:11:27,600
compute<font color="#E5E5E5"> the</font><font color="#CCCCCC"> expectation of this</font>

315
00:11:27,600 --> 00:11:30,720
<font color="#CCCCCC">distribution</font><font color="#E5E5E5"> P of</font><font color="#CCCCCC"> X</font><font color="#E5E5E5"> given Z under the</font>

316
00:11:30,720 --> 00:11:32,430
distribution<font color="#E5E5E5"> P of Z so that's our</font>

317
00:11:32,430 --> 00:11:34,350
integral problem<font color="#E5E5E5"> and typically if you</font>

318
00:11:34,350 --> 00:11:36,480
had studied<font color="#E5E5E5"> graphical models before</font><font color="#CCCCCC"> we</font>

319
00:11:36,480 --> 00:11:38,100
<font color="#CCCCCC">had have seen this kind of latent</font>

320
00:11:38,100 --> 00:11:39,480
variable problem has appeared before

321
00:11:39,480 --> 00:11:42,209
<font color="#E5E5E5">right and so the trick is</font><font color="#CCCCCC"> to</font><font color="#E5E5E5"> multiply</font>

322
00:11:42,209 --> 00:11:44,160
this integral by<font color="#E5E5E5"> one so I'm going to</font>

323
00:11:44,160 --> 00:11:46,920
<font color="#E5E5E5">introduce this new distribution Q and Q</font>

324
00:11:46,920 --> 00:11:48,779
divided by<font color="#CCCCCC"> Q is 1 so the integral is</font>

325
00:11:48,779 --> 00:11:50,970
exactly the same right<font color="#CCCCCC"> but now I can</font>

326
00:11:50,970 --> 00:11:53,610
regroup the terms I<font color="#E5E5E5"> can leave P of</font><font color="#CCCCCC"> x</font>

327
00:11:53,610 --> 00:11:56,430
given<font color="#E5E5E5"> Z</font><font color="#CCCCCC"> and I can group P of Z divided</font>

328
00:11:56,430 --> 00:11:58,649
<font color="#CCCCCC">by Q of Z as some new quantity and</font>

329
00:11:58,649 --> 00:12:00,660
actually<font color="#E5E5E5"> this ratio is</font><font color="#CCCCCC"> something just</font><font color="#E5E5E5"> to</font>

330
00:12:00,660 --> 00:12:01,860
keep<font color="#CCCCCC"> in mind we'll look at it a bit</font>

331
00:12:01,860 --> 00:12:03,720
<font color="#CCCCCC">later</font><font color="#E5E5E5"> and now there's a new distribution</font>

332
00:12:03,720 --> 00:12:06,149
<font color="#CCCCCC">Z</font><font color="#E5E5E5"> and now this is an integral which is</font>

333
00:12:06,149 --> 00:12:08,339
an expectation<font color="#E5E5E5"> under the</font><font color="#CCCCCC"> distribution Q</font>

334
00:12:08,339 --> 00:12:11,699
of this new quantity right and so this

335
00:12:11,699 --> 00:12:14,220
trick you<font color="#E5E5E5"> can</font><font color="#CCCCCC"> use almost everywhere</font><font color="#E5E5E5"> and</font>

336
00:12:14,220 --> 00:12:16,260
it's one<font color="#E5E5E5"> of the most useful tricks</font>

337
00:12:16,260 --> 00:12:17,579
<font color="#CCCCCC">you'll use it in reinforcement learning</font>

338
00:12:17,579 --> 00:12:20,250
<font color="#CCCCCC">to do sort of</font><font color="#E5E5E5"> policy Corrections you</font>

339
00:12:20,250 --> 00:12:21,899
<font color="#CCCCCC">will use it in the next slide so there's</font>

340
00:12:21,899 --> 00:12:23,579
just<font color="#E5E5E5"> some</font><font color="#CCCCCC"> things</font><font color="#E5E5E5"> to know because we are</font>

341
00:12:23,579 --> 00:12:25,620
dividing by<font color="#E5E5E5"> Q you need to make sure that</font>

342
00:12:25,620 --> 00:12:27,870
<font color="#CCCCCC">Q is greater than zero</font><font color="#E5E5E5"> wherever this</font>

343
00:12:27,870 --> 00:12:29,940
product of P times P<font color="#E5E5E5"> is greater than</font>

344
00:12:29,940 --> 00:12:33,149
<font color="#E5E5E5">zero and Q that you are</font><font color="#CCCCCC"> into introducing</font>

345
00:12:33,149 --> 00:12:34,649
<font color="#CCCCCC">and choosing must be something you</font>

346
00:12:34,649 --> 00:12:36,829
should easily<font color="#E5E5E5"> manipulate right and so</font>

347
00:12:36,829 --> 00:12:39,180
this trick is obviously the basis of

348
00:12:39,180 --> 00:12:40,620
important<font color="#E5E5E5"> sampling that you've seen</font>

349
00:12:40,620 --> 00:12:43,709
<font color="#CCCCCC">before where we call the ratio P divided</font>

350
00:12:43,709 --> 00:12:46,769
by Q<font color="#E5E5E5"> the importance weight which is W</font>

351
00:12:46,769 --> 00:12:49,380
you can<font color="#E5E5E5"> simulate</font><font color="#CCCCCC"> a</font><font color="#E5E5E5"> sample from Q of Z</font>

352
00:12:49,380 --> 00:12:51,180
and then you can<font color="#E5E5E5"> evaluate this integral</font>

353
00:12:51,180 --> 00:12:54,120
<font color="#E5E5E5">by Monte Carlo integration</font><font color="#CCCCCC"> right and</font>

354
00:12:54,120 --> 00:12:56,130
this is one of<font color="#E5E5E5"> the most useful tools</font><font color="#CCCCCC"> to</font>

355
00:12:56,130 --> 00:13:00,420
keep in mind this<font color="#CCCCCC"> to of integration</font><font color="#E5E5E5"> by</font>

356
00:13:00,420 --> 00:13:02,190
<font color="#E5E5E5">using Monte Carlo</font><font color="#CCCCCC"> an important something</font>

357
00:13:02,190 --> 00:13:03,899
<font color="#E5E5E5">is one of the most basic tools we have</font>

358
00:13:03,899 --> 00:13:06,209
<font color="#CCCCCC">for manipulating probabilities but it is</font>

359
00:13:06,209 --> 00:13:08,310
<font color="#CCCCCC">useful only if you want to</font><font color="#E5E5E5"> know the</font>

360
00:13:08,310 --> 00:13:11,010
value<font color="#CCCCCC"> of an integral</font><font color="#E5E5E5"> right because</font>

361
00:13:11,010 --> 00:13:12,569
that's what important<font color="#E5E5E5"> sampling does if</font>

362
00:13:12,569 --> 00:13:14,100
you wanted to do<font color="#CCCCCC"> more than</font><font color="#E5E5E5"> know the</font>

363
00:13:14,100 --> 00:13:15,839
value<font color="#E5E5E5"> of an integral</font><font color="#CCCCCC"> if you wanted to</font>

364
00:13:15,839 --> 00:13:17,459
use it<font color="#CCCCCC"> for learning</font><font color="#E5E5E5"> then important</font>

365
00:13:17,459 --> 00:13:19,449
sampling<font color="#E5E5E5"> is not going to be quite</font><font color="#CCCCCC"> useful</font>

366
00:13:19,449 --> 00:13:22,579
<font color="#CCCCCC">but we'll come</font><font color="#E5E5E5"> to that</font><font color="#CCCCCC"> a bit</font><font color="#E5E5E5"> later</font><font color="#CCCCCC"> on so</font>

367
00:13:22,579 --> 00:13:24,050
<font color="#E5E5E5">you would have seen this kind of</font>

368
00:13:24,050 --> 00:13:26,180
identity trick<font color="#E5E5E5"> not just an important</font>

369
00:13:26,180 --> 00:13:28,100
sampling<font color="#CCCCCC"> you will of you we will use it</font>

370
00:13:28,100 --> 00:13:29,839
again later<font color="#CCCCCC"> on</font><font color="#E5E5E5"> to manipulate certain</font>

371
00:13:29,839 --> 00:13:32,600
stochastic gradients<font color="#CCCCCC"> if you are doing</font>

372
00:13:32,600 --> 00:13:33,980
sort of<font color="#E5E5E5"> probability theory you will</font>

373
00:13:33,980 --> 00:13:35,779
always<font color="#E5E5E5"> use</font><font color="#CCCCCC"> this trick</font><font color="#E5E5E5"> to derive certain</font>

374
00:13:35,779 --> 00:13:38,089
<font color="#E5E5E5">types of bounds to</font><font color="#CCCCCC"> show the convergence</font>

375
00:13:38,089 --> 00:13:39,800
properties and asymptotic<font color="#E5E5E5"> properties of</font>

376
00:13:39,800 --> 00:13:41,360
<font color="#CCCCCC">their convergence and then if you're</font>

377
00:13:41,360 --> 00:13:43,040
<font color="#E5E5E5">actually dealing with</font><font color="#CCCCCC"> a</font><font color="#E5E5E5"> reinforcement</font>

378
00:13:43,040 --> 00:13:44,209
learning where you're thinking<font color="#E5E5E5"> of off</font>

379
00:13:44,209 --> 00:13:46,189
policy<font color="#E5E5E5"> Corrections then you will</font><font color="#CCCCCC"> always</font>

380
00:13:46,189 --> 00:13:48,259
introduce these kind of ratios which

381
00:13:48,259 --> 00:13:49,850
will appear to handle these kind<font color="#E5E5E5"> of</font>

382
00:13:49,850 --> 00:13:52,069
<font color="#E5E5E5">settings so this is a very useful trick</font>

383
00:13:52,069 --> 00:13:54,439
to have in mind the<font color="#E5E5E5"> next kind</font><font color="#CCCCCC"> of trick</font>

384
00:13:54,439 --> 00:13:56,990
<font color="#CCCCCC">is</font><font color="#E5E5E5"> the bound and trick and there's so</font>

385
00:13:56,990 --> 00:13:58,550
many bounding tricks that I just chose

386
00:13:58,550 --> 00:14:00,560
the most general<font color="#E5E5E5"> one to do and I'll</font>

387
00:14:00,560 --> 00:14:02,509
mention a few<font color="#E5E5E5"> others at the end but one</font>

388
00:14:02,509 --> 00:14:03,920
of<font color="#E5E5E5"> the most important results from</font>

389
00:14:03,920 --> 00:14:06,290
convex analysis is that<font color="#CCCCCC"> you can always</font>

390
00:14:06,290 --> 00:14:08,689
take the<font color="#E5E5E5"> function of an expectation can</font>

391
00:14:08,689 --> 00:14:11,149
<font color="#CCCCCC">be bounded by the</font><font color="#E5E5E5"> expectation of a</font>

392
00:14:11,149 --> 00:14:13,279
function<font color="#E5E5E5"> for functions f that are</font>

393
00:14:13,279 --> 00:14:15,579
concave right and<font color="#E5E5E5"> this is super useful</font>

394
00:14:15,579 --> 00:14:17,959
<font color="#CCCCCC">because</font><font color="#E5E5E5"> sometimes dealing with the</font>

395
00:14:17,959 --> 00:14:19,879
function of<font color="#E5E5E5"> an expectation is difficult</font>

396
00:14:19,879 --> 00:14:22,100
to do and<font color="#E5E5E5"> you want to swap the integral</font>

397
00:14:22,100 --> 00:14:24,050
with the function<font color="#E5E5E5"> that's coming up there</font>

398
00:14:24,050 --> 00:14:25,819
<font color="#E5E5E5">and of course this is</font><font color="#CCCCCC"> equal if the</font>

399
00:14:25,819 --> 00:14:27,589
function<font color="#CCCCCC"> is linear</font><font color="#E5E5E5"> because expectation</font>

400
00:14:27,589 --> 00:14:29,689
is linear but for other functions<font color="#CCCCCC"> you</font>

401
00:14:29,689 --> 00:14:32,389
won't have this probability<font color="#E5E5E5"> so logs are</font>

402
00:14:32,389 --> 00:14:33,980
<font color="#CCCCCC">the one we</font><font color="#E5E5E5"> will</font><font color="#CCCCCC"> always</font><font color="#E5E5E5"> be interested in</font>

403
00:14:33,980 --> 00:14:35,569
or most often interesting<font color="#CCCCCC"> because we</font>

404
00:14:35,569 --> 00:14:37,100
want<font color="#E5E5E5"> to use log probabilities for</font>

405
00:14:37,100 --> 00:14:39,379
numerical reasons and for<font color="#CCCCCC"> simplification</font>

406
00:14:39,379 --> 00:14:42,620
of the additive property<font color="#E5E5E5"> and so we will</font>

407
00:14:42,620 --> 00:14:44,480
always take the log of<font color="#CCCCCC"> this integral</font>

408
00:14:44,480 --> 00:14:46,250
which<font color="#CCCCCC"> is the log of an expectation</font><font color="#E5E5E5"> is</font>

409
00:14:46,250 --> 00:14:49,490
the expectation of<font color="#E5E5E5"> the log and this will</font>

410
00:14:49,490 --> 00:14:53,360
be is also<font color="#CCCCCC"> used almost everywhere you</font>

411
00:14:53,360 --> 00:14:55,009
would<font color="#E5E5E5"> have seen this in optimization of</font>

412
00:14:55,009 --> 00:14:56,360
<font color="#E5E5E5">course because you are deriving what</font>

413
00:14:56,360 --> 00:14:57,829
they<font color="#E5E5E5"> ask what the rates of convergence</font>

414
00:14:57,829 --> 00:15:00,199
<font color="#E5E5E5">of optimization algorithms are we're</font>

415
00:15:00,199 --> 00:15:01,670
going<font color="#E5E5E5"> to talk about variational</font>

416
00:15:01,670 --> 00:15:04,759
inference<font color="#E5E5E5"> and variational inference is</font>

417
00:15:04,759 --> 00:15:07,459
derived on this quantity and if<font color="#CCCCCC"> you've</font>

418
00:15:07,459 --> 00:15:09,050
studied<font color="#E5E5E5"> Monte Carlo Markov chain Monte</font>

419
00:15:09,050 --> 00:15:11,689
<font color="#E5E5E5">Carlo in other courses then you would</font>

420
00:15:11,689 --> 00:15:14,389
have proven<font color="#CCCCCC"> that there</font><font color="#E5E5E5"> Monte Carlo</font>

421
00:15:14,389 --> 00:15:16,610
integrators or Markov chain<font color="#CCCCCC"> Monte Carlo</font>

422
00:15:16,610 --> 00:15:18,259
methods can have lower variance

423
00:15:18,259 --> 00:15:19,939
under what using raw<font color="#CCCCCC"> Blackwell's theorem</font>

424
00:15:19,939 --> 00:15:22,069
<font color="#E5E5E5">and the crux of our Blackwell theorem is</font>

425
00:15:22,069 --> 00:15:24,410
to use this form of Jensen's inequality

426
00:15:24,410 --> 00:15:26,779
<font color="#E5E5E5">and there are many other ways of</font>

427
00:15:26,779 --> 00:15:28,309
building kinds of bounds of

428
00:15:28,309 --> 00:15:30,559
probabilities this<font color="#E5E5E5"> way the other very</font>

429
00:15:30,559 --> 00:15:33,350
<font color="#CCCCCC">useful one</font><font color="#E5E5E5"> is to use central duality and</font>

430
00:15:33,350 --> 00:15:35,210
now the tool that<font color="#E5E5E5"> we get from convex</font>

431
00:15:35,210 --> 00:15:37,670
analysis then when you can turn<font color="#E5E5E5"> use your</font>

432
00:15:37,670 --> 00:15:39,380
<font color="#CCCCCC">central</font><font color="#E5E5E5"> jewel then you can get another</font>

433
00:15:39,380 --> 00:15:41,330
bound which will<font color="#E5E5E5"> always be convex and</font>

434
00:15:41,330 --> 00:15:43,490
bound the<font color="#CCCCCC"> probability and then that will</font>

435
00:15:43,490 --> 00:15:45,110
<font color="#E5E5E5">be</font><font color="#CCCCCC"> useful for doing</font><font color="#E5E5E5"> various</font><font color="#CCCCCC"> kinds</font><font color="#E5E5E5"> of</font>

436
00:15:45,110 --> 00:15:47,150
other variational methods holders

437
00:15:47,150 --> 00:15:48,890
inequality which just<font color="#E5E5E5"> helps you do</font>

438
00:15:48,890 --> 00:15:51,290
<font color="#E5E5E5">products of probabilities</font><font color="#CCCCCC"> into their</font>

439
00:15:51,290 --> 00:15:53,300
products of the individual norms<font color="#CCCCCC"> is</font>

440
00:15:53,300 --> 00:15:56,180
<font color="#E5E5E5">always useful and many of you</font><font color="#CCCCCC"> have heard</font>

441
00:15:56,180 --> 00:15:59,720
<font color="#E5E5E5">about optimal transport and integral</font>

442
00:15:59,720 --> 00:16:01,550
probability metrics<font color="#E5E5E5"> and that's where</font>

443
00:16:01,550 --> 00:16:03,710
<font color="#E5E5E5">this famous</font><font color="#CCCCCC"> Monch kantorovich</font><font color="#E5E5E5"> inequality</font>

444
00:16:03,710 --> 00:16:05,450
<font color="#E5E5E5">comes in which helps you derive yet</font>

445
00:16:05,450 --> 00:16:07,220
<font color="#E5E5E5">other kinds of bounds of probabilities</font>

446
00:16:07,220 --> 00:16:09,500
<font color="#E5E5E5">and these three together and when you</font>

447
00:16:09,500 --> 00:16:11,000
put all<font color="#CCCCCC"> of</font><font color="#E5E5E5"> them together you</font><font color="#CCCCCC"> can build</font>

448
00:16:11,000 --> 00:16:16,310
<font color="#CCCCCC">very flexible kinds of tools so the next</font>

449
00:16:16,310 --> 00:16:20,570
<font color="#E5E5E5">one is about evidence bounds so we want</font>

450
00:16:20,570 --> 00:16:23,030
to actually use<font color="#E5E5E5"> this trick</font><font color="#CCCCCC"> that we just</font>

451
00:16:23,030 --> 00:16:24,470
<font color="#CCCCCC">used so again this is the</font><font color="#E5E5E5"> integral</font>

452
00:16:24,470 --> 00:16:27,200
problem<font color="#E5E5E5"> I want to compute the evidence P</font>

453
00:16:27,200 --> 00:16:29,510
of X which<font color="#E5E5E5"> will be the integral of this</font>

454
00:16:29,510 --> 00:16:31,670
likelihood function P of<font color="#CCCCCC"> X given there</font>

455
00:16:31,670 --> 00:16:35,510
again some<font color="#E5E5E5"> distribution P of Z now I can</font>

456
00:16:35,510 --> 00:16:38,330
introduce this identity trick again by

457
00:16:38,330 --> 00:16:41,390
multiplying by Q and<font color="#E5E5E5"> dividing by Q and Q</font>

458
00:16:41,390 --> 00:16:44,150
of<font color="#E5E5E5"> Z was the proposal distribution in</font>

459
00:16:44,150 --> 00:16:46,910
important sampling<font color="#E5E5E5"> but here it may have</font>

460
00:16:46,910 --> 00:16:49,010
a<font color="#E5E5E5"> different name</font><font color="#CCCCCC"> well recreate the</font>

461
00:16:49,010 --> 00:16:51,040
importance<font color="#CCCCCC"> way to</font><font color="#E5E5E5"> P divided by Q</font>

462
00:16:51,040 --> 00:16:53,540
multiplied by Q<font color="#E5E5E5"> and when we did</font>

463
00:16:53,540 --> 00:16:55,400
<font color="#CCCCCC">importance sampling</font><font color="#E5E5E5"> at this point we</font>

464
00:16:55,400 --> 00:16:57,380
said we're<font color="#E5E5E5"> going to solve the</font><font color="#CCCCCC"> integral</font>

465
00:16:57,380 --> 00:17:00,320
by using Monte<font color="#E5E5E5"> Carlo</font><font color="#CCCCCC"> integration but now</font>

466
00:17:00,320 --> 00:17:02,060
we're<font color="#E5E5E5"> not going</font><font color="#CCCCCC"> to solve</font><font color="#E5E5E5"> the integral by</font>

467
00:17:02,060 --> 00:17:04,280
Monte Carlo<font color="#E5E5E5"> we're instead going to apply</font>

468
00:17:04,280 --> 00:17:06,859
Jensen's inequality<font color="#CCCCCC"> and so Jensen's</font>

469
00:17:06,859 --> 00:17:08,990
inequality so now do this on the log of

470
00:17:08,990 --> 00:17:10,760
<font color="#E5E5E5">everything so add a log</font><font color="#CCCCCC"> on</font><font color="#E5E5E5"> both sides</font>

471
00:17:10,760 --> 00:17:13,130
<font color="#E5E5E5">the log of P of X will be</font><font color="#CCCCCC"> greater than</font>

472
00:17:13,130 --> 00:17:15,500
<font color="#E5E5E5">the</font><font color="#CCCCCC"> expectation on the Q</font><font color="#E5E5E5"> of</font><font color="#CCCCCC"> the log of</font>

473
00:17:15,500 --> 00:17:18,230
this quantity<font color="#CCCCCC"> and the inside</font><font color="#E5E5E5"> right so</font>

474
00:17:18,230 --> 00:17:20,030
just read<font color="#E5E5E5"> that</font><font color="#CCCCCC"> again now this is an</font>

475
00:17:20,030 --> 00:17:23,630
integral and expectation of a Q of Z<font color="#E5E5E5"> of</font>

476
00:17:23,630 --> 00:17:27,440
this joint log probability and so we can

477
00:17:27,440 --> 00:17:29,090
<font color="#E5E5E5">just split</font><font color="#CCCCCC"> that using the property of</font>

478
00:17:29,090 --> 00:17:31,280
the log<font color="#E5E5E5"> to an integral of the log</font>

479
00:17:31,280 --> 00:17:34,040
<font color="#CCCCCC">probability</font><font color="#E5E5E5"> P</font><font color="#CCCCCC"> of X</font><font color="#E5E5E5"> given</font><font color="#CCCCCC"> Z minus the</font>

480
00:17:34,040 --> 00:17:36,440
integral<font color="#E5E5E5"> of this log ratio</font><font color="#CCCCCC"> Q of Z</font>

481
00:17:36,440 --> 00:17:38,960
divided<font color="#CCCCCC"> by P</font><font color="#E5E5E5"> of Z and this of course is</font>

482
00:17:38,960 --> 00:17:41,030
a<font color="#CCCCCC"> very famous lower bound it has very</font>

483
00:17:41,030 --> 00:17:43,400
several several names but it will be

484
00:17:43,400 --> 00:17:45,140
called the evidence<font color="#E5E5E5"> lower bound in this</font>

485
00:17:45,140 --> 00:17:45,550
case

486
00:17:45,550 --> 00:17:47,800
it's often<font color="#CCCCCC"> called</font><font color="#E5E5E5"> the variation of free</font>

487
00:17:47,800 --> 00:17:49,930
energy<font color="#E5E5E5"> or</font><font color="#CCCCCC"> just a variational lower bound</font>

488
00:17:49,930 --> 00:17:53,290
<font color="#E5E5E5">but what we can talk about that more in</font>

489
00:17:53,290 --> 00:17:56,050
<font color="#CCCCCC">the</font><font color="#E5E5E5"> next section but</font><font color="#CCCCCC"> just the point is</font>

490
00:17:56,050 --> 00:17:58,300
to know<font color="#CCCCCC"> that this is now</font><font color="#E5E5E5"> a bound on the</font>

491
00:17:58,300 --> 00:18:00,760
original quantity and knowing bounds of

492
00:18:00,760 --> 00:18:02,470
quantities<font color="#E5E5E5"> is sometimes better than</font>

493
00:18:02,470 --> 00:18:04,510
knowing the quantity itself because you

494
00:18:04,510 --> 00:18:06,340
know exactly<font color="#CCCCCC"> what the minimum was and</font>

495
00:18:06,340 --> 00:18:08,320
you can do manipulations on this which

496
00:18:08,320 --> 00:18:10,000
<font color="#E5E5E5">is easier than doing manipulations on</font>

497
00:18:10,000 --> 00:18:12,040
this and so this<font color="#CCCCCC"> is where this</font><font color="#E5E5E5"> kind of</font>

498
00:18:12,040 --> 00:18:14,380
trick of using<font color="#E5E5E5"> bounds comes in and in</font>

499
00:18:14,380 --> 00:18:16,540
all those previous inequalities<font color="#CCCCCC"> this is</font>

500
00:18:16,540 --> 00:18:18,610
the principle that<font color="#CCCCCC"> when you use holders</font>

501
00:18:18,610 --> 00:18:21,310
inequality or when you use the<font color="#E5E5E5"> central</font>

502
00:18:21,310 --> 00:18:22,840
<font color="#CCCCCC">dual to create a different kind of</font>

503
00:18:22,840 --> 00:18:24,400
bounds<font color="#E5E5E5"> you got a new</font><font color="#CCCCCC"> problem which</font><font color="#E5E5E5"> is</font>

504
00:18:24,400 --> 00:18:28,330
actually simpler to deal with<font color="#E5E5E5"> so I have</font>

505
00:18:28,330 --> 00:18:30,340
a fourth trick<font color="#E5E5E5"> which is</font><font color="#CCCCCC"> called the</font>

506
00:18:30,340 --> 00:18:33,790
density<font color="#CCCCCC"> ratio trick and</font><font color="#E5E5E5"> the density</font>

507
00:18:33,790 --> 00:18:37,930
ratio trick is is a very<font color="#E5E5E5"> simple trick</font>

508
00:18:37,930 --> 00:18:40,000
<font color="#CCCCCC">there's often like you just saw an</font>

509
00:18:40,000 --> 00:18:41,650
important sampling<font color="#E5E5E5"> there's always a</font>

510
00:18:41,650 --> 00:18:44,350
ratio<font color="#E5E5E5"> that appears some ratio of two</font>

511
00:18:44,350 --> 00:18:46,960
quantities<font color="#CCCCCC"> and this</font><font color="#E5E5E5"> density ratio trick</font>

512
00:18:46,960 --> 00:18:49,240
says sometimes<font color="#E5E5E5"> you</font><font color="#CCCCCC"> aren't at the naive</font>

513
00:18:49,240 --> 00:18:50,950
way to compute<font color="#E5E5E5"> the density ratio is to</font>

514
00:18:50,950 --> 00:18:53,290
compute<font color="#E5E5E5"> the top part compute the bottom</font>

515
00:18:53,290 --> 00:18:54,670
part then do the division<font color="#CCCCCC"> and</font><font color="#E5E5E5"> then</font>

516
00:18:54,670 --> 00:18:56,890
that's why you get the number but<font color="#E5E5E5"> that's</font>

517
00:18:56,890 --> 00:18:58,720
actually<font color="#CCCCCC"> sometimes very difficult to do</font>

518
00:18:58,720 --> 00:19:00,190
and<font color="#CCCCCC"> sometimes</font><font color="#E5E5E5"> you aren't</font><font color="#CCCCCC"> interested</font><font color="#E5E5E5"> in</font>

519
00:19:00,190 --> 00:19:02,050
knowing those individual<font color="#E5E5E5"> quantities you</font>

520
00:19:02,050 --> 00:19:04,870
only want<font color="#CCCCCC"> to know</font><font color="#E5E5E5"> the ratio directly so</font>

521
00:19:04,870 --> 00:19:07,330
often computing the ratio<font color="#CCCCCC"> is easier than</font>

522
00:19:07,330 --> 00:19:09,010
computing the individual<font color="#E5E5E5"> probabilities</font>

523
00:19:09,010 --> 00:19:11,830
and that's where this this trick comes

524
00:19:11,830 --> 00:19:14,320
in and the<font color="#CCCCCC"> trick simply says that if</font><font color="#E5E5E5"> you</font>

525
00:19:14,320 --> 00:19:15,910
<font color="#E5E5E5">want to know the</font><font color="#CCCCCC"> ratio</font><font color="#E5E5E5"> you can just</font>

526
00:19:15,910 --> 00:19:18,220
build<font color="#E5E5E5"> a classifier which will say</font>

527
00:19:18,220 --> 00:19:21,220
classify samples from the distribution<font color="#E5E5E5"> P</font>

528
00:19:21,220 --> 00:19:22,570
<font color="#CCCCCC">style versus</font><font color="#E5E5E5"> the samples from the</font>

529
00:19:22,570 --> 00:19:24,730
<font color="#E5E5E5">distribution Q star and if you can build</font>

530
00:19:24,730 --> 00:19:26,590
this classifier than that classifier has

531
00:19:26,590 --> 00:19:28,630
<font color="#CCCCCC">all the information that</font><font color="#E5E5E5"> you need</font><font color="#CCCCCC"> to</font>

532
00:19:28,630 --> 00:19:31,300
<font color="#E5E5E5">compute the</font><font color="#CCCCCC"> ratio of</font><font color="#E5E5E5"> P divided by Q</font><font color="#CCCCCC"> and</font>

533
00:19:31,300 --> 00:19:33,340
in fact you just<font color="#CCCCCC"> need to do this ratio</font>

534
00:19:33,340 --> 00:19:36,910
<font color="#E5E5E5">of P</font><font color="#CCCCCC"> of coming from</font><font color="#E5E5E5"> class P to this is 1</font>

535
00:19:36,910 --> 00:19:39,490
minus P<font color="#CCCCCC"> so not coming</font><font color="#E5E5E5"> from class P I'll</font>

536
00:19:39,490 --> 00:19:40,990
show you<font color="#CCCCCC"> we'll do the derivation</font><font color="#E5E5E5"> in the</font>

537
00:19:40,990 --> 00:19:43,330
next line but this<font color="#E5E5E5"> density ratio trick</font>

538
00:19:43,330 --> 00:19:44,980
is<font color="#E5E5E5"> also one of the most famous tricks we</font>

539
00:19:44,980 --> 00:19:47,080
have in machine learning<font color="#E5E5E5"> of course if</font>

540
00:19:47,080 --> 00:19:48,580
you've heard of<font color="#E5E5E5"> generative adversarial</font>

541
00:19:48,580 --> 00:19:50,770
networks<font color="#E5E5E5"> this is the core quantity or</font>

542
00:19:50,770 --> 00:19:52,630
the<font color="#E5E5E5"> core idea underlying</font><font color="#CCCCCC"> generates</font><font color="#E5E5E5"> about</font>

543
00:19:52,630 --> 00:19:55,030
the<font color="#CCCCCC"> cero Network but outside and if you</font>

544
00:19:55,030 --> 00:19:57,610
go a<font color="#E5E5E5"> bit wider</font><font color="#CCCCCC"> in</font><font color="#E5E5E5"> machine learning a bit</font>

545
00:19:57,610 --> 00:19:58,870
<font color="#CCCCCC">outside of</font><font color="#E5E5E5"> deep learning</font>

546
00:19:58,870 --> 00:20:00,100
<font color="#E5E5E5">then you'll find</font><font color="#CCCCCC"> a method</font><font color="#E5E5E5"> called noise</font>

547
00:20:00,100 --> 00:20:02,110
<font color="#CCCCCC">contrastive</font><font color="#E5E5E5"> estimation</font><font color="#CCCCCC"> and if you want</font>

548
00:20:02,110 --> 00:20:03,370
to derive the noise<font color="#CCCCCC"> contrastive</font>

549
00:20:03,370 --> 00:20:04,990
estimator this is the trick that's being

550
00:20:04,990 --> 00:20:07,450
used<font color="#CCCCCC"> if you've been</font><font color="#E5E5E5"> studying Markov</font>

551
00:20:07,450 --> 00:20:09,220
chain Monte Carlo methods there's a an

552
00:20:09,220 --> 00:20:11,200
approach<font color="#CCCCCC"> called</font><font color="#E5E5E5"> approximate</font><font color="#CCCCCC"> Bayesian</font>

553
00:20:11,200 --> 00:20:13,000
computation<font color="#E5E5E5"> and one way</font><font color="#CCCCCC"> of</font><font color="#E5E5E5"> doing</font>

554
00:20:13,000 --> 00:20:14,530
approximate Bayesian computation is

555
00:20:14,530 --> 00:20:16,600
using classifier by exploiting this

556
00:20:16,600 --> 00:20:18,640
trick<font color="#CCCCCC"> some of you would have done</font>

557
00:20:18,640 --> 00:20:21,010
<font color="#E5E5E5">hypothesis testing in the past and if</font>

558
00:20:21,010 --> 00:20:22,960
you want to do hypothesis testing<font color="#CCCCCC"> then</font>

559
00:20:22,960 --> 00:20:25,450
one way of doing<font color="#CCCCCC"> two</font><font color="#E5E5E5"> sample hypothesis</font>

560
00:20:25,450 --> 00:20:27,910
testing is to use this kind of trick<font color="#CCCCCC"> and</font>

561
00:20:27,910 --> 00:20:29,650
again any method where<font color="#CCCCCC"> you have a</font>

562
00:20:29,650 --> 00:20:31,300
difference<font color="#E5E5E5"> between the</font><font color="#CCCCCC"> distribution at</font>

563
00:20:31,300 --> 00:20:33,429
test<font color="#CCCCCC"> time versus training time what they</font>

564
00:20:33,429 --> 00:20:35,650
call the<font color="#CCCCCC"> covariant</font><font color="#E5E5E5"> or calibration</font>

565
00:20:35,650 --> 00:20:38,020
problem<font color="#CCCCCC"> will do that</font><font color="#E5E5E5"> is their question</font>

566
00:20:38,020 --> 00:20:46,480
yes<font color="#CCCCCC"> okay so let's do the derivation</font><font color="#E5E5E5"> and</font>

567
00:20:46,480 --> 00:20:48,970
then ask me this again<font color="#CCCCCC"> are there</font><font color="#E5E5E5"> any</font>

568
00:20:48,970 --> 00:20:51,880
other<font color="#E5E5E5"> questions at this point</font><font color="#CCCCCC"> or let's</font>

569
00:20:51,880 --> 00:20:53,290
do the<font color="#CCCCCC"> derivation then</font><font color="#E5E5E5"> we can have a</font>

570
00:20:53,290 --> 00:20:56,559
discussion<font color="#CCCCCC"> around all</font><font color="#E5E5E5"> these tricks</font><font color="#CCCCCC"> so</font>

571
00:20:56,559 --> 00:20:58,179
okay<font color="#E5E5E5"> let's do a reminder here's the</font>

572
00:20:58,179 --> 00:20:59,770
problem<font color="#E5E5E5"> you're interested in there's the</font>

573
00:20:59,770 --> 00:21:02,230
density<font color="#E5E5E5"> ratio of P divided by Q and it's</font>

574
00:21:02,230 --> 00:21:04,179
always<font color="#E5E5E5"> useful to keep in mind Bayes rule</font>

575
00:21:04,179 --> 00:21:06,010
so Bayes rule<font color="#CCCCCC"> it's just a rule to do</font>

576
00:21:06,010 --> 00:21:08,410
inverting probabilities so to invert the

577
00:21:08,410 --> 00:21:10,270
probability<font color="#E5E5E5"> of Y given X</font><font color="#CCCCCC"> to</font><font color="#E5E5E5"> the</font>

578
00:21:10,270 --> 00:21:12,220
<font color="#CCCCCC">probability of</font><font color="#E5E5E5"> x given Y using the ratio</font>

579
00:21:12,220 --> 00:21:15,130
<font color="#CCCCCC">of</font><font color="#E5E5E5"> the two marginals so what we're going</font>

580
00:21:15,130 --> 00:21:17,410
<font color="#E5E5E5">to do is we're</font><font color="#CCCCCC"> going to</font><font color="#E5E5E5"> sample data from</font>

581
00:21:17,410 --> 00:21:20,980
P star and sample data from Q of X and

582
00:21:20,980 --> 00:21:22,690
we're<font color="#CCCCCC"> going</font><font color="#E5E5E5"> to call all the data from P</font>

583
00:21:22,690 --> 00:21:25,210
star<font color="#CCCCCC"> x hat and all the data from</font><font color="#E5E5E5"> Q X</font>

584
00:21:25,210 --> 00:21:26,440
tilde<font color="#E5E5E5"> and we're going to put them</font>

585
00:21:26,440 --> 00:21:28,960
together<font color="#E5E5E5"> into</font><font color="#CCCCCC"> one big</font><font color="#E5E5E5"> data set then</font>

586
00:21:28,960 --> 00:21:30,610
because we are manipulating<font color="#CCCCCC"> the</font>

587
00:21:30,610 --> 00:21:32,559
probability we can<font color="#E5E5E5"> always introduce the</font>

588
00:21:32,559 --> 00:21:35,140
label so for every data<font color="#CCCCCC"> point we are</font>

589
00:21:35,140 --> 00:21:36,670
<font color="#E5E5E5">going to introduce</font><font color="#CCCCCC"> a label and a new</font>

590
00:21:36,670 --> 00:21:37,750
random<font color="#CCCCCC"> variable so we're</font><font color="#E5E5E5"> going to</font>

591
00:21:37,750 --> 00:21:40,000
introduce<font color="#E5E5E5"> this random variable Y and for</font>

592
00:21:40,000 --> 00:21:42,130
all the data<font color="#CCCCCC"> that came from P star we're</font>

593
00:21:42,130 --> 00:21:44,170
gonna give<font color="#CCCCCC"> it a label plus one and for</font>

594
00:21:44,170 --> 00:21:46,179
all the data<font color="#E5E5E5"> that came from Q we're</font>

595
00:21:46,179 --> 00:21:48,309
gonna give the label<font color="#CCCCCC"> minus</font><font color="#E5E5E5"> one so that's</font>

596
00:21:48,309 --> 00:21:51,280
<font color="#CCCCCC">not enough to set up a classification</font><font color="#E5E5E5"> or</font>

597
00:21:51,280 --> 00:21:53,410
a decision<font color="#E5E5E5"> problem</font><font color="#CCCCCC"> so there's an</font>

598
00:21:53,410 --> 00:21:55,540
equivalence by doing this<font color="#E5E5E5"> construction</font>

599
00:21:55,540 --> 00:21:58,390
by construction<font color="#E5E5E5"> P star of X can be</font>

600
00:21:58,390 --> 00:22:00,429
<font color="#E5E5E5">written as the probability</font><font color="#CCCCCC"> of P of X</font>

601
00:22:00,429 --> 00:22:03,580
<font color="#CCCCCC">given the label</font><font color="#E5E5E5"> y equals plus</font><font color="#CCCCCC"> 1 because</font>

602
00:22:03,580 --> 00:22:05,140
that's<font color="#E5E5E5"> how we defined it we said every</font>

603
00:22:05,140 --> 00:22:06,880
data point<font color="#CCCCCC"> that</font><font color="#E5E5E5"> came from what must be</font>

604
00:22:06,880 --> 00:22:09,910
from P and<font color="#CCCCCC"> similarly</font><font color="#E5E5E5"> from Q and then we</font>

605
00:22:09,910 --> 00:22:11,350
have this<font color="#CCCCCC"> too so now you can see the</font>

606
00:22:11,350 --> 00:22:12,150
trick

607
00:22:12,150 --> 00:22:14,940
gonna do<font color="#E5E5E5"> P star of X divided</font><font color="#CCCCCC"> by Q of</font><font color="#E5E5E5"> X</font>

608
00:22:14,940 --> 00:22:16,830
can<font color="#CCCCCC"> be rewritten as</font><font color="#E5E5E5"> the ratio</font><font color="#CCCCCC"> of these</font>

609
00:22:16,830 --> 00:22:18,900
two conditional distributions which are

610
00:22:18,900 --> 00:22:21,450
equivalent in all forms<font color="#E5E5E5"> now we're gonna</font>

611
00:22:21,450 --> 00:22:24,300
replace<font color="#E5E5E5"> knowing P of X given Y is</font>

612
00:22:24,300 --> 00:22:26,160
actually<font color="#E5E5E5"> not nice to use my Bayes rule</font>

613
00:22:26,160 --> 00:22:28,800
will help us to undo that<font color="#CCCCCC"> so we're</font><font color="#E5E5E5"> going</font>

614
00:22:28,800 --> 00:22:30,840
<font color="#E5E5E5">to do the base substitution and we're</font>

615
00:22:30,840 --> 00:22:32,340
going<font color="#E5E5E5"> to replace all of</font><font color="#CCCCCC"> this with</font>

616
00:22:32,340 --> 00:22:34,980
distributions of P<font color="#CCCCCC"> of Y given X instead</font>

617
00:22:34,980 --> 00:22:36,840
so this is where the classification is

618
00:22:36,840 --> 00:22:38,820
going to come in<font color="#E5E5E5"> now I'm doing a slight</font>

619
00:22:38,820 --> 00:22:41,220
assumption here I'm<font color="#E5E5E5"> assuming that the</font>

620
00:22:41,220 --> 00:22:44,010
probability of<font color="#E5E5E5"> P of y equals 1 is equal</font>

621
00:22:44,010 --> 00:22:45,990
to<font color="#E5E5E5"> the probability of P equals y</font><font color="#CCCCCC"> equals</font>

622
00:22:45,990 --> 00:22:47,490
<font color="#E5E5E5">minus 1 so I'm assuming there's a</font>

623
00:22:47,490 --> 00:22:49,890
<font color="#E5E5E5">balance</font><font color="#CCCCCC"> data set because I got to just</font>

624
00:22:49,890 --> 00:22:51,930
draw the samples from P<font color="#CCCCCC"> and Q but if</font>

625
00:22:51,930 --> 00:22:53,430
they are not balanced then<font color="#E5E5E5"> you will</font>

626
00:22:53,430 --> 00:22:55,050
actually have<font color="#CCCCCC"> the ratio of the imbalance</font>

627
00:22:55,050 --> 00:22:57,240
<font color="#E5E5E5">which will be these two quantities and</font>

628
00:22:57,240 --> 00:22:59,520
then P of X and P of<font color="#E5E5E5"> X is the same on</font>

629
00:22:59,520 --> 00:23:01,410
both<font color="#CCCCCC"> sides</font><font color="#E5E5E5"> because the data</font><font color="#CCCCCC"> is invariant</font>

630
00:23:01,410 --> 00:23:03,480
<font color="#E5E5E5">and those two will cancel</font><font color="#CCCCCC"> so then what</font>

631
00:23:03,480 --> 00:23:04,740
you're left<font color="#E5E5E5"> with is this class</font>

632
00:23:04,740 --> 00:23:07,410
probability it says<font color="#E5E5E5"> basically to compute</font>

633
00:23:07,410 --> 00:23:08,910
the<font color="#CCCCCC"> ratio</font><font color="#E5E5E5"> you just need to know the</font>

634
00:23:08,910 --> 00:23:11,040
ratio<font color="#E5E5E5"> of the two class</font><font color="#CCCCCC"> probability you</font>

635
00:23:11,040 --> 00:23:12,870
just need to know P of y equals<font color="#E5E5E5"> 1 given</font>

636
00:23:12,870 --> 00:23:16,110
<font color="#CCCCCC">X divided by P</font><font color="#E5E5E5"> of y equals -1 given X</font>

637
00:23:16,110 --> 00:23:18,180
right<font color="#E5E5E5"> and so that is the point of this</font>

638
00:23:18,180 --> 00:23:20,550
<font color="#E5E5E5">trip it says computing a density ratio</font>

639
00:23:20,550 --> 00:23:24,870
is equivalent to compress tomato the

640
00:23:24,870 --> 00:23:27,180
class probability estimation<font color="#E5E5E5"> then you do</font>

641
00:23:27,180 --> 00:23:28,830
that<font color="#CCCCCC"> so now coming to your question at</font>

642
00:23:28,830 --> 00:23:30,570
the<font color="#CCCCCC"> end the</font><font color="#E5E5E5"> thing that answers your</font>

643
00:23:30,570 --> 00:23:32,730
question is that<font color="#E5E5E5"> you will do this</font>

644
00:23:32,730 --> 00:23:35,580
<font color="#E5E5E5">quantity for</font><font color="#CCCCCC"> 1x</font><font color="#E5E5E5"> but you will do this in</font>

645
00:23:35,580 --> 00:23:38,190
expectation<font color="#CCCCCC"> over the whole data set</font><font color="#E5E5E5"> or</font>

646
00:23:38,190 --> 00:23:40,970
over<font color="#CCCCCC"> two different data sets</font><font color="#E5E5E5"> and then</font>

647
00:23:40,970 --> 00:23:42,840
that's<font color="#CCCCCC"> how you</font><font color="#E5E5E5"> will use this for</font>

648
00:23:42,840 --> 00:23:46,860
<font color="#E5E5E5">learning so</font><font color="#CCCCCC"> ok so that's basically this</font>

649
00:23:46,860 --> 00:23:49,290
<font color="#E5E5E5">density ratio trick and I have two last</font>

650
00:23:49,290 --> 00:23:53,100
tricks and then<font color="#E5E5E5"> then let's summarize and</font>

651
00:23:53,100 --> 00:23:55,350
have a<font color="#E5E5E5"> discussion about what these</font>

652
00:23:55,350 --> 00:23:58,350
tricks mean so that<font color="#CCCCCC"> was about</font>

653
00:23:58,350 --> 00:24:00,480
<font color="#E5E5E5">manipulating densities and now I want to</font>

654
00:24:00,480 --> 00:24:02,810
go to the<font color="#E5E5E5"> problem of manipulating</font>

655
00:24:02,810 --> 00:24:05,310
gradients themselves so one<font color="#CCCCCC"> of the most</font>

656
00:24:05,310 --> 00:24:07,800
common problems<font color="#CCCCCC"> in all of machine</font>

657
00:24:07,800 --> 00:24:09,960
learning<font color="#CCCCCC"> in fact all of statistical</font>

658
00:24:09,960 --> 00:24:11,730
science whether you call it statistic

659
00:24:11,730 --> 00:24:14,760
operations research in finance<font color="#CCCCCC"> is to</font>

660
00:24:14,760 --> 00:24:16,290
compute this quantity<font color="#CCCCCC"> you have an</font>

661
00:24:16,290 --> 00:24:18,900
expectation of a function<font color="#E5E5E5"> and the</font>

662
00:24:18,900 --> 00:24:20,430
expectations with<font color="#CCCCCC"> respect to some</font>

663
00:24:20,430 --> 00:24:22,320
<font color="#CCCCCC">distribution Q and you want to compute</font>

664
00:24:22,320 --> 00:24:23,910
<font color="#E5E5E5">the gradient of this course this</font>

665
00:24:23,910 --> 00:24:25,830
<font color="#CCCCCC">expectation</font><font color="#E5E5E5"> but if you want to compute</font>

666
00:24:25,830 --> 00:24:26,160
<font color="#E5E5E5">the</font>

667
00:24:26,160 --> 00:24:27,570
radiant with respect to these district

668
00:24:27,570 --> 00:24:30,240
these parameters<font color="#E5E5E5"> Phi which live in</font><font color="#CCCCCC"> the</font>

669
00:24:30,240 --> 00:24:31,860
<font color="#E5E5E5">distribution with which you're taking</font>

670
00:24:31,860 --> 00:24:34,680
the<font color="#CCCCCC"> expectation</font><font color="#E5E5E5"> so I'm just gonna</font>

671
00:24:34,680 --> 00:24:36,420
rewrite that integral<font color="#E5E5E5"> out it's the</font>

672
00:24:36,420 --> 00:24:38,190
gradient<font color="#CCCCCC"> of the integral of the</font>

673
00:24:38,190 --> 00:24:40,410
distribution<font color="#E5E5E5"> Q what the</font><font color="#CCCCCC"> parameter is Phi</font>

674
00:24:40,410 --> 00:24:42,600
that we are introducing<font color="#CCCCCC"> in again some</font>

675
00:24:42,600 --> 00:24:44,580
function<font color="#E5E5E5"> f that function may have some</font>

676
00:24:44,580 --> 00:24:46,830
other parameters that<font color="#E5E5E5"> for the purpose of</font>

677
00:24:46,830 --> 00:24:48,690
<font color="#E5E5E5">this gradient computation we aren't</font>

678
00:24:48,690 --> 00:24:51,480
<font color="#E5E5E5">going to be interested in and if the if</font>

679
00:24:51,480 --> 00:24:53,280
all<font color="#E5E5E5"> these were simple distributions</font><font color="#CCCCCC"> that</font>

680
00:24:53,280 --> 00:24:54,900
<font color="#CCCCCC">linear functions and it's may be</font>

681
00:24:54,900 --> 00:24:56,790
<font color="#CCCCCC">one-dimensional you'd be able</font><font color="#E5E5E5"> to compute</font>

682
00:24:56,790 --> 00:24:59,070
<font color="#E5E5E5">this integral very easily but in general</font>

683
00:24:59,070 --> 00:25:01,350
you won't know this<font color="#E5E5E5"> be</font><font color="#CCCCCC"> able</font><font color="#E5E5E5"> to compute</font>

684
00:25:01,350 --> 00:25:02,730
the<font color="#E5E5E5"> integral and if you don't know the</font>

685
00:25:02,730 --> 00:25:04,200
integral you can't<font color="#E5E5E5"> compute the gradient</font>

686
00:25:04,200 --> 00:25:07,470
and that's because these<font color="#CCCCCC"> are high</font>

687
00:25:07,470 --> 00:25:09,000
dimensional quantities and because

688
00:25:09,000 --> 00:25:10,140
they're high dimensional you won't be

689
00:25:10,140 --> 00:25:12,750
able<font color="#CCCCCC"> to do the the gradient you</font><font color="#E5E5E5"> won't</font>

690
00:25:12,750 --> 00:25:14,520
know<font color="#E5E5E5"> the expectation in closed</font><font color="#CCCCCC"> form</font>

691
00:25:14,520 --> 00:25:16,200
because typically you have<font color="#E5E5E5"> nonlinear</font>

692
00:25:16,200 --> 00:25:18,300
functions and very<font color="#E5E5E5"> complicated non</font>

693
00:25:18,300 --> 00:25:20,610
primitive distributions and the facts

694
00:25:20,610 --> 00:25:22,380
what<font color="#E5E5E5"> really makes it complicated is</font><font color="#CCCCCC"> that</font>

695
00:25:22,380 --> 00:25:24,360
the gradient<font color="#E5E5E5"> I introduced interested in</font>

696
00:25:24,360 --> 00:25:26,190
is with respect<font color="#E5E5E5"> to these parameters</font><font color="#CCCCCC"> Phi</font>

697
00:25:26,190 --> 00:25:28,020
and so we're going to need<font color="#E5E5E5"> to do several</font>

698
00:25:28,020 --> 00:25:31,770
<font color="#E5E5E5">tricks to manipulate this but I just</font>

699
00:25:31,770 --> 00:25:33,720
<font color="#E5E5E5">wanted to point out where you would see</font>

700
00:25:33,720 --> 00:25:36,060
this integral<font color="#E5E5E5"> so we're gonna talk this</font>

701
00:25:36,060 --> 00:25:37,590
<font color="#E5E5E5">integral is obviously one of the key</font>

702
00:25:37,590 --> 00:25:39,270
<font color="#E5E5E5">things to</font><font color="#CCCCCC"> Jean learning in generative</font>

703
00:25:39,270 --> 00:25:41,460
<font color="#E5E5E5">models so we'll see in all the problems</font>

704
00:25:41,460 --> 00:25:42,960
of inference in generative models that

705
00:25:42,960 --> 00:25:45,540
we<font color="#CCCCCC"> use if you've already started down a</font>

706
00:25:45,540 --> 00:25:46,890
lot of reinforcement<font color="#CCCCCC"> learning</font><font color="#E5E5E5"> and</font>

707
00:25:46,890 --> 00:25:48,840
control and this<font color="#E5E5E5"> is</font><font color="#CCCCCC"> the key question</font><font color="#E5E5E5"> to</font>

708
00:25:48,840 --> 00:25:51,150
computing<font color="#E5E5E5"> the expected expectation under</font>

709
00:25:51,150 --> 00:25:53,040
<font color="#E5E5E5">your</font><font color="#CCCCCC"> policy and doing</font><font color="#E5E5E5"> policy learning in</font>

710
00:25:53,040 --> 00:25:56,070
the policy gradient<font color="#CCCCCC"> a framework</font><font color="#E5E5E5"> for</font>

711
00:25:56,070 --> 00:25:57,420
<font color="#E5E5E5">doing reinforcement learning in</font>

712
00:25:57,420 --> 00:25:59,760
operations research you would<font color="#CCCCCC"> F</font><font color="#E5E5E5"> is set</font>

713
00:25:59,760 --> 00:26:01,710
<font color="#E5E5E5">up the same problem of estimating a</font>

714
00:26:01,710 --> 00:26:03,150
<font color="#CCCCCC">queueing problem and then</font><font color="#E5E5E5"> wanting to</font>

715
00:26:03,150 --> 00:26:04,980
know the<font color="#E5E5E5"> probability or</font><font color="#CCCCCC"> queueing rate</font>

716
00:26:04,980 --> 00:26:06,720
and then<font color="#E5E5E5"> you would get</font><font color="#CCCCCC"> the same kind of</font>

717
00:26:06,720 --> 00:26:08,940
question coming<font color="#E5E5E5"> up if you're doing Monte</font>

718
00:26:08,940 --> 00:26:10,710
Carlo simulation in finance<font color="#CCCCCC"> they</font>

719
00:26:10,710 --> 00:26:12,240
<font color="#CCCCCC">actually have a name for</font><font color="#E5E5E5"> this gradient</font>

720
00:26:12,240 --> 00:26:14,460
<font color="#E5E5E5">it's some Greek alphabet Alpha Gamma</font>

721
00:26:14,460 --> 00:26:16,560
Delta<font color="#CCCCCC"> it just depends which gradient you</font>

722
00:26:16,560 --> 00:26:18,390
are taking so all the<font color="#E5E5E5"> finance is</font>

723
00:26:18,390 --> 00:26:20,160
basically about computing this integral

724
00:26:20,160 --> 00:26:22,050
and they have entire<font color="#E5E5E5"> textbooks</font><font color="#CCCCCC"> just to</font>

725
00:26:22,050 --> 00:26:23,880
give us<font color="#CCCCCC"> all</font><font color="#E5E5E5"> the tricks to compute this</font>

726
00:26:23,880 --> 00:26:26,220
integral<font color="#E5E5E5"> and actually in many other</font>

727
00:26:26,220 --> 00:26:28,560
<font color="#E5E5E5">areas of optimization they will call</font>

728
00:26:28,560 --> 00:26:30,900
knowing this quantity or doing this kind

729
00:26:30,900 --> 00:26:33,390
of analysis sensitivity analysis so it's

730
00:26:33,390 --> 00:26:35,160
<font color="#E5E5E5">really one of the most</font><font color="#CCCCCC"> fundamental</font>

731
00:26:35,160 --> 00:26:36,570
<font color="#CCCCCC">things that we know a lot of</font><font color="#E5E5E5"> tricks from</font>

732
00:26:36,570 --> 00:26:39,070
many other<font color="#CCCCCC"> areas so they're to basically</font>

733
00:26:39,070 --> 00:26:40,419
two things<font color="#E5E5E5"> you</font><font color="#CCCCCC"> can do with an integral</font>

734
00:26:40,419 --> 00:26:42,429
<font color="#CCCCCC">like this we manipulate you do some</font>

735
00:26:42,429 --> 00:26:44,950
trick with<font color="#E5E5E5"> F or</font><font color="#CCCCCC"> you do some trick with Q</font>

736
00:26:44,950 --> 00:26:46,659
and basically this is<font color="#E5E5E5"> everything that</font>

737
00:26:46,659 --> 00:26:48,580
<font color="#E5E5E5">you can do there are</font><font color="#CCCCCC"> two other tricks</font>

738
00:26:48,580 --> 00:26:50,289
<font color="#E5E5E5">you can do on an integral like</font><font color="#CCCCCC"> this but</font>

739
00:26:50,289 --> 00:26:52,330
not useful<font color="#CCCCCC"> for us</font><font color="#E5E5E5"> this machine</font><font color="#CCCCCC"> learners</font>

740
00:26:52,330 --> 00:26:54,130
because we want to<font color="#E5E5E5"> build scalable</font>

741
00:26:54,130 --> 00:26:56,970
<font color="#E5E5E5">large-scale easily easily codable</font>

742
00:26:56,970 --> 00:27:00,130
<font color="#CCCCCC">solutions so let's look at those two and</font>

743
00:27:00,130 --> 00:27:02,169
I'm<font color="#E5E5E5"> going to call things where we</font>

744
00:27:02,169 --> 00:27:04,480
manipulate<font color="#CCCCCC"> F a path wise estimator and</font>

745
00:27:04,480 --> 00:27:06,220
you'll see why<font color="#CCCCCC"> we'll call it the</font>

746
00:27:06,220 --> 00:27:08,529
pathways estimator later and we'll call

747
00:27:08,529 --> 00:27:11,019
things where we manipulate Q a score

748
00:27:11,019 --> 00:27:12,820
function estimate and<font color="#E5E5E5"> you also see why</font>

749
00:27:12,820 --> 00:27:15,789
<font color="#E5E5E5">that's the case</font><font color="#CCCCCC"> so</font><font color="#E5E5E5"> the first one is</font>

750
00:27:15,789 --> 00:27:17,769
<font color="#E5E5E5">based on the long term relative trick</font>

751
00:27:17,769 --> 00:27:20,200
and it's not really a trick<font color="#CCCCCC"> because</font><font color="#E5E5E5"> it's</font>

752
00:27:20,200 --> 00:27:22,299
basic basic calculus<font color="#CCCCCC"> it says the</font>

753
00:27:22,299 --> 00:27:24,429
gradient of<font color="#E5E5E5"> a log is just equal</font><font color="#CCCCCC"> to that</font>

754
00:27:24,429 --> 00:27:26,200
quantity the gradient of the quantity

755
00:27:26,200 --> 00:27:28,870
divided<font color="#E5E5E5"> by itself right</font><font color="#CCCCCC"> so the</font><font color="#E5E5E5"> gradient</font>

756
00:27:28,870 --> 00:27:30,940
of<font color="#E5E5E5"> a log must be this and you're just</font>

757
00:27:30,940 --> 00:27:33,220
<font color="#E5E5E5">going</font><font color="#CCCCCC"> to use this fact</font><font color="#E5E5E5"> that you can jump</font>

758
00:27:33,220 --> 00:27:35,649
between<font color="#E5E5E5"> so why this is important is that</font>

759
00:27:35,649 --> 00:27:37,389
it takes you<font color="#E5E5E5"> from gradients of a log</font>

760
00:27:37,389 --> 00:27:39,039
probability to gradients<font color="#E5E5E5"> of a</font>

761
00:27:39,039 --> 00:27:40,929
probability<font color="#CCCCCC"> so you can just jump back</font>

762
00:27:40,929 --> 00:27:42,820
<font color="#E5E5E5">and forth between these</font><font color="#CCCCCC"> two things</font>

763
00:27:42,820 --> 00:27:44,980
whenever<font color="#E5E5E5"> you like so and that kind of</font>

764
00:27:44,980 --> 00:27:47,649
<font color="#E5E5E5">manipulation is</font><font color="#CCCCCC"> very flexible</font><font color="#E5E5E5"> because it</font>

765
00:27:47,649 --> 00:27:49,840
lets you rewrite the<font color="#E5E5E5"> integral in much</font>

766
00:27:49,840 --> 00:27:51,399
more interesting ways<font color="#CCCCCC"> so it has several</font>

767
00:27:51,399 --> 00:27:53,320
<font color="#E5E5E5">useful properties which we've already</font>

768
00:27:53,320 --> 00:27:55,629
seen so if you have done a<font color="#E5E5E5"> course in</font>

769
00:27:55,629 --> 00:27:57,460
<font color="#CCCCCC">statistics and you</font><font color="#E5E5E5"> know what maximum</font>

770
00:27:57,460 --> 00:27:59,769
likelihood is which you<font color="#CCCCCC"> all do</font><font color="#E5E5E5"> then you</font>

771
00:27:59,769 --> 00:28:01,990
would obviously know this key<font color="#CCCCCC"> quantity</font>

772
00:28:01,990 --> 00:28:03,879
that gradient of a log probability is

773
00:28:03,879 --> 00:28:06,850
called the score<font color="#CCCCCC"> and</font><font color="#E5E5E5"> the expectation of</font>

774
00:28:06,850 --> 00:28:09,190
the score is always<font color="#E5E5E5"> zero and this is why</font>

775
00:28:09,190 --> 00:28:11,919
we<font color="#CCCCCC"> actually use</font><font color="#E5E5E5"> it for and how</font><font color="#CCCCCC"> you</font><font color="#E5E5E5"> prove</font>

776
00:28:11,919 --> 00:28:13,629
this<font color="#CCCCCC"> and this is part</font><font color="#E5E5E5"> of building</font><font color="#CCCCCC"> this</font>

777
00:28:13,629 --> 00:28:16,000
probabilistic flexibility<font color="#E5E5E5"> you'll have to</font>

778
00:28:16,000 --> 00:28:17,620
derive<font color="#E5E5E5"> it</font><font color="#CCCCCC"> on paper now but</font><font color="#E5E5E5"> eventually</font>

779
00:28:17,620 --> 00:28:19,389
you<font color="#CCCCCC"> should all get to</font><font color="#E5E5E5"> the point where</font>

780
00:28:19,389 --> 00:28:20,830
you see this thing and then you see well

781
00:28:20,830 --> 00:28:22,360
<font color="#CCCCCC">oh</font><font color="#E5E5E5"> obviously this is true because</font><font color="#CCCCCC"> I can</font>

782
00:28:22,360 --> 00:28:24,580
replace this graded by Delta Q divided

783
00:28:24,580 --> 00:28:27,190
<font color="#CCCCCC">by Q this Q that</font><font color="#E5E5E5"> Q will cancel you take</font>

784
00:28:27,190 --> 00:28:28,870
the grade<font color="#E5E5E5"> and outside</font><font color="#CCCCCC"> of the expectation</font>

785
00:28:28,870 --> 00:28:30,850
<font color="#E5E5E5">the gradient the integral of a</font>

786
00:28:30,850 --> 00:28:32,559
distribution<font color="#E5E5E5"> is</font><font color="#CCCCCC"> one in the</font><font color="#E5E5E5"> gradient of</font>

787
00:28:32,559 --> 00:28:33,940
something that's<font color="#E5E5E5"> one is zero and that's</font>

788
00:28:33,940 --> 00:28:36,549
why<font color="#E5E5E5"> this</font><font color="#CCCCCC"> thing</font><font color="#E5E5E5"> is</font><font color="#CCCCCC"> it's true right</font><font color="#E5E5E5"> and</font>

789
00:28:36,549 --> 00:28:38,500
this thing you<font color="#CCCCCC"> have also seen if you</font>

790
00:28:38,500 --> 00:28:39,909
have started<font color="#E5E5E5"> the policy gradient theorem</font>

791
00:28:39,909 --> 00:28:41,620
because this was the key thing<font color="#CCCCCC"> that they</font>

792
00:28:41,620 --> 00:28:43,779
<font color="#E5E5E5">used to make</font><font color="#CCCCCC"> sure that it was sensible</font>

793
00:28:43,779 --> 00:28:45,789
<font color="#E5E5E5">and then the other interesting property</font>

794
00:28:45,789 --> 00:28:47,740
<font color="#CCCCCC">of the score is that the variance of the</font>

795
00:28:47,740 --> 00:28:48,909
score is called the Fisher information

796
00:28:48,909 --> 00:28:51,580
<font color="#CCCCCC">and this</font><font color="#E5E5E5"> is one</font><font color="#CCCCCC"> of the most important</font>

797
00:28:51,580 --> 00:28:52,590
quantities in all of

798
00:28:52,590 --> 00:28:55,140
<font color="#CCCCCC">Nonnie</font><font color="#E5E5E5"> because this is the claim or</font><font color="#CCCCCC"> a</font>

799
00:28:55,140 --> 00:28:56,580
lower bound which<font color="#CCCCCC"> is the minimum</font>

800
00:28:56,580 --> 00:28:58,470
<font color="#CCCCCC">variance</font><font color="#E5E5E5"> estimator that you can get is</font>

801
00:28:58,470 --> 00:29:00,450
defining<font color="#E5E5E5"> this in terms of this quantity</font>

802
00:29:00,450 --> 00:29:02,970
<font color="#CCCCCC">and so all of the properties of</font><font color="#E5E5E5"> maximum</font>

803
00:29:02,970 --> 00:29:05,070
<font color="#CCCCCC">likelihood that we get come from</font><font color="#E5E5E5"> this</font>

804
00:29:05,070 --> 00:29:08,270
just this simple<font color="#CCCCCC"> using this simple trick</font>

805
00:29:08,270 --> 00:29:11,250
<font color="#E5E5E5">so let's use this trick to manipulate</font>

806
00:29:11,250 --> 00:29:13,919
that<font color="#E5E5E5"> first integral that we have so</font>

807
00:29:13,919 --> 00:29:16,080
here's the integral<font color="#CCCCCC"> and now it's the</font>

808
00:29:16,080 --> 00:29:18,720
gradient again<font color="#CCCCCC"> of expectation of a</font>

809
00:29:18,720 --> 00:29:20,640
function<font color="#CCCCCC"> f with respect to distribution</font>

810
00:29:20,640 --> 00:29:23,340
Q now I'm going to<font color="#E5E5E5"> use the</font><font color="#CCCCCC"> liveness</font>

811
00:29:23,340 --> 00:29:25,500
integral rule which means that I'm<font color="#E5E5E5"> going</font>

812
00:29:25,500 --> 00:29:27,570
<font color="#CCCCCC">to swap the gradient</font><font color="#E5E5E5"> in to the end the</font>

813
00:29:27,570 --> 00:29:29,789
integral and<font color="#CCCCCC"> you can typically do that</font>

814
00:29:29,789 --> 00:29:31,320
for these probabilities<font color="#CCCCCC"> because they're</font>

815
00:29:31,320 --> 00:29:32,700
all operate in the same domain

816
00:29:32,700 --> 00:29:35,970
they're<font color="#E5E5E5"> all continuous and so the kind</font>

817
00:29:35,970 --> 00:29:38,909
of conditions<font color="#CCCCCC"> that you need are true and</font>

818
00:29:38,909 --> 00:29:40,380
I can<font color="#E5E5E5"> point you to a much more deeper</font>

819
00:29:40,380 --> 00:29:42,600
paper if you<font color="#E5E5E5"> actually want to see all</font>

820
00:29:42,600 --> 00:29:45,059
the the depths of actually why it is

821
00:29:45,059 --> 00:29:47,610
<font color="#CCCCCC">that you can do that</font><font color="#E5E5E5"> but and as I said</font>

822
00:29:47,610 --> 00:29:49,409
<font color="#E5E5E5">we</font><font color="#CCCCCC"> are going</font><font color="#E5E5E5"> to try and</font><font color="#CCCCCC"> manipulate</font><font color="#E5E5E5"> this</font>

823
00:29:49,409 --> 00:29:53,309
quantity Q so what I'm going to<font color="#CCCCCC"> now if</font>

824
00:29:53,309 --> 00:29:54,899
the gradient goes inside so<font color="#CCCCCC"> that's</font><font color="#E5E5E5"> why</font>

825
00:29:54,899 --> 00:29:57,149
<font color="#CCCCCC">this gradient of Q times f I'm going to</font>

826
00:29:57,149 --> 00:29:58,890
use the<font color="#CCCCCC"> identity trick again so I'm just</font>

827
00:29:58,890 --> 00:30:01,440
going<font color="#E5E5E5"> to multiply by</font><font color="#CCCCCC"> one now I'm going</font>

828
00:30:01,440 --> 00:30:04,890
to reform<font color="#E5E5E5"> this</font><font color="#CCCCCC"> ratio</font><font color="#E5E5E5"> of Delta</font><font color="#CCCCCC"> Q divided</font>

829
00:30:04,890 --> 00:30:07,440
<font color="#CCCCCC">by Q and then that basically</font><font color="#E5E5E5"> using the</font>

830
00:30:07,440 --> 00:30:09,659
log derivative trick from above means

831
00:30:09,659 --> 00:30:11,880
that this quantity<font color="#CCCCCC"> is just the log the</font>

832
00:30:11,880 --> 00:30:13,820
gradient of<font color="#E5E5E5"> the log probability</font><font color="#CCCCCC"> and</font>

833
00:30:13,820 --> 00:30:16,470
exactly the reason<font color="#CCCCCC"> why</font><font color="#E5E5E5"> we use identity</font>

834
00:30:16,470 --> 00:30:18,360
tricks is that this now is<font color="#CCCCCC"> the</font><font color="#E5E5E5"> new</font>

835
00:30:18,360 --> 00:30:20,429
expectation with respect<font color="#E5E5E5"> to Q this is an</font>

836
00:30:20,429 --> 00:30:22,940
expectation under<font color="#E5E5E5"> the distribution Q of</font>

837
00:30:22,940 --> 00:30:25,590
this function f times the<font color="#E5E5E5"> gradient of</font>

838
00:30:25,590 --> 00:30:27,659
the log probability<font color="#CCCCCC"> right and I'm</font><font color="#E5E5E5"> going</font>

839
00:30:27,659 --> 00:30:30,899
<font color="#CCCCCC">to do one more</font><font color="#E5E5E5"> step which</font><font color="#CCCCCC"> is I just said</font>

840
00:30:30,899 --> 00:30:32,970
to you the good and useful property was

841
00:30:32,970 --> 00:30:34,950
<font color="#CCCCCC">that the expectations of this</font><font color="#E5E5E5"> long</font>

842
00:30:34,950 --> 00:30:37,320
probability are<font color="#CCCCCC"> zero and so I can</font>

843
00:30:37,320 --> 00:30:39,679
<font color="#E5E5E5">actually subtract any other quantity</font>

844
00:30:39,679 --> 00:30:43,320
<font color="#E5E5E5">some constant any constant C and because</font>

845
00:30:43,320 --> 00:30:45,539
<font color="#CCCCCC">that constant C is independent of the</font>

846
00:30:45,539 --> 00:30:47,100
parameters<font color="#CCCCCC"> and</font><font color="#E5E5E5"> the expectation of this</font>

847
00:30:47,100 --> 00:30:48,750
<font color="#E5E5E5">thing is zero it</font><font color="#CCCCCC"> doesn't affect anything</font>

848
00:30:48,750 --> 00:30:50,640
that I'm doing<font color="#E5E5E5"> but the thing</font><font color="#CCCCCC"> that it</font>

849
00:30:50,640 --> 00:30:52,649
will affect<font color="#E5E5E5"> is</font><font color="#CCCCCC"> the gradient the variance</font>

850
00:30:52,649 --> 00:30:54,270
of<font color="#E5E5E5"> this quantity and knowing the</font>

851
00:30:54,270 --> 00:30:55,799
variance in controlling the variance is

852
00:30:55,799 --> 00:30:57,630
a whole area<font color="#CCCCCC"> of</font><font color="#E5E5E5"> statistics which is</font>

853
00:30:57,630 --> 00:30:59,399
<font color="#E5E5E5">called control variant estimation and</font>

854
00:30:59,399 --> 00:31:01,860
knowing how to choose<font color="#CCCCCC"> see what the</font>

855
00:31:01,860 --> 00:31:03,929
optimal<font color="#E5E5E5"> C is for your particular problem</font>

856
00:31:03,929 --> 00:31:05,909
is something<font color="#CCCCCC"> you can design and</font>

857
00:31:05,909 --> 00:31:08,669
<font color="#E5E5E5">let me reax</font><font color="#CCCCCC"> plain this equation</font><font color="#E5E5E5"> to you</font>

858
00:31:08,669 --> 00:31:10,559
the way you learnt it in reinforcement

859
00:31:10,559 --> 00:31:12,600
learning<font color="#E5E5E5"> so in reinforcement learning</font><font color="#CCCCCC"> q</font>

860
00:31:12,600 --> 00:31:14,759
is what you called the policy and you

861
00:31:14,759 --> 00:31:16,950
wanted<font color="#E5E5E5"> to compute the policy gradient</font>

862
00:31:16,950 --> 00:31:18,330
which<font color="#CCCCCC"> is the gradient</font><font color="#E5E5E5"> with respect to</font>

863
00:31:18,330 --> 00:31:19,950
<font color="#E5E5E5">the policy parameters which are</font><font color="#CCCCCC"> Phi</font><font color="#E5E5E5"> in</font>

864
00:31:19,950 --> 00:31:22,049
<font color="#CCCCCC">this case you had a reward function</font>

865
00:31:22,049 --> 00:31:24,509
<font color="#E5E5E5">which was log Q and then you call this</font>

866
00:31:24,509 --> 00:31:26,820
<font color="#CCCCCC">reinforce well the word function is f</font>

867
00:31:26,820 --> 00:31:28,889
sorry and then you<font color="#E5E5E5"> have the gradient of</font>

868
00:31:28,889 --> 00:31:31,350
this<font color="#E5E5E5"> log key which is the policy and so</font>

869
00:31:31,350 --> 00:31:32,970
then you said things that<font color="#E5E5E5"> have high</font>

870
00:31:32,970 --> 00:31:35,639
<font color="#E5E5E5">reward</font><font color="#CCCCCC"> f you will reinforce that</font>

871
00:31:35,639 --> 00:31:37,109
gradient and then<font color="#E5E5E5"> you will send it up</font>

872
00:31:37,109 --> 00:31:38,759
and things that have low reward

873
00:31:38,759 --> 00:31:41,070
<font color="#CCCCCC">F</font><font color="#E5E5E5"> well then you will not reinforce</font><font color="#CCCCCC"> and</font>

874
00:31:41,070 --> 00:31:42,359
then you will<font color="#E5E5E5"> multiply that policy</font>

875
00:31:42,359 --> 00:31:44,039
gradient and then you<font color="#CCCCCC"> also introduced</font>

876
00:31:44,039 --> 00:31:46,859
this thing called C what<font color="#CCCCCC"> you</font><font color="#E5E5E5"> called the</font>

877
00:31:46,859 --> 00:31:48,690
<font color="#CCCCCC">in had various names I</font><font color="#E5E5E5"> don't you just</font>

878
00:31:48,690 --> 00:31:49,979
call<font color="#E5E5E5"> it a</font><font color="#CCCCCC"> base line I think in</font>

879
00:31:49,979 --> 00:31:52,799
reinforcement<font color="#CCCCCC"> learning</font><font color="#E5E5E5"> but in statistics</font>

880
00:31:52,799 --> 00:31:54,269
we<font color="#E5E5E5"> will call this the control</font><font color="#CCCCCC"> bears</font><font color="#E5E5E5"> and</font>

881
00:31:54,269 --> 00:31:56,190
then you can design<font color="#E5E5E5"> this and knowing</font>

882
00:31:56,190 --> 00:31:57,509
that in reinforcement learning<font color="#E5E5E5"> you have</font>

883
00:31:57,509 --> 00:32:00,779
a particular<font color="#CCCCCC"> n MVP you</font><font color="#E5E5E5"> can design a C</font>

884
00:32:00,779 --> 00:32:02,629
<font color="#CCCCCC">for your</font><font color="#E5E5E5"> MVP and if you were doing</font>

885
00:32:02,629 --> 00:32:05,159
<font color="#E5E5E5">finance problem where you were computing</font>

886
00:32:05,159 --> 00:32:08,220
<font color="#E5E5E5">a time series of returns of a stock then</font>

887
00:32:08,220 --> 00:32:10,320
you'll be able<font color="#E5E5E5"> to compute a C using the</font>

888
00:32:10,320 --> 00:32:11,849
kind of<font color="#E5E5E5"> auto regressive</font><font color="#CCCCCC"> model you</font><font color="#E5E5E5"> were</font>

889
00:32:11,849 --> 00:32:14,789
building<font color="#CCCCCC"> for</font><font color="#E5E5E5"> that sound so there's lots</font>

890
00:32:14,789 --> 00:32:17,460
of different<font color="#CCCCCC"> names also</font><font color="#E5E5E5"> some people</font>

891
00:32:17,460 --> 00:32:19,259
<font color="#CCCCCC">you'd like to call</font><font color="#E5E5E5"> this the likelihood</font>

892
00:32:19,259 --> 00:32:21,059
ratio method<font color="#E5E5E5"> I think</font><font color="#CCCCCC"> that's a bad bad</font>

893
00:32:21,059 --> 00:32:23,759
name so<font color="#CCCCCC"> don't</font><font color="#E5E5E5"> ever call it that</font><font color="#CCCCCC"> I'm just</font>

894
00:32:23,759 --> 00:32:25,320
telling you<font color="#CCCCCC"> so when you Google you'll</font>

895
00:32:25,320 --> 00:32:27,080
sometimes see this and in<font color="#E5E5E5"> some</font>

896
00:32:27,080 --> 00:32:28,859
reinforcement learning<font color="#E5E5E5"> textbooks they</font>

897
00:32:28,859 --> 00:32:30,929
use this terrible naming then of course

898
00:32:30,929 --> 00:32:32,789
you've<font color="#CCCCCC"> seen a reinforcement reinforced</font>

899
00:32:32,789 --> 00:32:35,070
algorithm and the policy gradients are

900
00:32:35,070 --> 00:32:37,229
built in this exact<font color="#E5E5E5"> concept and then in</font>

901
00:32:37,229 --> 00:32:38,820
other<font color="#E5E5E5"> areas like more in probabilistic</font>

902
00:32:38,820 --> 00:32:40,349
<font color="#E5E5E5">inference reinforcement learning they</font>

903
00:32:40,349 --> 00:32:43,019
would call<font color="#E5E5E5"> the automated inference or</font>

904
00:32:43,019 --> 00:32:44,789
sometimes<font color="#E5E5E5"> black box inference right</font>

905
00:32:44,789 --> 00:32:47,070
because this<font color="#E5E5E5"> is</font><font color="#CCCCCC"> a kind</font><font color="#E5E5E5"> of black box</font><font color="#CCCCCC"> in</font>

906
00:32:47,070 --> 00:32:48,599
to go and when will<font color="#E5E5E5"> you want to use this</font>

907
00:32:48,599 --> 00:32:51,239
kind of integral you will use it when

908
00:32:51,239 --> 00:32:53,129
this function<font color="#CCCCCC"> f is not differentiable</font>

909
00:32:53,129 --> 00:32:54,570
because<font color="#CCCCCC"> you don't</font><font color="#E5E5E5"> need to</font><font color="#CCCCCC"> differentiate</font>

910
00:32:54,570 --> 00:32:56,340
it all you need to be able<font color="#E5E5E5"> to do is</font>

911
00:32:56,340 --> 00:32:58,919
evaluate the function f what you need to

912
00:32:58,919 --> 00:33:00,989
do though<font color="#E5E5E5"> is evaluate this</font><font color="#CCCCCC"> expectation</font>

913
00:33:00,989 --> 00:33:02,999
so you<font color="#E5E5E5"> will assume</font><font color="#CCCCCC"> that Q is something</font>

914
00:33:02,999 --> 00:33:05,070
which<font color="#E5E5E5"> is simple that</font><font color="#CCCCCC"> you can sample from</font>

915
00:33:05,070 --> 00:33:07,710
<font color="#E5E5E5">and because you want to differentiate Q</font>

916
00:33:07,710 --> 00:33:09,389
you<font color="#E5E5E5"> will need to be able to take its</font>

917
00:33:09,389 --> 00:33:10,859
derivative you<font color="#CCCCCC"> will need to know it</font>

918
00:33:10,859 --> 00:33:13,379
analytically<font color="#E5E5E5"> so almost all problems can</font>

919
00:33:13,379 --> 00:33:15,149
<font color="#CCCCCC">have this thing</font><font color="#E5E5E5"> so this</font><font color="#CCCCCC"> is usually the</font>

920
00:33:15,149 --> 00:33:17,730
<font color="#E5E5E5">first default way of approaching a</font>

921
00:33:17,730 --> 00:33:19,779
gradient<font color="#E5E5E5"> estimator</font>

922
00:33:19,779 --> 00:33:22,570
so let's do a<font color="#E5E5E5"> different trick and this</font>

923
00:33:22,570 --> 00:33:24,429
one has<font color="#E5E5E5"> many different names but most</font>

924
00:33:24,429 --> 00:33:26,889
common today people<font color="#E5E5E5"> call this</font><font color="#CCCCCC"> the Reaper</font>

925
00:33:26,889 --> 00:33:31,330
<font color="#E5E5E5">amortization trick</font><font color="#CCCCCC"> so every distribution</font>

926
00:33:31,330 --> 00:33:33,969
can<font color="#E5E5E5"> be re-expressed</font><font color="#CCCCCC"> as a</font><font color="#E5E5E5"> function of</font>

927
00:33:33,969 --> 00:33:37,749
some other distribution<font color="#E5E5E5"> or so let's do</font>

928
00:33:37,749 --> 00:33:39,669
this so I have a<font color="#CCCCCC"> distribution</font><font color="#E5E5E5"> Q of some</font>

929
00:33:39,669 --> 00:33:41,830
function<font color="#E5E5E5"> of some random</font><font color="#CCCCCC"> variable</font><font color="#E5E5E5"> said</font>

930
00:33:41,830 --> 00:33:44,200
this<font color="#CCCCCC"> distribution</font><font color="#E5E5E5"> Q can always be</font>

931
00:33:44,200 --> 00:33:47,049
re-expressed<font color="#E5E5E5"> in terms of a deterministic</font>

932
00:33:47,049 --> 00:33:49,570
function<font color="#E5E5E5"> with some parameters</font><font color="#CCCCCC"> v which</font>

933
00:33:49,570 --> 00:33:51,039
will transform another distribution

934
00:33:51,039 --> 00:33:52,809
<font color="#E5E5E5">which will have some distribution</font>

935
00:33:52,809 --> 00:33:56,259
<font color="#CCCCCC">epsilon</font><font color="#E5E5E5"> intercept so the simplest one to</font>

936
00:33:56,259 --> 00:33:58,749
do is to<font color="#E5E5E5"> think of the inverse sampling</font>

937
00:33:58,749 --> 00:34:00,789
theorem you have a uniform<font color="#CCCCCC"> distribution</font>

938
00:34:00,789 --> 00:34:03,039
<font color="#CCCCCC">all uniform distributions can</font><font color="#E5E5E5"> be</font>

939
00:34:03,039 --> 00:34:05,440
<font color="#E5E5E5">transformed into another</font><font color="#CCCCCC"> distribution</font>

940
00:34:05,440 --> 00:34:06,879
using<font color="#E5E5E5"> the inverse</font><font color="#CCCCCC"> CDF of</font><font color="#E5E5E5"> the</font>

941
00:34:06,879 --> 00:34:08,770
<font color="#CCCCCC">distribution you are interested in</font><font color="#E5E5E5"> right</font>

942
00:34:08,770 --> 00:34:10,119
and<font color="#E5E5E5"> this was the first thing you learned</font>

943
00:34:10,119 --> 00:34:12,969
about probabilistic sampling<font color="#E5E5E5"> and this is</font>

944
00:34:12,969 --> 00:34:14,710
if you always<font color="#E5E5E5"> think in this uniform</font>

945
00:34:14,710 --> 00:34:16,480
<font color="#E5E5E5">setup you'll be able to derive the most</font>

946
00:34:16,480 --> 00:34:19,599
generic<font color="#CCCCCC"> form</font><font color="#E5E5E5"> of what this trick is but</font>

947
00:34:19,599 --> 00:34:22,029
typically we use<font color="#E5E5E5"> other just slightly</font>

948
00:34:22,029 --> 00:34:23,949
<font color="#E5E5E5">more we don't use start to primitive</font>

949
00:34:23,949 --> 00:34:25,869
distribution as the<font color="#E5E5E5"> uniform we use other</font>

950
00:34:25,869 --> 00:34:27,909
distributions like gaussians<font color="#CCCCCC"> betas</font>

951
00:34:27,909 --> 00:34:30,520
<font color="#CCCCCC">bernoulliís etc and how</font><font color="#E5E5E5"> I like to</font><font color="#CCCCCC"> think</font>

952
00:34:30,520 --> 00:34:33,369
<font color="#CCCCCC">of that is more</font><font color="#E5E5E5"> like a set of</font><font color="#CCCCCC"> pipe</font><font color="#E5E5E5"> so</font>

953
00:34:33,369 --> 00:34:35,679
you have some distribution<font color="#CCCCCC"> which we</font>

954
00:34:35,679 --> 00:34:37,210
start off with which is this base

955
00:34:37,210 --> 00:34:39,190
distribution<font color="#E5E5E5"> P of Z</font><font color="#CCCCCC"> and what I was</font>

956
00:34:39,190 --> 00:34:41,020
calling<font color="#CCCCCC"> P of epsilon here sorry for the</font>

957
00:34:41,020 --> 00:34:43,869
mix-up<font color="#CCCCCC"> and</font><font color="#E5E5E5"> then you have to sort of</font>

958
00:34:43,869 --> 00:34:45,760
things which mix into<font color="#E5E5E5"> this pipe you</font><font color="#CCCCCC"> have</font>

959
00:34:45,760 --> 00:34:48,369
some<font color="#E5E5E5"> parameters mu and some</font><font color="#CCCCCC"> parameters</font><font color="#E5E5E5"> R</font>

960
00:34:48,369 --> 00:34:50,260
and then they<font color="#E5E5E5"> get mixed up through this</font>

961
00:34:50,260 --> 00:34:52,449
<font color="#E5E5E5">function R so the case of a Gaussian is</font>

962
00:34:52,449 --> 00:34:54,849
just mu plus R times<font color="#E5E5E5"> ed and then once</font>

963
00:34:54,849 --> 00:34:56,230
you<font color="#E5E5E5"> get out is a new distribution</font>

964
00:34:56,230 --> 00:34:59,200
<font color="#E5E5E5">instead and knowing this pipe in knowing</font>

965
00:34:59,200 --> 00:35:00,910
the configuration<font color="#CCCCCC"> of the pipe is useful</font>

966
00:35:00,910 --> 00:35:02,950
<font color="#E5E5E5">for the gradient because you can follow</font>

967
00:35:02,950 --> 00:35:05,380
the path of this pipe backwards and it

968
00:35:05,380 --> 00:35:06,910
is this reason<font color="#CCCCCC"> because of this</font>

969
00:35:06,910 --> 00:35:08,829
transformation<font color="#CCCCCC"> of</font><font color="#E5E5E5"> this variable through</font>

970
00:35:08,829 --> 00:35:10,900
the path function G to get<font color="#E5E5E5"> this new</font>

971
00:35:10,900 --> 00:35:14,170
random<font color="#E5E5E5"> variable</font><font color="#CCCCCC"> said that this the trick</font>

972
00:35:14,170 --> 00:35:16,109
<font color="#CCCCCC">that we're about to</font><font color="#E5E5E5"> talk</font><font color="#CCCCCC"> about will be</font>

973
00:35:16,109 --> 00:35:19,770
called path wise estimation or path wise

974
00:35:19,770 --> 00:35:23,170
computation<font color="#E5E5E5"> and so the main point of</font>

975
00:35:23,170 --> 00:35:25,029
<font color="#CCCCCC">Reaper</font><font color="#E5E5E5"> ammeter ization trick like every</font>

976
00:35:25,029 --> 00:35:26,740
<font color="#E5E5E5">route parameterization is just the</font>

977
00:35:26,740 --> 00:35:28,000
change of variables<font color="#CCCCCC"> rules for</font>

978
00:35:28,000 --> 00:35:29,890
probability<font color="#E5E5E5"> and if we had more time I</font>

979
00:35:29,890 --> 00:35:31,750
would<font color="#E5E5E5"> have done a whole trick just for</font>

980
00:35:31,750 --> 00:35:33,140
change of<font color="#E5E5E5"> variables rules and</font><font color="#CCCCCC"> sure</font>

981
00:35:33,140 --> 00:35:35,089
you instead<font color="#E5E5E5"> of how they appear in</font>

982
00:35:35,089 --> 00:35:37,190
continuous and discrete spaces but we're

983
00:35:37,190 --> 00:35:39,109
<font color="#E5E5E5">gonna skip that today but just know</font><font color="#CCCCCC"> that</font>

984
00:35:39,109 --> 00:35:40,400
this<font color="#E5E5E5"> is the rule</font><font color="#CCCCCC"> for</font><font color="#E5E5E5"> the change of</font>

985
00:35:40,400 --> 00:35:41,869
variables<font color="#CCCCCC"> if you have a</font><font color="#E5E5E5"> distribution</font>

986
00:35:41,869 --> 00:35:44,420
<font color="#E5E5E5">epsilon and you have some function</font><font color="#CCCCCC"> that</font>

987
00:35:44,420 --> 00:35:47,059
changes epsilon to Z like here<font color="#E5E5E5"> you can</font>

988
00:35:47,059 --> 00:35:49,250
know the<font color="#CCCCCC"> distribution P of Z by</font>

989
00:35:49,250 --> 00:35:51,019
computing this<font color="#E5E5E5"> gradient and multiplying</font>

990
00:35:51,019 --> 00:35:53,599
<font color="#E5E5E5">by the original</font><font color="#CCCCCC"> thing</font><font color="#E5E5E5"> this is you</font>

991
00:35:53,599 --> 00:35:55,010
<font color="#CCCCCC">learned this</font><font color="#E5E5E5"> in calculus under the</font>

992
00:35:55,010 --> 00:35:56,390
change of<font color="#E5E5E5"> variables and the rule</font><font color="#CCCCCC"> is</font>

993
00:35:56,390 --> 00:35:58,250
identical<font color="#E5E5E5"> when we come to</font><font color="#CCCCCC"> it in</font>

994
00:35:58,250 --> 00:36:00,049
probability and<font color="#E5E5E5"> we're going to use this</font>

995
00:36:00,049 --> 00:36:02,269
trick<font color="#E5E5E5"> and just keep in mind that because</font>

996
00:36:02,269 --> 00:36:04,460
<font color="#E5E5E5">we</font><font color="#CCCCCC"> are probabilities and</font><font color="#E5E5E5"> we must exist</font>

997
00:36:04,460 --> 00:36:06,740
in<font color="#E5E5E5"> a probability space where things must</font>

998
00:36:06,740 --> 00:36:08,750
integrate to one and volume is never

999
00:36:08,750 --> 00:36:10,970
lost as a conservation property<font color="#E5E5E5"> you can</font>

1000
00:36:10,970 --> 00:36:13,700
<font color="#CCCCCC">always use this sort of loose notation</font>

1001
00:36:13,700 --> 00:36:16,099
<font color="#CCCCCC">just to say the</font><font color="#E5E5E5"> volume of probabilities</font>

1002
00:36:16,099 --> 00:36:18,170
must be conserved that<font color="#CCCCCC"> P Z times D Z</font>

1003
00:36:18,170 --> 00:36:20,750
must<font color="#E5E5E5"> always PE times de and you must</font>

1004
00:36:20,750 --> 00:36:22,760
these things can be equal<font color="#CCCCCC"> that's why you</font>

1005
00:36:22,760 --> 00:36:24,589
can do manipulations by substituting

1006
00:36:24,589 --> 00:36:26,599
this for<font color="#E5E5E5"> this so let's</font><font color="#CCCCCC"> just do the trick</font>

1007
00:36:26,599 --> 00:36:30,230
<font color="#CCCCCC">a bit</font><font color="#E5E5E5"> of a non rigorous weird kind of</font>

1008
00:36:30,230 --> 00:36:33,559
derivation<font color="#E5E5E5"> but I think it will work</font><font color="#CCCCCC"> so</font>

1009
00:36:33,559 --> 00:36:35,630
again<font color="#CCCCCC"> this is our integral problem it is</font>

1010
00:36:35,630 --> 00:36:38,119
<font color="#E5E5E5">the integral of a distribution of a</font>

1011
00:36:38,119 --> 00:36:40,099
function f against the<font color="#E5E5E5"> distribution Q</font>

1012
00:36:40,099 --> 00:36:41,869
and we want to know the gradients with

1013
00:36:41,869 --> 00:36:43,910
respect to Phi<font color="#E5E5E5"> and you have a known</font>

1014
00:36:43,910 --> 00:36:46,759
transformation which<font color="#CCCCCC"> is</font><font color="#E5E5E5"> G which</font>

1015
00:36:46,759 --> 00:36:48,980
<font color="#CCCCCC">transform samples</font><font color="#E5E5E5"> epsilon with</font>

1016
00:36:48,980 --> 00:36:52,460
parameters Phi into Z<font color="#E5E5E5"> so we're going to</font>

1017
00:36:52,460 --> 00:36:54,950
<font color="#E5E5E5">do this change of</font><font color="#CCCCCC"> variables now</font><font color="#E5E5E5"> let's</font>

1018
00:36:54,950 --> 00:36:57,470
write this a bit slowly just<font color="#E5E5E5"> say you see</font>

1019
00:36:57,470 --> 00:37:00,140
wherever you see that you can replace

1020
00:37:00,140 --> 00:37:03,559
<font color="#E5E5E5">that by the function G of epsilon comma</font>

1021
00:37:03,559 --> 00:37:04,910
<font color="#E5E5E5">Phi because that is what the</font>

1022
00:37:04,910 --> 00:37:07,759
substitution<font color="#CCCCCC"> rule is and then I said</font>

1023
00:37:07,759 --> 00:37:10,670
<font color="#CCCCCC">well Q of Z</font><font color="#E5E5E5"> can</font><font color="#CCCCCC"> be rewritten in terms</font><font color="#E5E5E5"> of</font>

1024
00:37:10,670 --> 00:37:12,680
<font color="#CCCCCC">epsilon</font><font color="#E5E5E5"> using this change of</font><font color="#CCCCCC"> variables</font>

1025
00:37:12,680 --> 00:37:14,839
rule<font color="#E5E5E5"> so it's P of epsilon which is</font><font color="#CCCCCC"> the</font>

1026
00:37:14,839 --> 00:37:16,880
base distribution times the derivative<font color="#E5E5E5"> D</font>

1027
00:37:16,880 --> 00:37:21,049
epsilon over<font color="#CCCCCC"> D Z and there's this</font>

1028
00:37:21,049 --> 00:37:22,930
gradient which<font color="#CCCCCC"> is going</font><font color="#E5E5E5"> to appear</font>

1029
00:37:22,930 --> 00:37:25,099
<font color="#CCCCCC">because of the change of variables rule</font>

1030
00:37:25,099 --> 00:37:27,650
<font color="#E5E5E5">so I'm also going to change DZ to D</font>

1031
00:37:27,650 --> 00:37:29,660
<font color="#CCCCCC">Epsilon</font><font color="#E5E5E5"> and the way you do DZ to D</font>

1032
00:37:29,660 --> 00:37:31,970
<font color="#E5E5E5">epsilon is to know that these epsilon DS</font>

1033
00:37:31,970 --> 00:37:33,890
<font color="#CCCCCC">there is equal</font><font color="#E5E5E5"> to G prime and so again</font>

1034
00:37:33,890 --> 00:37:36,049
this is where this volume is<font color="#CCCCCC"> being</font>

1035
00:37:36,049 --> 00:37:38,170
preserved<font color="#E5E5E5"> right and then I'm going</font><font color="#CCCCCC"> to</font>

1036
00:37:38,170 --> 00:37:40,730
apply<font color="#E5E5E5"> the inverse function theorem on</font>

1037
00:37:40,730 --> 00:37:43,490
this derivative<font color="#CCCCCC"> D</font><font color="#E5E5E5"> epsilon by DZ and that</font>

1038
00:37:43,490 --> 00:37:45,619
effectively is going to cancel out this

1039
00:37:45,619 --> 00:37:47,089
derivative<font color="#E5E5E5"> here G Prime</font>

1040
00:37:47,089 --> 00:37:49,249
and so G<font color="#CCCCCC"> Prime will disappear</font><font color="#E5E5E5"> and then</font>

1041
00:37:49,249 --> 00:37:50,809
basically<font color="#E5E5E5"> what you'll be left is with an</font>

1042
00:37:50,809 --> 00:37:52,969
expectation<font color="#E5E5E5"> on</font><font color="#CCCCCC"> the P of epsilon instead</font>

1043
00:37:52,969 --> 00:37:57,440
then of the function of G directly and

1044
00:37:57,440 --> 00:37:59,839
now actually the integral<font color="#E5E5E5"> is over</font>

1045
00:37:59,839 --> 00:38:02,059
<font color="#CCCCCC">epsilon which has nothing to do with Phi</font>

1046
00:38:02,059 --> 00:38:03,619
so now you are<font color="#E5E5E5"> free to take the gradient</font>

1047
00:38:03,619 --> 00:38:05,450
<font color="#E5E5E5">directly through and you don't have to</font>

1048
00:38:05,450 --> 00:38:06,739
worry<font color="#E5E5E5"> about any there are no conditions</font>

1049
00:38:06,739 --> 00:38:08,809
<font color="#E5E5E5">to check the gradient can just go</font>

1050
00:38:08,809 --> 00:38:10,640
directly<font color="#CCCCCC"> through</font><font color="#E5E5E5"> the integral and so</font>

1051
00:38:10,640 --> 00:38:12,880
this then<font color="#CCCCCC"> is</font><font color="#E5E5E5"> basically what is called</font>

1052
00:38:12,880 --> 00:38:15,799
<font color="#CCCCCC">this path wise gradient estimator it</font>

1053
00:38:15,799 --> 00:38:19,099
says that if you<font color="#CCCCCC"> no</font><font color="#E5E5E5"> change of variables</font>

1054
00:38:19,099 --> 00:38:21,440
for Q you can then rewrite<font color="#E5E5E5"> it through</font>

1055
00:38:21,440 --> 00:38:23,210
this path wise gradient<font color="#E5E5E5"> estimate of</font>

1056
00:38:23,210 --> 00:38:26,089
<font color="#CCCCCC">Epsilon and</font><font color="#E5E5E5"> a simpler distribution P of</font>

1057
00:38:26,089 --> 00:38:27,619
epsilon<font color="#CCCCCC"> of the gradient of the</font><font color="#E5E5E5"> function</font>

1058
00:38:27,619 --> 00:38:30,619
of<font color="#E5E5E5"> the change of variables</font><font color="#CCCCCC"> and this is</font>

1059
00:38:30,619 --> 00:38:32,660
<font color="#E5E5E5">nice to do</font><font color="#CCCCCC"> because now it's</font><font color="#E5E5E5"> just back</font>

1060
00:38:32,660 --> 00:38:35,180
<font color="#E5E5E5">prop you just can take the</font><font color="#CCCCCC"> gradient of</font>

1061
00:38:35,180 --> 00:38:37,999
<font color="#CCCCCC">fee</font><font color="#E5E5E5"> of</font><font color="#CCCCCC"> f through G to get to Phi</font><font color="#E5E5E5"> right</font>

1062
00:38:37,999 --> 00:38:41,029
and this<font color="#E5E5E5"> is why in other papers we call</font>

1063
00:38:41,029 --> 00:38:43,549
this stochastic back propagation<font color="#E5E5E5"> it's</font>

1064
00:38:43,549 --> 00:38:45,049
called the real parameterization trick

1065
00:38:45,049 --> 00:38:49,369
and for every<font color="#E5E5E5"> other area</font><font color="#CCCCCC"> of probability</font>

1066
00:38:49,369 --> 00:38:50,599
outside of machine learning they<font color="#CCCCCC"> will</font>

1067
00:38:50,599 --> 00:38:52,789
<font color="#CCCCCC">call this the pathways estimate pathways</font>

1068
00:38:52,789 --> 00:38:55,130
great<font color="#CCCCCC"> and estimator so</font><font color="#E5E5E5"> where you would</font>

1069
00:38:55,130 --> 00:38:57,289
have seen<font color="#E5E5E5"> this before</font><font color="#CCCCCC"> is when you</font><font color="#E5E5E5"> learnt</font>

1070
00:38:57,289 --> 00:38:59,809
about the basics of expectation they

1071
00:38:59,809 --> 00:39:01,130
would have<font color="#E5E5E5"> maybe mentioned this things</font>

1072
00:39:01,130 --> 00:39:02,719
<font color="#E5E5E5">you call the law</font><font color="#CCCCCC"> of the unconscious</font>

1073
00:39:02,719 --> 00:39:04,759
statistician which is if you are given<font color="#CCCCCC"> a</font>

1074
00:39:04,759 --> 00:39:06,769
transformation<font color="#E5E5E5"> how can you compute this</font>

1075
00:39:06,769 --> 00:39:08,960
expectation<font color="#CCCCCC"> under that transformation</font>

1076
00:39:08,960 --> 00:39:11,150
and<font color="#CCCCCC"> so you will see this is the</font><font color="#E5E5E5"> point</font><font color="#CCCCCC"> of</font>

1077
00:39:11,150 --> 00:39:12,799
doing this<font color="#E5E5E5"> change of</font><font color="#CCCCCC"> variables as I</font>

1078
00:39:12,799 --> 00:39:14,269
mentioned we call the stochastic back

1079
00:39:14,269 --> 00:39:16,910
propagation<font color="#E5E5E5"> this idea of computing</font>

1080
00:39:16,910 --> 00:39:18,859
gradients is where they<font color="#E5E5E5"> could call this</font>

1081
00:39:18,859 --> 00:39:20,869
<font color="#CCCCCC">perturbation analysis and under</font>

1082
00:39:20,869 --> 00:39:22,759
perturbation analysis you can do derive

1083
00:39:22,759 --> 00:39:24,279
<font color="#CCCCCC">certain other variations of this</font>

1084
00:39:24,279 --> 00:39:25,519
<font color="#E5E5E5">gradient</font>

1085
00:39:25,519 --> 00:39:27,739
there's the<font color="#CCCCCC"> reaper ammeter ization trick</font>

1086
00:39:27,739 --> 00:39:29,569
and other people have called this a fine

1087
00:39:29,569 --> 00:39:31,099
independent inference<font color="#E5E5E5"> and in fact this</font>

1088
00:39:31,099 --> 00:39:34,269
trick is also<font color="#E5E5E5"> used in Monte Carlo</font>

1089
00:39:34,269 --> 00:39:37,819
sampling<font color="#CCCCCC"> but I forget the name the name</font>

1090
00:39:37,819 --> 00:39:40,219
of it I have a reference<font color="#E5E5E5"> at</font><font color="#CCCCCC"> the end so</font>

1091
00:39:40,219 --> 00:39:41,719
when when should you use<font color="#E5E5E5"> this kind of</font>

1092
00:39:41,719 --> 00:39:43,940
gradient estimator<font color="#CCCCCC"> so now you need</font><font color="#E5E5E5"> to do</font>

1093
00:39:43,940 --> 00:39:45,229
<font color="#E5E5E5">it much more</font><font color="#CCCCCC"> you actually need to</font>

1094
00:39:45,229 --> 00:39:48,229
differentiate through<font color="#E5E5E5"> F so f you need to</font>

1095
00:39:48,229 --> 00:39:50,710
be<font color="#E5E5E5"> know F and F must be differentiable</font>

1096
00:39:50,710 --> 00:39:53,900
<font color="#E5E5E5">again Q is a distribution with the</font>

1097
00:39:53,900 --> 00:39:55,999
transform you need to be able<font color="#CCCCCC"> to rewrite</font>

1098
00:39:55,999 --> 00:39:59,410
Q of Z in<font color="#CCCCCC"> terms of P epsilon and</font><font color="#E5E5E5"> a</font>

1099
00:39:59,410 --> 00:40:00,560
deterministic

1100
00:40:00,560 --> 00:40:03,800
function G<font color="#E5E5E5"> and so these functions</font><font color="#CCCCCC"> can be</font>

1101
00:40:03,800 --> 00:40:06,230
anything<font color="#E5E5E5"> can be the inverse CDF which is</font>

1102
00:40:06,230 --> 00:40:08,000
how you'll derive<font color="#CCCCCC"> the most</font><font color="#E5E5E5"> generic</font><font color="#CCCCCC"> form</font>

1103
00:40:08,000 --> 00:40:10,100
of this root parameterization trick it

1104
00:40:10,100 --> 00:40:12,110
can be a<font color="#E5E5E5"> location scale transform like</font>

1105
00:40:12,110 --> 00:40:14,690
<font color="#CCCCCC">this function here for the Gaussian</font><font color="#E5E5E5"> or</font>

1106
00:40:14,690 --> 00:40:16,280
it can be any other<font color="#E5E5E5"> kind of coordinate</font>

1107
00:40:16,280 --> 00:40:18,440
transform that<font color="#E5E5E5"> you have and then</font><font color="#CCCCCC"> you</font>

1108
00:40:18,440 --> 00:40:19,520
want to<font color="#E5E5E5"> make</font><font color="#CCCCCC"> sure that</font><font color="#E5E5E5"> this base</font>

1109
00:40:19,520 --> 00:40:21,680
distribution P<font color="#CCCCCC"> epsilon is actually</font>

1110
00:40:21,680 --> 00:40:24,880
something<font color="#E5E5E5"> simple and easy to sample from</font>

1111
00:40:24,880 --> 00:40:28,420
<font color="#E5E5E5">okay so that was basically five tricks</font>

1112
00:40:28,420 --> 00:40:31,400
<font color="#CCCCCC">we looked at the identity trick</font><font color="#E5E5E5"> and the</font>

1113
00:40:31,400 --> 00:40:34,160
identity trick was a way<font color="#CCCCCC"> of rewriting</font><font color="#E5E5E5"> an</font>

1114
00:40:34,160 --> 00:40:35,960
expectation from<font color="#E5E5E5"> one</font><font color="#CCCCCC"> distribution to</font>

1115
00:40:35,960 --> 00:40:37,910
another this<font color="#E5E5E5"> is the number one trick</font>

1116
00:40:37,910 --> 00:40:38,870
<font color="#E5E5E5">it's the first thing</font><font color="#CCCCCC"> you will probably</font>

1117
00:40:38,870 --> 00:40:41,000
<font color="#E5E5E5">do for most</font><font color="#CCCCCC"> everything so one of the</font>

1118
00:40:41,000 --> 00:40:42,860
<font color="#E5E5E5">most</font><font color="#CCCCCC"> useful things to know then we looks</font>

1119
00:40:42,860 --> 00:40:44,960
at one bounding trick based on Jensen's

1120
00:40:44,960 --> 00:40:48,290
inequality<font color="#CCCCCC"> but building bounds is one</font><font color="#E5E5E5"> of</font>

1121
00:40:48,290 --> 00:40:49,550
the most useful tricks for doing

1122
00:40:49,550 --> 00:40:51,620
<font color="#E5E5E5">learning and when you especially when</font>

1123
00:40:51,620 --> 00:40:53,600
<font color="#CCCCCC">you want</font><font color="#E5E5E5"> to build very</font><font color="#CCCCCC"> large scale</font>

1124
00:40:53,600 --> 00:40:55,820
learning algorithms that<font color="#E5E5E5"> go</font><font color="#CCCCCC"> to millions</font>

1125
00:40:55,820 --> 00:40:57,860
and<font color="#E5E5E5"> tens of thousands of parameters the</font>

1126
00:40:57,860 --> 00:40:59,600
density ratio trick is a<font color="#E5E5E5"> useful one</font>

1127
00:40:59,600 --> 00:41:01,490
<font color="#E5E5E5">because</font><font color="#CCCCCC"> like you saw an important</font>

1128
00:41:01,490 --> 00:41:03,230
sampling like you see in reinforcement

1129
00:41:03,230 --> 00:41:05,540
learning<font color="#CCCCCC"> like</font><font color="#E5E5E5"> you saw in almost all the</font>

1130
00:41:05,540 --> 00:41:07,250
<font color="#CCCCCC">other</font><font color="#E5E5E5"> tricks there's always a ratio of</font>

1131
00:41:07,250 --> 00:41:09,260
<font color="#E5E5E5">two distributions that appear and this</font>

1132
00:41:09,260 --> 00:41:11,390
ratio<font color="#CCCCCC"> can give you knowledge</font><font color="#E5E5E5"> of how</font>

1133
00:41:11,390 --> 00:41:13,280
<font color="#CCCCCC">things are we gonna look at that</font><font color="#E5E5E5"> the log</font>

1134
00:41:13,280 --> 00:41:15,650
derivative trick basically exploits the

1135
00:41:15,650 --> 00:41:17,750
the<font color="#E5E5E5"> definition of what the gradient of</font>

1136
00:41:17,750 --> 00:41:20,600
the log<font color="#E5E5E5"> is to show the properties</font><font color="#CCCCCC"> of log</font>

1137
00:41:20,600 --> 00:41:22,460
likelihood functions and score functions

1138
00:41:22,460 --> 00:41:23,630
<font color="#CCCCCC">we showed</font><font color="#E5E5E5"> the basis of maximum</font>

1139
00:41:23,630 --> 00:41:25,100
<font color="#CCCCCC">likelihood</font><font color="#E5E5E5"> and then we looked at three</font>

1140
00:41:25,100 --> 00:41:26,690
parameter ization tricks which

1141
00:41:26,690 --> 00:41:29,720
effectively<font color="#E5E5E5"> just deployed the rule for</font>

1142
00:41:29,720 --> 00:41:31,550
change of variables of probability in

1143
00:41:31,550 --> 00:41:33,470
all different<font color="#E5E5E5"> settings to help us derive</font>

1144
00:41:33,470 --> 00:41:35,360
different kinds<font color="#E5E5E5"> of gradient estimators</font>

1145
00:41:35,360 --> 00:41:39,140
so basically the<font color="#E5E5E5"> point of this half was</font>

1146
00:41:39,140 --> 00:41:41,420
just<font color="#E5E5E5"> to leave with you the message to</font>

1147
00:41:41,420 --> 00:41:43,640
sharpen<font color="#E5E5E5"> your probabilistic tools to</font>

1148
00:41:43,640 --> 00:41:45,680
always search for<font color="#E5E5E5"> those tricks that</font>

1149
00:41:45,680 --> 00:41:48,170
<font color="#E5E5E5">means you can manipulate probabilities</font>

1150
00:41:48,170 --> 00:41:50,060
in the right way<font color="#E5E5E5"> and as these tricks</font>

1151
00:41:50,060 --> 00:41:52,040
show that if you manipulate them you can

1152
00:41:52,040 --> 00:41:54,020
<font color="#CCCCCC">actually do</font><font color="#E5E5E5"> very</font><font color="#CCCCCC"> interesting kind</font><font color="#E5E5E5"> of</font>

1153
00:41:54,020 --> 00:41:55,580
things<font color="#E5E5E5"> that transform what were</font>

1154
00:41:55,580 --> 00:41:57,050
difficult problems<font color="#E5E5E5"> that you could only</font>

1155
00:41:57,050 --> 00:42:00,290
solve<font color="#E5E5E5"> in 1d</font><font color="#CCCCCC"> two things</font><font color="#E5E5E5"> that you can work</font>

1156
00:42:00,290 --> 00:42:03,170
on<font color="#CCCCCC"> imagenet scale of data ok so</font><font color="#E5E5E5"> that's</font>

1157
00:42:03,170 --> 00:42:04,400
the<font color="#CCCCCC"> end of</font><font color="#E5E5E5"> the first half we can take a</font>

1158
00:42:04,400 --> 00:42:06,710
<font color="#E5E5E5">10-minute break or have</font><font color="#CCCCCC"> a discussion on</font>

1159
00:42:06,710 --> 00:42:09,020
any questions around what we discussed

1160
00:42:09,020 --> 00:42:10,910
in<font color="#E5E5E5"> the</font><font color="#CCCCCC"> first half</font><font color="#E5E5E5"> are there any thoughts</font>

1161
00:42:10,910 --> 00:42:15,460
<font color="#CCCCCC">questions comments confusions</font>

1162
00:42:17,950 --> 00:42:20,660
<font color="#CCCCCC">No okay so let's</font><font color="#E5E5E5"> take a 10-minute</font><font color="#CCCCCC"> break</font>

1163
00:42:20,660 --> 00:42:22,430
and<font color="#CCCCCC"> then we'll</font><font color="#E5E5E5"> come</font><font color="#CCCCCC"> back</font><font color="#E5E5E5"> into the second</font>

1164
00:42:22,430 --> 00:42:26,390
bar<font color="#E5E5E5"> okay so equipped with these five</font>

1165
00:42:26,390 --> 00:42:29,030
tricks<font color="#CCCCCC"> I want to actually talk about</font>

1166
00:42:29,030 --> 00:42:31,070
<font color="#E5E5E5">generative</font><font color="#CCCCCC"> model it's the topic that</font>

1167
00:42:31,070 --> 00:42:33,170
<font color="#E5E5E5">we're here</font><font color="#CCCCCC"> to talk</font><font color="#E5E5E5"> about and I'm going</font>

1168
00:42:33,170 --> 00:42:35,930
<font color="#CCCCCC">to talk specifically about two types of</font>

1169
00:42:35,930 --> 00:42:38,210
models<font color="#E5E5E5"> prescribed and implicit models</font>

1170
00:42:38,210 --> 00:42:40,520
this<font color="#E5E5E5"> is</font><font color="#CCCCCC"> the way I split</font><font color="#E5E5E5"> thinking about</font>

1171
00:42:40,520 --> 00:42:43,250
generative model<font color="#E5E5E5"> so who wants to tell me</font>

1172
00:42:43,250 --> 00:42:47,990
<font color="#CCCCCC">what</font><font color="#E5E5E5"> density estimation is anyone once</font>

1173
00:42:47,990 --> 00:42:50,240
the volunteer<font color="#CCCCCC"> and</font><font color="#E5E5E5"> explanation what is</font>

1174
00:42:50,240 --> 00:42:55,430
density<font color="#E5E5E5"> estimation no takers today good</font>

1175
00:42:55,430 --> 00:42:57,950
<font color="#CCCCCC">guess shot from the back okay so we're</font>

1176
00:42:57,950 --> 00:42:59,750
going to look a little<font color="#E5E5E5"> bit about density</font>

1177
00:42:59,750 --> 00:43:01,310
estimation<font color="#E5E5E5"> people don't talk about</font>

1178
00:43:01,310 --> 00:43:04,010
<font color="#E5E5E5">density ation density estimation that</font>

1179
00:43:04,010 --> 00:43:06,950
much<font color="#E5E5E5"> so let's talk a bit</font><font color="#CCCCCC"> about models</font>

1180
00:43:06,950 --> 00:43:09,500
<font color="#E5E5E5">see you have typically you</font><font color="#CCCCCC"> have been</font>

1181
00:43:09,500 --> 00:43:12,109
<font color="#E5E5E5">looking at conditional models so when</font>

1182
00:43:12,109 --> 00:43:14,450
you are building a classifier you are

1183
00:43:14,450 --> 00:43:16,520
doing supervised learning<font color="#E5E5E5"> it is a</font>

1184
00:43:16,520 --> 00:43:19,099
conditional model because it is a P of Y

1185
00:43:19,099 --> 00:43:21,890
<font color="#E5E5E5">conditioned on some other</font><font color="#CCCCCC"> observed</font>

1186
00:43:21,890 --> 00:43:24,770
variable X right<font color="#E5E5E5"> and typically these</font>

1187
00:43:24,770 --> 00:43:26,570
<font color="#E5E5E5">kind of models are called</font><font color="#CCCCCC"> regression</font>

1188
00:43:26,570 --> 00:43:29,210
models or classification models or in

1189
00:43:29,210 --> 00:43:31,099
the most<font color="#E5E5E5"> generic sense conditional</font>

1190
00:43:31,099 --> 00:43:33,290
density estimation<font color="#CCCCCC"> and</font><font color="#E5E5E5"> I'll I'll say</font>

1191
00:43:33,290 --> 00:43:35,630
what density estimation is then the

1192
00:43:35,630 --> 00:43:37,670
other side<font color="#E5E5E5"> is to not be conditional is</font>

1193
00:43:37,670 --> 00:43:39,980
to do unconditional models so that's

1194
00:43:39,980 --> 00:43:41,690
typically what<font color="#CCCCCC"> people mean when they say</font>

1195
00:43:41,690 --> 00:43:44,270
supervised learning<font color="#CCCCCC"> you want to learn P</font>

1196
00:43:44,270 --> 00:43:46,250
of X like someone said at the back

1197
00:43:46,250 --> 00:43:48,080
earlier there's no targets there's no

1198
00:43:48,080 --> 00:43:51,140
labels and typically a generative<font color="#E5E5E5"> model</font>

1199
00:43:51,140 --> 00:43:54,140
<font color="#CCCCCC">is meant</font><font color="#E5E5E5"> an unconditional model but even</font>

1200
00:43:54,140 --> 00:43:56,210
that you know it's<font color="#E5E5E5"> not it's not</font>

1201
00:43:56,210 --> 00:43:58,270
something<font color="#CCCCCC"> you should</font><font color="#E5E5E5"> really live by</font><font color="#CCCCCC"> I</font>

1202
00:43:58,270 --> 00:44:00,800
guess the key point of<font color="#E5E5E5"> this is</font><font color="#CCCCCC"> to know</font>

1203
00:44:00,800 --> 00:44:02,030
that<font color="#CCCCCC"> they are conditional and</font>

1204
00:44:02,030 --> 00:44:03,500
unconditional models and these are the

1205
00:44:03,500 --> 00:44:05,119
things<font color="#CCCCCC"> we're</font><font color="#E5E5E5"> going to</font><font color="#CCCCCC"> be building</font><font color="#E5E5E5"> but</font>

1206
00:44:05,119 --> 00:44:07,970
<font color="#CCCCCC">that every probabilistic model</font><font color="#E5E5E5"> in some</font>

1207
00:44:07,970 --> 00:44:10,010
form of the<font color="#CCCCCC"> other is a</font><font color="#E5E5E5"> generative</font><font color="#CCCCCC"> model</font>

1208
00:44:10,010 --> 00:44:12,310
and I'm just<font color="#CCCCCC"> pointing this out is that</font>

1209
00:44:12,310 --> 00:44:15,950
<font color="#E5E5E5">generative model can</font><font color="#CCCCCC"> be an odd word to</font>

1210
00:44:15,950 --> 00:44:17,900
use it can<font color="#E5E5E5"> be an odd statement may be</font>

1211
00:44:17,900 --> 00:44:21,020
meaningless<font color="#E5E5E5"> at some</font><font color="#CCCCCC"> point in time so</font>

1212
00:44:21,020 --> 00:44:23,359
just<font color="#E5E5E5"> always think</font><font color="#CCCCCC"> about and</font><font color="#E5E5E5"> clarify as</font>

1213
00:44:23,359 --> 00:44:25,010
to what we're going to<font color="#E5E5E5"> mean so we're</font>

1214
00:44:25,010 --> 00:44:26,840
going to<font color="#E5E5E5"> talk</font><font color="#CCCCCC"> about</font>

1215
00:44:26,840 --> 00:44:29,510
generative<font color="#E5E5E5"> models of particular</font><font color="#CCCCCC"> Pines</font>

1216
00:44:29,510 --> 00:44:31,950
<font color="#E5E5E5">right so the first thing to</font><font color="#CCCCCC"> think about</font>

1217
00:44:31,950 --> 00:44:35,010
is density estimation<font color="#E5E5E5"> when you did your</font>

1218
00:44:35,010 --> 00:44:38,190
<font color="#E5E5E5">first thinking about statistics and</font>

1219
00:44:38,190 --> 00:44:40,200
probability then you were<font color="#E5E5E5"> exposed to</font>

1220
00:44:40,200 --> 00:44:43,020
this idea<font color="#CCCCCC"> of</font><font color="#E5E5E5"> the density of data and</font>

1221
00:44:43,020 --> 00:44:45,900
knowing its probability<font color="#E5E5E5"> and you did that</font>

1222
00:44:45,900 --> 00:44:48,120
kind<font color="#E5E5E5"> of</font><font color="#CCCCCC"> density estimation by histograms</font>

1223
00:44:48,120 --> 00:44:51,270
<font color="#E5E5E5">by kernel density estimation then later</font>

1224
00:44:51,270 --> 00:44:53,880
on<font color="#E5E5E5"> came PCA and factor analysis and</font><font color="#CCCCCC"> then</font>

1225
00:44:53,880 --> 00:44:55,890
mixture models came so these were the

1226
00:44:55,890 --> 00:44:58,320
<font color="#CCCCCC">basic tools of density estimation and</font>

1227
00:44:58,320 --> 00:45:00,960
they were any way of learning about<font color="#E5E5E5"> the</font>

1228
00:45:00,960 --> 00:45:03,270
probability density<font color="#E5E5E5"> P of</font><font color="#CCCCCC"> X from observed</font>

1229
00:45:03,270 --> 00:45:05,070
data and<font color="#E5E5E5"> all we're going to do is</font>

1230
00:45:05,070 --> 00:45:07,380
<font color="#E5E5E5">continue in this tradition and</font><font color="#CCCCCC"> build</font>

1231
00:45:07,380 --> 00:45:09,650
<font color="#E5E5E5">richer models that are going</font><font color="#CCCCCC"> to</font><font color="#E5E5E5"> help us</font>

1232
00:45:09,650 --> 00:45:13,770
deal<font color="#CCCCCC"> with really complex data so when a</font>

1233
00:45:13,770 --> 00:45:15,510
generative<font color="#CCCCCC"> model</font><font color="#E5E5E5"> can mean various</font>

1234
00:45:15,510 --> 00:45:16,380
different<font color="#CCCCCC"> things</font>

1235
00:45:16,380 --> 00:45:19,440
many people will refer to<font color="#CCCCCC"> a generative</font>

1236
00:45:19,440 --> 00:45:22,590
<font color="#E5E5E5">model</font><font color="#CCCCCC"> by this word to generate so a</font>

1237
00:45:22,590 --> 00:45:24,300
model that<font color="#E5E5E5"> allows</font><font color="#CCCCCC"> us to learn a</font>

1238
00:45:24,300 --> 00:45:27,090
simulator of data<font color="#CCCCCC"> right so in that</font><font color="#E5E5E5"> case</font>

1239
00:45:27,090 --> 00:45:29,130
you can simulate you can generate and a

1240
00:45:29,130 --> 00:45:31,260
model that<font color="#E5E5E5"> lets you do that then is a</font>

1241
00:45:31,260 --> 00:45:34,770
<font color="#E5E5E5">generative</font><font color="#CCCCCC"> model for some other people</font>

1242
00:45:34,770 --> 00:45:37,410
<font color="#CCCCCC">you will talk about</font><font color="#E5E5E5"> a model for density</font>

1243
00:45:37,410 --> 00:45:39,660
estimation and that will be a<font color="#E5E5E5"> generative</font>

1244
00:45:39,660 --> 00:45:41,400
<font color="#CCCCCC">model so if you are learning</font><font color="#E5E5E5"> P of X in</font>

1245
00:45:41,400 --> 00:45:43,980
some way or some high dimensional P of X

1246
00:45:43,980 --> 00:45:46,110
then<font color="#CCCCCC"> that</font><font color="#E5E5E5"> will be it and</font><font color="#CCCCCC"> for other</font>

1247
00:45:46,110 --> 00:45:48,090
<font color="#CCCCCC">people will</font><font color="#E5E5E5"> be just anything that</font><font color="#CCCCCC"> is</font>

1248
00:45:48,090 --> 00:45:50,130
unsupervised will also be a generative

1249
00:45:50,130 --> 00:45:52,200
<font color="#E5E5E5">model</font><font color="#CCCCCC"> in some some way or</font><font color="#E5E5E5"> the other so</font>

1250
00:45:52,200 --> 00:45:54,900
<font color="#CCCCCC">you can choose whichever definition</font><font color="#E5E5E5"> that</font>

1251
00:45:54,900 --> 00:45:56,310
you like but typically the

1252
00:45:56,310 --> 00:45:58,260
characteristics<font color="#E5E5E5"> that's</font><font color="#CCCCCC"> come</font><font color="#E5E5E5"> between all</font>

1253
00:45:58,260 --> 00:46:00,300
<font color="#CCCCCC">of</font><font color="#E5E5E5"> them is that there are probabilistic</font>

1254
00:46:00,300 --> 00:46:01,980
<font color="#E5E5E5">models that have some</font><font color="#CCCCCC"> form of</font>

1255
00:46:01,980 --> 00:46:04,170
uncertainty some form of distribution

1256
00:46:04,170 --> 00:46:06,170
<font color="#E5E5E5">that we're going to be</font><font color="#CCCCCC"> manipulating</font>

1257
00:46:06,170 --> 00:46:08,370
typically<font color="#E5E5E5"> we're always going to target</font>

1258
00:46:08,370 --> 00:46:10,860
the distribution<font color="#CCCCCC"> of the data P of X</font>

1259
00:46:10,860 --> 00:46:14,610
<font color="#E5E5E5">either directly</font><font color="#CCCCCC"> or indirectly</font><font color="#E5E5E5"> and the</font>

1260
00:46:14,610 --> 00:46:16,020
<font color="#CCCCCC">keeping I</font><font color="#E5E5E5"> think that makes it a</font>

1261
00:46:16,020 --> 00:46:18,270
generative model<font color="#E5E5E5"> rather than just</font>

1262
00:46:18,270 --> 00:46:19,680
calling<font color="#E5E5E5"> to classify is that you have</font>

1263
00:46:19,680 --> 00:46:21,360
very<font color="#CCCCCC"> high dimensional output so a</font>

1264
00:46:21,360 --> 00:46:23,850
classifier has very low dimensional

1265
00:46:23,850 --> 00:46:26,120
output it's<font color="#CCCCCC"> 1 in binary</font><font color="#E5E5E5"> classification</font>

1266
00:46:26,120 --> 00:46:28,650
<font color="#CCCCCC">1,000</font><font color="#E5E5E5"> for</font><font color="#CCCCCC"> imagenet but it's not more</font>

1267
00:46:28,650 --> 00:46:31,290
than<font color="#CCCCCC"> that</font><font color="#E5E5E5"> whereas P of X or generative</font>

1268
00:46:31,290 --> 00:46:33,180
models are entire images entire

1269
00:46:33,180 --> 00:46:35,670
sequences of events<font color="#CCCCCC"> entire speech</font>

1270
00:46:35,670 --> 00:46:38,910
signals so these are sort<font color="#CCCCCC"> of</font><font color="#E5E5E5"> some of the</font>

1271
00:46:38,910 --> 00:46:39,680
definitions

1272
00:46:39,680 --> 00:46:42,920
so I will<font color="#E5E5E5"> always encourage people to</font>

1273
00:46:42,920 --> 00:46:45,859
<font color="#CCCCCC">think about machine learning in as built</font>

1274
00:46:45,859 --> 00:46:48,500
up of three<font color="#CCCCCC"> components you have models</font>

1275
00:46:48,500 --> 00:46:50,390
and models are<font color="#E5E5E5"> the thing that you used</font>

1276
00:46:50,390 --> 00:46:52,460
<font color="#E5E5E5">to describe</font><font color="#CCCCCC"> the</font><font color="#E5E5E5"> world that you use to</font>

1277
00:46:52,460 --> 00:46:53,869
describe<font color="#CCCCCC"> the data</font><font color="#E5E5E5"> that you're actually</font>

1278
00:46:53,869 --> 00:46:55,760
<font color="#E5E5E5">dealing</font><font color="#CCCCCC"> with and to describe</font><font color="#E5E5E5"> your</font>

1279
00:46:55,760 --> 00:46:57,800
<font color="#E5E5E5">problem it will put all your domain</font>

1280
00:46:57,800 --> 00:46:59,960
knowledge<font color="#CCCCCC"> or the ways of things you</font>

1281
00:46:59,960 --> 00:47:02,839
<font color="#CCCCCC">think you should represent knowledge</font><font color="#E5E5E5"> in</font>

1282
00:47:02,839 --> 00:47:04,430
the world<font color="#E5E5E5"> and that</font><font color="#CCCCCC"> will be what you have</font>

1283
00:47:04,430 --> 00:47:07,250
in your<font color="#E5E5E5"> model</font><font color="#CCCCCC"> now you also have</font><font color="#E5E5E5"> data</font>

1284
00:47:07,250 --> 00:47:08,690
<font color="#E5E5E5">which came from the problem that you are</font>

1285
00:47:08,690 --> 00:47:10,760
thinking<font color="#CCCCCC"> about and then we have learning</font>

1286
00:47:10,760 --> 00:47:12,710
principles and learning principles are

1287
00:47:12,710 --> 00:47:14,630
those<font color="#E5E5E5"> things that connect the data that</font>

1288
00:47:14,630 --> 00:47:16,670
you<font color="#CCCCCC"> have with the model</font><font color="#E5E5E5"> that you</font><font color="#CCCCCC"> have</font>

1289
00:47:16,670 --> 00:47:18,680
specified<font color="#CCCCCC"> and you need this thing</font><font color="#E5E5E5"> that</font>

1290
00:47:18,680 --> 00:47:20,780
helps you interface these two things and

1291
00:47:20,780 --> 00:47:22,460
so<font color="#E5E5E5"> you will always have these learning</font>

1292
00:47:22,460 --> 00:47:24,319
principles and then for any choice of

1293
00:47:24,319 --> 00:47:26,480
model<font color="#CCCCCC"> and for any choice of</font><font color="#E5E5E5"> learning</font>

1294
00:47:26,480 --> 00:47:28,160
principle<font color="#E5E5E5"> you can then put those</font>

1295
00:47:28,160 --> 00:47:30,440
together<font color="#E5E5E5"> to form an algorithm and even</font>

1296
00:47:30,440 --> 00:47:32,569
an algorithm you<font color="#CCCCCC"> can form in very for</font>

1297
00:47:32,569 --> 00:47:34,190
even the same model and<font color="#E5E5E5"> the same</font>

1298
00:47:34,190 --> 00:47:35,599
principle<font color="#E5E5E5"> of learning</font><font color="#CCCCCC"> you can create</font>

1299
00:47:35,599 --> 00:47:38,240
many<font color="#E5E5E5"> different kinds of algorithms</font><font color="#CCCCCC"> and I</font>

1300
00:47:38,240 --> 00:47:40,520
<font color="#E5E5E5">think if you keep this structured point</font>

1301
00:47:40,520 --> 00:47:43,010
of view in mind<font color="#E5E5E5"> that is how you will see</font>

1302
00:47:43,010 --> 00:47:45,170
the connections like we did in all those

1303
00:47:45,170 --> 00:47:47,260
tricks<font color="#E5E5E5"> to every other</font><font color="#CCCCCC"> area of</font>

1304
00:47:47,260 --> 00:47:49,730
statistical science<font color="#CCCCCC"> whether that is in</font>

1305
00:47:49,730 --> 00:47:51,890
computational<font color="#E5E5E5"> neuroscience</font><font color="#CCCCCC"> or in</font>

1306
00:47:51,890 --> 00:47:54,049
probability<font color="#E5E5E5"> theory or in operations</font>

1307
00:47:54,049 --> 00:47:56,030
research<font color="#E5E5E5"> or even</font><font color="#CCCCCC"> in machine learning and</font>

1308
00:47:56,030 --> 00:47:58,040
that is how we<font color="#E5E5E5"> will see that things are</font>

1309
00:47:58,040 --> 00:48:00,140
<font color="#CCCCCC">the same</font><font color="#E5E5E5"> used in</font><font color="#CCCCCC"> different</font><font color="#E5E5E5"> ways and how</font>

1310
00:48:00,140 --> 00:48:02,000
we can<font color="#CCCCCC"> actually learn those tricks from</font>

1311
00:48:02,000 --> 00:48:03,740
other<font color="#E5E5E5"> fields to make our own world</font>

1312
00:48:03,740 --> 00:48:06,109
<font color="#E5E5E5">better so I want to talk just a little</font>

1313
00:48:06,109 --> 00:48:08,569
<font color="#E5E5E5">bit about models there are two types of</font>

1314
00:48:08,569 --> 00:48:10,760
<font color="#E5E5E5">models that will break them into one are</font>

1315
00:48:10,760 --> 00:48:12,950
the fully observed<font color="#E5E5E5"> models so fully</font>

1316
00:48:12,950 --> 00:48:14,510
observed models are models<font color="#CCCCCC"> that</font><font color="#E5E5E5"> you</font>

1317
00:48:14,510 --> 00:48:16,400
build based on the data that<font color="#E5E5E5"> you see</font>

1318
00:48:16,400 --> 00:48:18,799
only<font color="#E5E5E5"> right so they introduce no</font>

1319
00:48:18,799 --> 00:48:21,799
unobserved variables<font color="#E5E5E5"> and so here</font><font color="#CCCCCC"> you</font>

1320
00:48:21,799 --> 00:48:24,740
have a<font color="#E5E5E5"> undirected graphical model with a</font>

1321
00:48:24,740 --> 00:48:26,869
<font color="#CCCCCC">threeway</font><font color="#E5E5E5"> factor that is an unobserved</font>

1322
00:48:26,869 --> 00:48:29,480
<font color="#E5E5E5">graphical is a model of both the</font><font color="#CCCCCC"> machine</font>

1323
00:48:29,480 --> 00:48:31,940
is another kind<font color="#E5E5E5"> of model of that form or</font>

1324
00:48:31,940 --> 00:48:34,549
you have these kind of<font color="#E5E5E5"> auto regressive</font>

1325
00:48:34,549 --> 00:48:36,380
models<font color="#CCCCCC"> care order regress auto</font>

1326
00:48:36,380 --> 00:48:37,940
regressive models<font color="#E5E5E5"> they are fully</font>

1327
00:48:37,940 --> 00:48:39,440
observed they<font color="#CCCCCC"> only for build</font>

1328
00:48:39,440 --> 00:48:42,349
dependencies<font color="#CCCCCC"> based on observed data</font><font color="#E5E5E5"> and</font>

1329
00:48:42,349 --> 00:48:43,760
things that you can<font color="#CCCCCC"> actually</font><font color="#E5E5E5"> measure in</font>

1330
00:48:43,760 --> 00:48:46,040
the world then<font color="#CCCCCC"> you have latent</font><font color="#E5E5E5"> variable</font>

1331
00:48:46,040 --> 00:48:48,230
models<font color="#E5E5E5"> and latent variable models do</font>

1332
00:48:48,230 --> 00:48:50,480
<font color="#E5E5E5">that the opposite of that they introduce</font>

1333
00:48:50,480 --> 00:48:53,210
other variables which cannot<font color="#E5E5E5"> be</font><font color="#CCCCCC"> observed</font>

1334
00:48:53,210 --> 00:48:55,160
in the world but which we can<font color="#E5E5E5"> learn</font>

1335
00:48:55,160 --> 00:48:57,230
<font color="#CCCCCC">about based on the data</font><font color="#E5E5E5"> that we have</font>

1336
00:48:57,230 --> 00:49:00,350
seen and latent variable models are both

1337
00:49:00,350 --> 00:49:02,630
of these are equally<font color="#E5E5E5"> popular</font><font color="#CCCCCC"> machine</font>

1338
00:49:02,630 --> 00:49:04,520
<font color="#E5E5E5">learning and undulation variable models</font>

1339
00:49:04,520 --> 00:49:06,590
will have<font color="#E5E5E5"> two different kinds</font><font color="#CCCCCC"> of latent</font>

1340
00:49:06,590 --> 00:49:08,240
variable models<font color="#E5E5E5"> one will be the</font>

1341
00:49:08,240 --> 00:49:10,550
prescribed models and prescribed models

1342
00:49:10,550 --> 00:49:12,380
are<font color="#CCCCCC"> models where you</font><font color="#E5E5E5"> will decide that</font>

1343
00:49:12,380 --> 00:49:15,050
<font color="#CCCCCC">the observational data that you have</font><font color="#E5E5E5"> has</font>

1344
00:49:15,050 --> 00:49:17,150
<font color="#CCCCCC">some kind of likelihood</font><font color="#E5E5E5"> function there</font>

1345
00:49:17,150 --> 00:49:19,940
<font color="#E5E5E5">is a noise model and by choosing a noise</font>

1346
00:49:19,940 --> 00:49:21,770
model<font color="#E5E5E5"> say in this graphical model we</font>

1347
00:49:21,770 --> 00:49:23,420
have a<font color="#E5E5E5"> random variable</font><font color="#CCCCCC"> Z which</font><font color="#E5E5E5"> is</font>

1348
00:49:23,420 --> 00:49:25,820
unobserved<font color="#E5E5E5"> and</font><font color="#CCCCCC"> then we have a new random</font>

1349
00:49:25,820 --> 00:49:28,100
<font color="#CCCCCC">variable X which itself has some</font>

1350
00:49:28,100 --> 00:49:30,410
distribution<font color="#CCCCCC"> that we choose and this is</font>

1351
00:49:30,410 --> 00:49:32,270
a lot of knowledge that<font color="#CCCCCC"> we add we</font>

1352
00:49:32,270 --> 00:49:34,460
<font color="#E5E5E5">basically say that we can know something</font>

1353
00:49:34,460 --> 00:49:37,040
<font color="#CCCCCC">about</font><font color="#E5E5E5"> the probability</font><font color="#CCCCCC"> that</font><font color="#E5E5E5"> this data X</font>

1354
00:49:37,040 --> 00:49:39,230
has in the<font color="#E5E5E5"> world even</font><font color="#CCCCCC"> if</font><font color="#E5E5E5"> it's</font><font color="#CCCCCC"> just</font>

1355
00:49:39,230 --> 00:49:41,210
saying that it has Gaussian noise that

1356
00:49:41,210 --> 00:49:43,100
is a useful<font color="#CCCCCC"> amount of knowledge</font><font color="#E5E5E5"> and that</font>

1357
00:49:43,100 --> 00:49:44,780
is the likelihood<font color="#CCCCCC"> function</font><font color="#E5E5E5"> that we write</font>

1358
00:49:44,780 --> 00:49:46,520
in<font color="#CCCCCC"> all other</font><font color="#E5E5E5"> areas</font><font color="#CCCCCC"> of statistics</font><font color="#E5E5E5"> and</font>

1359
00:49:46,520 --> 00:49:48,860
machine<font color="#CCCCCC"> learning and</font><font color="#E5E5E5"> this is why this</font>

1360
00:49:48,860 --> 00:49:50,690
<font color="#E5E5E5">prescribed models are basically</font>

1361
00:49:50,690 --> 00:49:52,940
likelihood based methods of estimation

1362
00:49:52,940 --> 00:49:55,250
<font color="#CCCCCC">right and</font><font color="#E5E5E5"> similarly up at the top</font>

1363
00:49:55,250 --> 00:49:57,140
they are also<font color="#E5E5E5"> prescribed in this sense</font>

1364
00:49:57,140 --> 00:49:59,180
that<font color="#E5E5E5"> the observed models also use</font>

1365
00:49:59,180 --> 00:50:01,100
likelihood<font color="#CCCCCC"> functions and they are they</font>

1366
00:50:01,100 --> 00:50:04,970
<font color="#E5E5E5">are prescribed but implicit models they</font>

1367
00:50:04,970 --> 00:50:06,710
<font color="#E5E5E5">basically use this trick of the change</font>

1368
00:50:06,710 --> 00:50:08,480
of<font color="#E5E5E5"> variables that we discussed earlier</font>

1369
00:50:08,480 --> 00:50:10,820
<font color="#E5E5E5">they take a random variable Z and</font>

1370
00:50:10,820 --> 00:50:13,100
transform it through some function<font color="#CCCCCC"> f and</font>

1371
00:50:13,100 --> 00:50:15,020
that is what they say<font color="#E5E5E5"> they say</font><font color="#CCCCCC"> that you</font>

1372
00:50:15,020 --> 00:50:17,720
can simulate data instead and so these

1373
00:50:17,720 --> 00:50:19,430
models are sometimes called likelihood

1374
00:50:19,430 --> 00:50:21,230
free<font color="#CCCCCC"> models so if you are reading</font><font color="#E5E5E5"> in</font>

1375
00:50:21,230 --> 00:50:22,850
<font color="#CCCCCC">biostatistics you will often</font><font color="#E5E5E5"> see</font><font color="#CCCCCC"> this</font>

1376
00:50:22,850 --> 00:50:25,580
expression<font color="#E5E5E5"> of likelihood free estimation</font>

1377
00:50:25,580 --> 00:50:27,980
<font color="#E5E5E5">and likelihood free model</font><font color="#CCCCCC"> so we're going</font>

1378
00:50:27,980 --> 00:50:29,990
<font color="#CCCCCC">to look at this</font><font color="#E5E5E5"> but most of the time</font>

1379
00:50:29,990 --> 00:50:31,370
we'll<font color="#E5E5E5"> spend talking about latent</font>

1380
00:50:31,370 --> 00:50:34,310
variable models so when it comes to

1381
00:50:34,310 --> 00:50:36,740
<font color="#E5E5E5">learning principles you now have a whole</font>

1382
00:50:36,740 --> 00:50:38,960
plethora<font color="#E5E5E5"> of learning principles to</font>

1383
00:50:38,960 --> 00:50:40,370
choose from the first<font color="#CCCCCC"> ones that</font><font color="#E5E5E5"> you</font>

1384
00:50:40,370 --> 00:50:42,560
exposed<font color="#E5E5E5"> to what the exact methods where</font>

1385
00:50:42,560 --> 00:50:44,030
<font color="#CCCCCC">you did enumeration of all</font><font color="#E5E5E5"> the</font>

1386
00:50:44,030 --> 00:50:45,920
probabilities using<font color="#CCCCCC"> a</font><font color="#E5E5E5"> probability table</font>

1387
00:50:45,920 --> 00:50:47,600
or you learnt about conjugate

1388
00:50:47,600 --> 00:50:48,980
exponential<font color="#CCCCCC"> family models where you</font>

1389
00:50:48,980 --> 00:50:51,590
<font color="#E5E5E5">could do things in closed form you also</font>

1390
00:50:51,590 --> 00:50:53,450
<font color="#CCCCCC">did</font><font color="#E5E5E5"> in numerical methods</font><font color="#CCCCCC"> numerical</font>

1391
00:50:53,450 --> 00:50:54,770
integration for simple<font color="#E5E5E5"> one-dimensional</font>

1392
00:50:54,770 --> 00:50:57,320
<font color="#CCCCCC">maybe</font><font color="#E5E5E5"> two dimensional integrals you</font>

1393
00:50:57,320 --> 00:51:00,170
computed the integrals exactly<font color="#CCCCCC"> other</font>

1394
00:51:00,170 --> 00:51:02,180
<font color="#E5E5E5">methods like the generalized method of</font>

1395
00:51:02,180 --> 00:51:04,280
moments maximum likelihood maximum

1396
00:51:04,280 --> 00:51:06,760
<font color="#E5E5E5">a-posteriori</font><font color="#CCCCCC"> laplace estimation</font>

1397
00:51:06,760 --> 00:51:10,760
<font color="#CCCCCC">and so on and on and on so</font><font color="#E5E5E5"> many and many</font>

1398
00:51:10,760 --> 00:51:12,500
more being<font color="#CCCCCC"> added all the time and</font>

1399
00:51:12,500 --> 00:51:15,859
hopefully<font color="#CCCCCC"> you</font><font color="#E5E5E5"> would have seen at least a</font>

1400
00:51:15,859 --> 00:51:16,880
little<font color="#E5E5E5"> bit of all of</font><font color="#CCCCCC"> these different</font>

1401
00:51:16,880 --> 00:51:18,560
<font color="#CCCCCC">kinds of methods but we</font><font color="#E5E5E5"> all look at</font>

1402
00:51:18,560 --> 00:51:21,080
<font color="#E5E5E5">maximum likelihood expectation</font>

1403
00:51:21,080 --> 00:51:22,880
maximization variational methods and

1404
00:51:22,880 --> 00:51:25,310
well<font color="#E5E5E5"> I should update this list</font><font color="#CCCCCC"> we'll</font>

1405
00:51:25,310 --> 00:51:27,530
look at one other thing which<font color="#CCCCCC"> well just</font>

1406
00:51:27,530 --> 00:51:29,060
<font color="#E5E5E5">generically called non maximum</font>

1407
00:51:29,060 --> 00:51:30,470
<font color="#CCCCCC">likelihood so things which live outside</font>

1408
00:51:30,470 --> 00:51:33,740
<font color="#E5E5E5">this list</font><font color="#CCCCCC"> so now you get</font><font color="#E5E5E5"> to choose one</font>

1409
00:51:33,740 --> 00:51:35,450
of your<font color="#E5E5E5"> models</font><font color="#CCCCCC"> and you get to</font><font color="#E5E5E5"> choose</font>

1410
00:51:35,450 --> 00:51:37,220
literally any one of<font color="#E5E5E5"> these learning</font>

1411
00:51:37,220 --> 00:51:38,720
<font color="#E5E5E5">principles and then that's how you will</font>

1412
00:51:38,720 --> 00:51:40,940
fuse your<font color="#E5E5E5"> data in with your model and</font>

1413
00:51:40,940 --> 00:51:42,470
then<font color="#CCCCCC"> you will be able</font><font color="#E5E5E5"> to build an</font>

1414
00:51:42,470 --> 00:51:44,480
algorithm<font color="#CCCCCC"> of some sort so this is where</font>

1415
00:51:44,480 --> 00:51:46,820
everything<font color="#E5E5E5"> comes on so let's go through</font>

1416
00:51:46,820 --> 00:51:48,560
four<font color="#CCCCCC"> different cases so that you</font>

1417
00:51:48,560 --> 00:51:50,839
understand<font color="#E5E5E5"> this thing so you can take a</font>

1418
00:51:50,839 --> 00:51:52,339
convolution on your own<font color="#CCCCCC"> network this is</font>

1419
00:51:52,339 --> 00:51:54,650
<font color="#CCCCCC">a model this model</font><font color="#E5E5E5"> encodes the fact that</font>

1420
00:51:54,650 --> 00:51:56,780
<font color="#E5E5E5">we want to model images images have</font>

1421
00:51:56,780 --> 00:51:58,520
certain translational properties and

1422
00:51:58,520 --> 00:52:00,109
because of those<font color="#E5E5E5"> invariance and</font>

1423
00:52:00,109 --> 00:52:01,670
translational properties we will use

1424
00:52:01,670 --> 00:52:03,349
convolution so that's what goes<font color="#E5E5E5"> into the</font>

1425
00:52:03,349 --> 00:52:05,180
model<font color="#E5E5E5"> we're going to choose</font><font color="#CCCCCC"> to do</font>

1426
00:52:05,180 --> 00:52:06,800
penalize<font color="#CCCCCC"> maximum likelihood</font>

1427
00:52:06,800 --> 00:52:08,869
<font color="#CCCCCC">so that is</font><font color="#E5E5E5"> maximum likelihood with</font>

1428
00:52:08,869 --> 00:52:12,980
penalty Ridge regression methods or<font color="#E5E5E5"> map</font>

1429
00:52:12,980 --> 00:52:15,230
map estimation which is<font color="#CCCCCC"> core</font><font color="#E5E5E5"> and then</font>

1430
00:52:15,230 --> 00:52:17,119
<font color="#E5E5E5">you get to build an</font><font color="#CCCCCC"> algorithm in various</font>

1431
00:52:17,119 --> 00:52:18,950
different<font color="#CCCCCC"> ways you get to choose what</font>

1432
00:52:18,950 --> 00:52:21,500
<font color="#CCCCCC">kind of optimization that you</font><font color="#E5E5E5"> get are</font>

1433
00:52:21,500 --> 00:52:23,180
you going to use some precondition

1434
00:52:23,180 --> 00:52:24,650
optimizer<font color="#CCCCCC"> are you going to use some</font>

1435
00:52:24,650 --> 00:52:26,660
stochastic<font color="#CCCCCC"> optimized are you going to</font>

1436
00:52:26,660 --> 00:52:29,750
use a batch<font color="#CCCCCC"> method like BFGS you can</font>

1437
00:52:29,750 --> 00:52:31,609
choose what kind<font color="#CCCCCC"> of penalization you</font>

1438
00:52:31,609 --> 00:52:34,099
will use whether<font color="#CCCCCC"> it's l2</font><font color="#E5E5E5"> r1 what other</font>

1439
00:52:34,099 --> 00:52:36,020
kinds of regularization you are at and

1440
00:52:36,020 --> 00:52:38,240
all of these<font color="#E5E5E5"> will build very different</font>

1441
00:52:38,240 --> 00:52:40,280
kinds of<font color="#E5E5E5"> algorithms</font><font color="#CCCCCC"> that you will test</font>

1442
00:52:40,280 --> 00:52:41,300
and<font color="#E5E5E5"> this</font><font color="#CCCCCC"> is the thing that</font><font color="#E5E5E5"> you're doing</font>

1443
00:52:41,300 --> 00:52:42,650
<font color="#CCCCCC">sweeps over when</font><font color="#E5E5E5"> you're doing</font>

1444
00:52:42,650 --> 00:52:45,980
experiments<font color="#CCCCCC"> let's</font><font color="#E5E5E5"> look at this one which</font>

1445
00:52:45,980 --> 00:52:48,260
<font color="#E5E5E5">are we won't talk about</font><font color="#CCCCCC"> today</font><font color="#E5E5E5"> you have</font>

1446
00:52:48,260 --> 00:52:49,760
the restricted Boltzmann machine which

1447
00:52:49,760 --> 00:52:51,260
is<font color="#E5E5E5"> the latent variable model and it's</font>

1448
00:52:51,260 --> 00:52:53,300
undirected<font color="#E5E5E5"> and you can do maximum</font>

1449
00:52:53,300 --> 00:52:54,920
likelihood<font color="#E5E5E5"> estimation</font><font color="#CCCCCC"> to learn the</font>

1450
00:52:54,920 --> 00:52:56,599
parameters which live<font color="#CCCCCC"> in these arrows</font>

1451
00:52:56,599 --> 00:52:58,490
and there are various<font color="#CCCCCC"> different kinds of</font>

1452
00:52:58,490 --> 00:53:00,560
algorithms<font color="#CCCCCC"> that</font><font color="#E5E5E5"> you can</font><font color="#CCCCCC"> create you can</font>

1453
00:53:00,560 --> 00:53:02,060
solve that by doing contrastive

1454
00:53:02,060 --> 00:53:03,770
divergence which is built based<font color="#E5E5E5"> on</font>

1455
00:53:03,770 --> 00:53:05,869
<font color="#CCCCCC">building a Markov chain</font><font color="#E5E5E5"> to sample the</font>

1456
00:53:05,869 --> 00:53:08,570
<font color="#CCCCCC">Z's</font><font color="#E5E5E5"> given X you can do a variation</font><font color="#CCCCCC"> of</font>

1457
00:53:08,570 --> 00:53:09,950
<font color="#E5E5E5">that called persistent contrasted</font>

1458
00:53:09,950 --> 00:53:11,780
divergence you can do<font color="#E5E5E5"> other ways of</font>

1459
00:53:11,780 --> 00:53:13,700
manipulating<font color="#CCCCCC"> the gradients by tempering</font>

1460
00:53:13,700 --> 00:53:15,950
and<font color="#E5E5E5"> natural gradients and</font><font color="#CCCCCC"> the two things</font>

1461
00:53:15,950 --> 00:53:17,930
we<font color="#E5E5E5"> are going to look at</font><font color="#CCCCCC"> today are latent</font>

1462
00:53:17,930 --> 00:53:19,559
variable models

1463
00:53:19,559 --> 00:53:21,509
<font color="#CCCCCC">Plus variational inference that we can</font>

1464
00:53:21,509 --> 00:53:23,459
build many different algorithms we can

1465
00:53:23,459 --> 00:53:25,949
<font color="#CCCCCC">create a variation of the e/m algorithm</font>

1466
00:53:25,949 --> 00:53:27,930
we can<font color="#CCCCCC"> do another algorithm call</font>

1467
00:53:27,930 --> 00:53:29,819
expectation propagation we can do other

1468
00:53:29,819 --> 00:53:32,519
simplifications<font color="#CCCCCC"> call approximate message</font>

1469
00:53:32,519 --> 00:53:34,079
<font color="#E5E5E5">passing more recently we've creates</font>

1470
00:53:34,079 --> 00:53:36,410
things call variational<font color="#CCCCCC"> autoencoders</font><font color="#E5E5E5"> and</font>

1471
00:53:36,410 --> 00:53:38,939
then with the<font color="#E5E5E5"> implicit generator model</font>

1472
00:53:38,939 --> 00:53:40,949
setting<font color="#E5E5E5"> we can do the same thing we</font><font color="#CCCCCC"> can</font>

1473
00:53:40,949 --> 00:53:43,259
use<font color="#CCCCCC"> two sample testing as our</font><font color="#E5E5E5"> learning</font>

1474
00:53:43,259 --> 00:53:45,059
principle and then we can create<font color="#E5E5E5"> many</font>

1475
00:53:45,059 --> 00:53:46,650
different algorithms based on<font color="#E5E5E5"> what we</font>

1476
00:53:46,650 --> 00:53:49,019
have based a method of moments using

1477
00:53:49,019 --> 00:53:51,420
<font color="#CCCCCC">approximate Bayesian computation or the</font>

1478
00:53:51,420 --> 00:53:53,430
way<font color="#E5E5E5"> we do it in generative adversarial</font>

1479
00:53:53,430 --> 00:53:55,619
networks<font color="#E5E5E5"> but all of these take the same</font>

1480
00:53:55,619 --> 00:53:57,930
model the<font color="#CCCCCC"> same</font><font color="#E5E5E5"> inference and then end up</font>

1481
00:53:57,930 --> 00:53:59,579
<font color="#CCCCCC">with very different looking algorithms</font>

1482
00:53:59,579 --> 00:54:00,809
<font color="#E5E5E5">but they're not actually</font><font color="#CCCCCC"> that different</font>

1483
00:54:00,809 --> 00:54:02,910
<font color="#E5E5E5">right</font><font color="#CCCCCC"> they may behave different have</font>

1484
00:54:02,910 --> 00:54:04,799
different<font color="#CCCCCC"> kind of</font><font color="#E5E5E5"> use cases but</font><font color="#CCCCCC"> they are</font>

1485
00:54:04,799 --> 00:54:06,299
they have a lot<font color="#E5E5E5"> to share with each other</font>

1486
00:54:06,299 --> 00:54:09,809
<font color="#CCCCCC">so let's talk about</font><font color="#E5E5E5"> different types of</font>

1487
00:54:09,809 --> 00:54:14,009
generative models<font color="#E5E5E5"> and when you want to</font>

1488
00:54:14,009 --> 00:54:15,509
build a generative<font color="#CCCCCC"> model</font><font color="#E5E5E5"> there will be</font>

1489
00:54:15,509 --> 00:54:17,430
several design dimensions you want<font color="#E5E5E5"> to</font>

1490
00:54:17,430 --> 00:54:19,199
think about<font color="#E5E5E5"> the thing is like how</font><font color="#CCCCCC"> will</font>

1491
00:54:19,199 --> 00:54:20,819
<font color="#E5E5E5">you choose your model and choose your</font>

1492
00:54:20,819 --> 00:54:22,410
corresponding principles you<font color="#CCCCCC"> will need</font>

1493
00:54:22,410 --> 00:54:24,509
to<font color="#CCCCCC"> think about the data that</font><font color="#E5E5E5"> you</font><font color="#CCCCCC"> have</font>

1494
00:54:24,509 --> 00:54:26,910
whether the data is<font color="#E5E5E5"> binary whether it is</font>

1495
00:54:26,910 --> 00:54:28,619
real valued whether it's<font color="#CCCCCC"> some</font><font color="#E5E5E5"> mixture</font><font color="#CCCCCC"> of</font>

1496
00:54:28,619 --> 00:54:30,749
<font color="#E5E5E5">the two</font><font color="#CCCCCC"> whether they have some ordering</font>

1497
00:54:30,749 --> 00:54:32,519
involved in the data and that<font color="#CCCCCC"> data is</font>

1498
00:54:32,519 --> 00:54:34,559
<font color="#CCCCCC">going to affect</font><font color="#E5E5E5"> how you will design your</font>

1499
00:54:34,559 --> 00:54:36,179
model you will need to<font color="#CCCCCC"> think about</font><font color="#E5E5E5"> the</font>

1500
00:54:36,179 --> 00:54:38,549
dependency<font color="#E5E5E5"> structure in that data can</font>

1501
00:54:38,549 --> 00:54:40,170
<font color="#CCCCCC">you assume that</font><font color="#E5E5E5"> you just have a set of</font>

1502
00:54:40,170 --> 00:54:42,269
images<font color="#CCCCCC"> that</font><font color="#E5E5E5"> are all independent or</font><font color="#CCCCCC"> iya</font>

1503
00:54:42,269 --> 00:54:44,039
<font color="#E5E5E5">dealing with the time series in which</font>

1504
00:54:44,039 --> 00:54:45,599
<font color="#E5E5E5">case there is a temporal structure which</font>

1505
00:54:45,599 --> 00:54:47,279
<font color="#CCCCCC">you need to account for or are we</font>

1506
00:54:47,279 --> 00:54:48,660
<font color="#E5E5E5">dealing</font><font color="#CCCCCC"> with some maps where we're</font>

1507
00:54:48,660 --> 00:54:50,729
modeling bird movements or forest fires

1508
00:54:50,729 --> 00:54:52,979
<font color="#E5E5E5">and then there's a spatial aspect that</font>

1509
00:54:52,979 --> 00:54:55,349
we have<font color="#CCCCCC"> to deal with</font><font color="#E5E5E5"> you need to think</font>

1510
00:54:55,349 --> 00:54:56,999
about<font color="#E5E5E5"> elements</font><font color="#CCCCCC"> of the representation</font>

1511
00:54:56,999 --> 00:54:59,249
<font color="#CCCCCC">that were gee</font><font color="#E5E5E5"> are those latent variables</font>

1512
00:54:59,249 --> 00:55:00,869
we will introduce or unobserved

1513
00:55:00,869 --> 00:55:03,229
variables or<font color="#E5E5E5"> dealing</font><font color="#CCCCCC"> with</font><font color="#E5E5E5"> the causality</font>

1514
00:55:03,229 --> 00:55:05,459
<font color="#CCCCCC">continuous or the</font><font color="#E5E5E5"> discrete or</font><font color="#CCCCCC"> there's</font>

1515
00:55:05,459 --> 00:55:07,499
some<font color="#CCCCCC"> mixed</font><font color="#E5E5E5"> are the</font><font color="#CCCCCC"> continuous-time on</font>

1516
00:55:07,499 --> 00:55:09,390
the<font color="#E5E5E5"> discrete-time and then we the last</font>

1517
00:55:09,390 --> 00:55:11,309
one is often<font color="#CCCCCC"> to think about the kind of</font>

1518
00:55:11,309 --> 00:55:13,140
dimensionality we want to deal with<font color="#E5E5E5"> are</font>

1519
00:55:13,140 --> 00:55:14,910
we going to deal with<font color="#E5E5E5"> parametric</font>

1520
00:55:14,910 --> 00:55:16,619
functions and parametric models which

1521
00:55:16,619 --> 00:55:17,939
means we're going<font color="#E5E5E5"> to build very large</font>

1522
00:55:17,939 --> 00:55:20,549
functions with lots of parameters<font color="#CCCCCC"> that</font>

1523
00:55:20,549 --> 00:55:22,529
we're going<font color="#E5E5E5"> to choose by optimization or</font>

1524
00:55:22,529 --> 00:55:24,420
are<font color="#E5E5E5"> we going to do some other kind of</font>

1525
00:55:24,420 --> 00:55:26,429
methods which<font color="#E5E5E5"> are nonparametric</font><font color="#CCCCCC"> infinite</font>

1526
00:55:26,429 --> 00:55:28,229
<font color="#CCCCCC">dimensional</font><font color="#E5E5E5"> that will rely on the data</font>

1527
00:55:28,229 --> 00:55:30,420
to inform<font color="#CCCCCC"> the</font><font color="#E5E5E5"> kind of predictions or</font>

1528
00:55:30,420 --> 00:55:32,309
inferential questions that we ask<font color="#E5E5E5"> and</font>

1529
00:55:32,309 --> 00:55:33,460
then you have other kind

1530
00:55:33,460 --> 00:55:34,450
the things which<font color="#E5E5E5"> really affect your</font>

1531
00:55:34,450 --> 00:55:36,310
decision-making<font color="#CCCCCC"> the computational</font>

1532
00:55:36,310 --> 00:55:38,740
complexity<font color="#E5E5E5"> the</font><font color="#CCCCCC"> modeling capacity that</font>

1533
00:55:38,740 --> 00:55:41,290
<font color="#E5E5E5">they have whether there's bias</font><font color="#CCCCCC"> whether</font>

1534
00:55:41,290 --> 00:55:43,750
<font color="#E5E5E5">you need uncertainty how well calibrated</font>

1535
00:55:43,750 --> 00:55:45,730
those models<font color="#E5E5E5"> are as it they represent</font>

1536
00:55:45,730 --> 00:55:47,410
the probability<font color="#CCCCCC"> of the</font><font color="#E5E5E5"> data</font><font color="#CCCCCC"> that you saw</font>

1537
00:55:47,410 --> 00:55:49,540
<font color="#E5E5E5">whether your model</font><font color="#CCCCCC"> is interpretable to</font>

1538
00:55:49,540 --> 00:55:51,580
humans or not<font color="#E5E5E5"> all of</font><font color="#CCCCCC"> these</font><font color="#E5E5E5"> things matter</font>

1539
00:55:51,580 --> 00:55:53,800
<font color="#E5E5E5">and there is a generative model</font><font color="#CCCCCC"> that you</font>

1540
00:55:53,800 --> 00:55:57,460
can decide and design based on<font color="#E5E5E5"> what your</font>

1541
00:55:57,460 --> 00:55:59,560
needs<font color="#CCCCCC"> are so the first kind of</font>

1542
00:55:59,560 --> 00:56:01,240
<font color="#CCCCCC">generative model have these fully</font>

1543
00:56:01,240 --> 00:56:03,550
observed models and as they said<font color="#E5E5E5"> a fully</font>

1544
00:56:03,550 --> 00:56:05,619
observed model deals<font color="#CCCCCC"> on the data</font>

1545
00:56:05,619 --> 00:56:07,839
<font color="#E5E5E5">directly and they don't introduce any</font>

1546
00:56:07,839 --> 00:56:09,730
other variables other than<font color="#E5E5E5"> what is</font>

1547
00:56:09,730 --> 00:56:12,339
<font color="#E5E5E5">observed so here is a chain of data and</font>

1548
00:56:12,339 --> 00:56:14,619
the data is<font color="#CCCCCC"> just dependent</font><font color="#E5E5E5"> only</font><font color="#CCCCCC"> another</font>

1549
00:56:14,619 --> 00:56:16,450
<font color="#CCCCCC">data point</font><font color="#E5E5E5"> and of course all the arrows</font>

1550
00:56:16,450 --> 00:56:19,330
represent some kind<font color="#E5E5E5"> of functions and I</font>

1551
00:56:19,330 --> 00:56:21,490
just want to<font color="#E5E5E5"> make a distinction between</font>

1552
00:56:21,490 --> 00:56:24,460
two<font color="#CCCCCC"> different</font><font color="#E5E5E5"> things we call sometimes</font>

1553
00:56:24,460 --> 00:56:25,990
these model parameters we'll call<font color="#E5E5E5"> them</font>

1554
00:56:25,990 --> 00:56:28,390
global parameters global parameters are

1555
00:56:28,390 --> 00:56:30,160
things which<font color="#E5E5E5"> are relevant to all</font><font color="#CCCCCC"> data</font>

1556
00:56:30,160 --> 00:56:32,380
points<font color="#CCCCCC"> that you see and</font><font color="#E5E5E5"> often you will</font>

1557
00:56:32,380 --> 00:56:35,200
talk about local variables or local

1558
00:56:35,200 --> 00:56:37,089
parameters and local variables<font color="#E5E5E5"> and local</font>

1559
00:56:37,089 --> 00:56:38,950
parameters are something which are

1560
00:56:38,950 --> 00:56:41,190
specific<font color="#CCCCCC"> to individual data points so</font>

1561
00:56:41,190 --> 00:56:43,720
<font color="#E5E5E5">typically parameters theta of your model</font>

1562
00:56:43,720 --> 00:56:45,099
<font color="#CCCCCC">are</font><font color="#E5E5E5"> something you learn over the whole</font>

1563
00:56:45,099 --> 00:56:47,109
data set<font color="#E5E5E5"> but</font><font color="#CCCCCC"> you will learn latent</font>

1564
00:56:47,109 --> 00:56:50,500
<font color="#E5E5E5">variables</font><font color="#CCCCCC"> em4 data point</font><font color="#E5E5E5"> xn so this is</font>

1565
00:56:50,500 --> 00:56:52,240
this<font color="#E5E5E5"> distinction between local and</font>

1566
00:56:52,240 --> 00:56:54,550
<font color="#E5E5E5">global variables</font><font color="#CCCCCC"> and so one kind are</font>

1567
00:56:54,550 --> 00:56:56,470
fully observed models are Markov models

1568
00:56:56,470 --> 00:56:58,990
they will start with some<font color="#E5E5E5"> X 1 with a</font>

1569
00:56:58,990 --> 00:57:01,180
<font color="#E5E5E5">categorical distribution then</font><font color="#CCCCCC"> you above</font>

1570
00:57:01,180 --> 00:57:03,160
build an autoregressive chain<font color="#CCCCCC"> we'll say</font>

1571
00:57:03,160 --> 00:57:05,109
<font color="#E5E5E5">X 2 is the categorical distribution</font>

1572
00:57:05,109 --> 00:57:08,109
conditioned on X 1 with<font color="#E5E5E5"> some function</font><font color="#CCCCCC"> pi</font>

1573
00:57:08,109 --> 00:57:10,780
and so<font color="#CCCCCC"> on and so forth</font><font color="#E5E5E5"> until</font><font color="#CCCCCC"> you reach X</font>

1574
00:57:10,780 --> 00:57:12,849
I this is one of<font color="#E5E5E5"> the most children the</font>

1575
00:57:12,849 --> 00:57:14,560
oldest<font color="#CCCCCC"> model probably</font><font color="#E5E5E5"> in all of</font>

1576
00:57:14,560 --> 00:57:17,290
computational<font color="#CCCCCC"> thinking</font><font color="#E5E5E5"> people</font><font color="#CCCCCC"> have won</font>

1577
00:57:17,290 --> 00:57:19,690
Nobel<font color="#E5E5E5"> prizes</font><font color="#CCCCCC"> for building a model like</font>

1578
00:57:19,690 --> 00:57:21,730
this<font color="#CCCCCC"> right</font><font color="#E5E5E5"> and so very important and</font>

1579
00:57:21,730 --> 00:57:24,490
very powerful<font color="#CCCCCC"> and then you can write the</font>

1580
00:57:24,490 --> 00:57:26,619
joint probability as simply the<font color="#E5E5E5"> product</font>

1581
00:57:26,619 --> 00:57:30,330
<font color="#E5E5E5">of I</font><font color="#CCCCCC"> or P</font><font color="#E5E5E5"> of</font><font color="#CCCCCC"> x given</font><font color="#E5E5E5"> all previous</font><font color="#CCCCCC"> FS</font><font color="#E5E5E5"> and</font>

1582
00:57:30,330 --> 00:57:33,250
depending on<font color="#E5E5E5"> what she chooses</font><font color="#CCCCCC"> F if F</font>

1583
00:57:33,250 --> 00:57:36,609
allows you to do some variable order you

1584
00:57:36,609 --> 00:57:38,680
can build infinite dimensional or K

1585
00:57:38,680 --> 00:57:41,109
order autoregressive<font color="#E5E5E5"> models and you can</font>

1586
00:57:41,109 --> 00:57:42,820
<font color="#E5E5E5">condition these kind of models</font><font color="#CCCCCC"> on other</font>

1587
00:57:42,820 --> 00:57:44,200
external<font color="#CCCCCC"> quantity</font>

1588
00:57:44,200 --> 00:57:46,150
and<font color="#E5E5E5"> what</font><font color="#CCCCCC"> for example in economics you'll</font>

1589
00:57:46,150 --> 00:57:48,999
<font color="#E5E5E5">hear is called these narcs models and</font>

1590
00:57:48,999 --> 00:57:50,950
<font color="#E5E5E5">nonlinear auto regressive models with</font>

1591
00:57:50,950 --> 00:57:53,109
exogenous variables<font color="#CCCCCC"> if you've seen those</font>

1592
00:57:53,109 --> 00:57:55,269
these are all these models<font color="#E5E5E5"> but for the</font>

1593
00:57:55,269 --> 00:57:56,890
case<font color="#CCCCCC"> that I</font><font color="#E5E5E5"> am thinking of the case</font><font color="#CCCCCC"> that</font>

1594
00:57:56,890 --> 00:57:58,779
<font color="#E5E5E5">many people do these days</font><font color="#CCCCCC"> all these</font>

1595
00:57:58,779 --> 00:58:01,660
functions<font color="#CCCCCC"> are for PI R builds by deep</font>

1596
00:58:01,660 --> 00:58:03,099
neural<font color="#E5E5E5"> networks so</font><font color="#CCCCCC"> that you can actually</font>

1597
00:58:03,099 --> 00:58:05,440
build and learn<font color="#CCCCCC"> Mis cannibal way</font><font color="#E5E5E5"> so</font>

1598
00:58:05,440 --> 00:58:06,730
fully observed models have several

1599
00:58:06,730 --> 00:58:08,769
properties<font color="#E5E5E5"> as I said because they are</font>

1600
00:58:08,769 --> 00:58:11,140
fully observed they directly encode how

1601
00:58:11,140 --> 00:58:12,819
data points are observed in<font color="#E5E5E5"> the world</font>

1602
00:58:12,819 --> 00:58:14,349
which<font color="#CCCCCC"> means you don't</font><font color="#E5E5E5"> need to make too</font>

1603
00:58:14,349 --> 00:58:17,109
many assumptions about what's going on

1604
00:58:17,109 --> 00:58:19,299
<font color="#E5E5E5">any data type can be used whether it's</font>

1605
00:58:19,299 --> 00:58:21,160
discrete or continuous or even a mixed

1606
00:58:21,160 --> 00:58:23,289
and if you're building a directed

1607
00:58:23,289 --> 00:58:25,029
graphical model like the graphs that I

1608
00:58:25,029 --> 00:58:26,140
was just<font color="#E5E5E5"> showing where there is the</font>

1609
00:58:26,140 --> 00:58:28,809
<font color="#E5E5E5">arrow of dependency then parameter</font>

1610
00:58:28,809 --> 00:58:30,430
<font color="#CCCCCC">learning is going to be very simple</font><font color="#E5E5E5"> you</font>

1611
00:58:30,430 --> 00:58:32,019
can<font color="#CCCCCC"> write up the log likelihood that</font><font color="#E5E5E5"> we</font>

1612
00:58:32,019 --> 00:58:34,299
wrote there before and then<font color="#CCCCCC"> you just</font>

1613
00:58:34,299 --> 00:58:35,740
<font color="#E5E5E5">need to take simply the gradient and</font>

1614
00:58:35,740 --> 00:58:37,450
<font color="#CCCCCC">that is the</font><font color="#E5E5E5"> easiest thing you</font><font color="#CCCCCC"> can do</font><font color="#E5E5E5"> you</font>

1615
00:58:37,450 --> 00:58:41,109
know<font color="#CCCCCC"> the log likelihood</font><font color="#E5E5E5"> exactly and you</font>

1616
00:58:41,109 --> 00:58:42,910
<font color="#E5E5E5">can do that fast and you can scale this</font>

1617
00:58:42,910 --> 00:58:45,069
out<font color="#E5E5E5"> to very large</font><font color="#CCCCCC"> models then you have</font>

1618
00:58:45,069 --> 00:58:46,839
lots<font color="#E5E5E5"> of</font><font color="#CCCCCC"> different kind of optimization</font>

1619
00:58:46,839 --> 00:58:48,400
and lots of different<font color="#CCCCCC"> applications over</font>

1620
00:58:48,400 --> 00:58:51,190
<font color="#E5E5E5">time but of course there is this order</font>

1621
00:58:51,190 --> 00:58:52,660
which were<font color="#E5E5E5"> using things in and so</font>

1622
00:58:52,660 --> 00:58:54,670
there's an order sensitivity which is

1623
00:58:54,670 --> 00:58:56,589
<font color="#E5E5E5">coming in and if we were</font><font color="#CCCCCC"> dealing with</font>

1624
00:58:56,589 --> 00:58:58,930
directed models then think undirected

1625
00:58:58,930 --> 00:59:01,119
models<font color="#E5E5E5"> and that's much more</font><font color="#CCCCCC"> difficult</font>

1626
00:59:01,119 --> 00:59:03,130
because<font color="#CCCCCC"> parameter learning in undirected</font>

1627
00:59:03,130 --> 00:59:05,319
models is very difficult because<font color="#E5E5E5"> we</font><font color="#CCCCCC"> need</font>

1628
00:59:05,319 --> 00:59:06,999
to<font color="#E5E5E5"> know the normalizing constant it's</font>

1629
00:59:06,999 --> 00:59:10,019
not known as in<font color="#E5E5E5"> the directed case and</font>

1630
00:59:10,019 --> 00:59:12,880
generation can<font color="#CCCCCC"> be very slow in either</font><font color="#E5E5E5"> of</font>

1631
00:59:12,880 --> 00:59:14,170
<font color="#E5E5E5">these models</font><font color="#CCCCCC"> because we need to go</font>

1632
00:59:14,170 --> 00:59:16,359
through<font color="#E5E5E5"> the sequential process of</font>

1633
00:59:16,359 --> 00:59:17,980
simulating from the Markov chain but

1634
00:59:17,980 --> 00:59:19,960
when you do do that<font color="#E5E5E5"> for example</font><font color="#CCCCCC"> one</font>

1635
00:59:19,960 --> 00:59:21,880
model called pixel<font color="#CCCCCC"> CNN and it has now</font>

1636
00:59:21,880 --> 00:59:25,150
various kinds<font color="#E5E5E5"> of instantiation thus far</font>

1637
00:59:25,150 --> 00:59:27,630
can give you like<font color="#E5E5E5"> really amazing</font>

1638
00:59:27,630 --> 00:59:29,650
<font color="#E5E5E5">unconditional samples or conditional</font>

1639
00:59:29,650 --> 00:59:32,019
samples in this case that really<font color="#E5E5E5"> sure it</font>

1640
00:59:32,019 --> 00:59:34,390
has<font color="#E5E5E5"> learnt to date on and really can do</font>

1641
00:59:34,390 --> 00:59:37,450
amazing<font color="#E5E5E5"> things</font><font color="#CCCCCC"> so if you want to</font><font color="#E5E5E5"> look at</font>

1642
00:59:37,450 --> 00:59:39,849
sort<font color="#E5E5E5"> of the space of different kinds of</font>

1643
00:59:39,849 --> 00:59:43,029
models then one way to<font color="#E5E5E5"> think of there</font>

1644
00:59:43,029 --> 00:59:44,980
would<font color="#CCCCCC"> be</font><font color="#E5E5E5"> along an axis of directed</font>

1645
00:59:44,980 --> 00:59:46,690
models versus undirected graphical

1646
00:59:46,690 --> 00:59:49,599
models and another axis of continuous

1647
00:59:49,599 --> 00:59:52,029
variables<font color="#E5E5E5"> versus discrete variables and</font>

1648
00:59:52,029 --> 00:59:54,009
then you in any<font color="#E5E5E5"> quadrant that you choose</font>

1649
00:59:54,009 --> 00:59:56,380
you can find a kind of fully observed

1650
00:59:56,380 --> 00:59:57,570
model

1651
00:59:57,570 --> 01:00:00,570
across any any area of statistical

1652
01:00:00,570 --> 01:00:03,750
science so let's choose a good<font color="#E5E5E5"> block so</font>

1653
01:00:03,750 --> 01:00:04,620
here's a good one

1654
01:00:04,620 --> 01:00:06,360
undirected graphical models with

1655
01:00:06,360 --> 01:00:08,790
continuous variables<font color="#E5E5E5"> for example many of</font>

1656
01:00:08,790 --> 01:00:10,770
<font color="#E5E5E5">you would</font><font color="#CCCCCC"> have studied thousand mrs</font>

1657
01:00:10,770 --> 01:00:12,720
which are Markov random field<font color="#E5E5E5"> and then</font>

1658
01:00:12,720 --> 01:00:14,610
these<font color="#CCCCCC"> are models there and then you can</font>

1659
01:00:14,610 --> 01:00:16,440
learn<font color="#CCCCCC"> about all the things in this</font><font color="#E5E5E5"> they</font>

1660
01:00:16,440 --> 01:00:17,670
<font color="#E5E5E5">are long linear models as they're</font>

1661
01:00:17,670 --> 01:00:19,830
sometimes call people don't work in this

1662
01:00:19,830 --> 01:00:21,750
space<font color="#E5E5E5"> anymore of discrete and undirected</font>

1663
01:00:21,750 --> 01:00:23,730
models but both<font color="#CCCCCC"> of</font><font color="#E5E5E5"> machines are there</font>

1664
01:00:23,730 --> 01:00:26,040
Ising models<font color="#E5E5E5"> hopfield networks parts</font>

1665
01:00:26,040 --> 01:00:28,350
models live in this case but<font color="#E5E5E5"> where most</font>

1666
01:00:28,350 --> 01:00:30,360
people<font color="#E5E5E5"> work in this case is in the</font>

1667
01:00:30,360 --> 01:00:33,210
<font color="#E5E5E5">discrete and directed case this images</font>

1668
01:00:33,210 --> 01:00:35,640
can be represented between 0 and 255 and

1669
01:00:35,640 --> 01:00:37,290
so that's discrete<font color="#E5E5E5"> and you can build</font>

1670
01:00:37,290 --> 01:00:39,240
made fully visible sigmoid belief

1671
01:00:39,240 --> 01:00:41,340
networks this pixel<font color="#CCCCCC"> CNN which I just</font>

1672
01:00:41,340 --> 01:00:44,310
described to<font color="#E5E5E5"> you</font><font color="#CCCCCC"> any RNN language model</font>

1673
01:00:44,310 --> 01:00:45,570
that<font color="#E5E5E5"> people brought which in this</font>

1674
01:00:45,570 --> 01:00:47,580
category<font color="#E5E5E5"> and other models which live</font>

1675
01:00:47,580 --> 01:00:51,060
<font color="#CCCCCC">outside even of more the mainstream like</font>

1676
01:00:51,060 --> 01:00:53,820
<font color="#E5E5E5">context-free switching algorithms that</font>

1677
01:00:53,820 --> 01:00:56,130
<font color="#E5E5E5">live in this space so so</font><font color="#CCCCCC"> a lot of fun</font>

1678
01:00:56,130 --> 01:00:58,800
fun models and to<font color="#E5E5E5"> explore in this in</font>

1679
01:00:58,800 --> 01:01:01,980
this space<font color="#CCCCCC"> so then we'll go to</font><font color="#E5E5E5"> the</font>

1680
01:01:01,980 --> 01:01:03,300
second<font color="#CCCCCC"> part which are</font><font color="#E5E5E5"> these latent</font>

1681
01:01:03,300 --> 01:01:04,350
variable months the latent variable

1682
01:01:04,350 --> 01:01:07,500
<font color="#CCCCCC">model introduced and unobserved and what</font>

1683
01:01:07,500 --> 01:01:09,420
<font color="#CCCCCC">we'll call local random variables</font><font color="#E5E5E5"> so</font>

1684
01:01:09,420 --> 01:01:11,370
instead of<font color="#E5E5E5"> only X</font><font color="#CCCCCC"> there's now another</font>

1685
01:01:11,370 --> 01:01:13,320
variable<font color="#E5E5E5"> said and that is something you</font>

1686
01:01:13,320 --> 01:01:15,300
<font color="#E5E5E5">can't measure but said is something very</font>

1687
01:01:15,300 --> 01:01:17,850
powerful<font color="#E5E5E5"> because introducing said a</font>

1688
01:01:17,850 --> 01:01:20,070
helps you understand issues<font color="#E5E5E5"> around the</font>

1689
01:01:20,070 --> 01:01:21,690
causal structure of the data and the

1690
01:01:21,690 --> 01:01:23,610
world that<font color="#E5E5E5"> you're dealing in and it</font>

1691
01:01:23,610 --> 01:01:24,890
allows you to<font color="#CCCCCC"> build very complex</font>

1692
01:01:24,890 --> 01:01:27,270
dependency structures so you don't need

1693
01:01:27,270 --> 01:01:29,160
to<font color="#CCCCCC"> design the dependency structure</font><font color="#E5E5E5"> by</font>

1694
01:01:29,160 --> 01:01:31,920
hand<font color="#E5E5E5"> you</font><font color="#CCCCCC"> can introduce the</font><font color="#E5E5E5"> Z integrated</font>

1695
01:01:31,920 --> 01:01:33,420
out and then it will induce<font color="#CCCCCC"> the</font>

1696
01:01:33,420 --> 01:01:35,430
dependency structure<font color="#E5E5E5"> in X which is the</font>

1697
01:01:35,430 --> 01:01:37,290
thing you actually<font color="#E5E5E5"> want to have and so</font>

1698
01:01:37,290 --> 01:01:39,120
here's one<font color="#E5E5E5"> kind of</font><font color="#CCCCCC"> model</font><font color="#E5E5E5"> which is called</font>

1699
01:01:39,120 --> 01:01:40,890
a deep latent Gaussian model it's a

1700
01:01:40,890 --> 01:01:43,590
directed graphical model it has<font color="#CCCCCC"> several</font>

1701
01:01:43,590 --> 01:01:46,590
layers of latent variables which are

1702
01:01:46,590 --> 01:01:48,780
stochastic hidden layers and<font color="#E5E5E5"> then you</font>

1703
01:01:48,780 --> 01:01:50,820
connect<font color="#E5E5E5"> them through deep networks in</font>

1704
01:01:50,820 --> 01:01:52,050
any way<font color="#E5E5E5"> that</font><font color="#CCCCCC"> she likes</font>

1705
01:01:52,050 --> 01:01:54,810
example<font color="#E5E5E5"> z3 from a Gaussian</font><font color="#CCCCCC"> you can</font>

1706
01:01:54,810 --> 01:01:57,510
<font color="#E5E5E5">sample z2 conditioned on z3 from a</font>

1707
01:01:57,510 --> 01:01:59,400
Gaussian with parameters<font color="#CCCCCC"> mu and Sigma</font>

1708
01:01:59,400 --> 01:02:01,890
<font color="#CCCCCC">that are functions of the previous Z and</font>

1709
01:02:01,890 --> 01:02:05,310
we can create a<font color="#E5E5E5"> tree a hierarchy</font><font color="#CCCCCC"> in this</font>

1710
01:02:05,310 --> 01:02:07,320
case and then finally we'd get<font color="#E5E5E5"> your</font>

1711
01:02:07,320 --> 01:02:09,240
final the observed data where<font color="#CCCCCC"> we choose</font>

1712
01:02:09,240 --> 01:02:11,069
a<font color="#E5E5E5"> likelihood function when this</font>

1713
01:02:11,069 --> 01:02:12,749
case it is a Gaussian<font color="#CCCCCC"> but it can be</font>

1714
01:02:12,749 --> 01:02:14,609
anything can be Bernoulli distribution

1715
01:02:14,609 --> 01:02:16,920
for binary<font color="#E5E5E5"> data can be a multinomial</font>

1716
01:02:16,920 --> 01:02:18,509
distribution<font color="#E5E5E5"> if we had some form of</font>

1717
01:02:18,509 --> 01:02:20,789
<font color="#CCCCCC">categorical data</font><font color="#E5E5E5"> and it can be products</font>

1718
01:02:20,789 --> 01:02:23,339
for mixed mixed<font color="#E5E5E5"> kind of distributions or</font>

1719
01:02:23,339 --> 01:02:26,549
<font color="#E5E5E5">non-negative quantities so latent</font>

1720
01:02:26,549 --> 01:02:27,630
variable models have very different

1721
01:02:27,630 --> 01:02:30,119
<font color="#CCCCCC">properties</font><font color="#E5E5E5"> they have</font><font color="#CCCCCC"> very easy sampling</font>

1722
01:02:30,119 --> 01:02:31,920
<font color="#CCCCCC">because you can just</font><font color="#E5E5E5"> follow this tree</font>

1723
01:02:31,920 --> 01:02:33,719
and<font color="#E5E5E5"> then you can start</font><font color="#CCCCCC"> from dead tree</font>

1724
01:02:33,719 --> 01:02:35,579
generates<font color="#E5E5E5"> it to generate said one get</font>

1725
01:02:35,579 --> 01:02:37,499
the sampling<font color="#CCCCCC"> they're an easy</font><font color="#E5E5E5"> way to</font>

1726
01:02:37,499 --> 01:02:40,019
include<font color="#E5E5E5"> this hierarchical structure or</font>

1727
01:02:40,019 --> 01:02:43,170
depth into your model<font color="#CCCCCC"> and it's easy to</font>

1728
01:02:43,170 --> 01:02:45,329
<font color="#E5E5E5">encode the structure that</font><font color="#CCCCCC"> you</font><font color="#E5E5E5"> believe in</font>

1729
01:02:45,329 --> 01:02:47,309
the world so<font color="#E5E5E5"> for example physicists</font>

1730
01:02:47,309 --> 01:02:49,440
actually do have<font color="#CCCCCC"> knowledge about how</font>

1731
01:02:49,440 --> 01:02:51,779
they think<font color="#CCCCCC"> the</font><font color="#E5E5E5"> image of the</font><font color="#CCCCCC"> galaxy</font>

1732
01:02:51,779 --> 01:02:54,269
<font color="#CCCCCC">appearing on the telescope appears how</font>

1733
01:02:54,269 --> 01:02:55,949
every pixel appears and you<font color="#E5E5E5"> can put that</font>

1734
01:02:55,949 --> 01:02:57,589
<font color="#CCCCCC">knowledge in build a graphical model</font>

1735
01:02:57,589 --> 01:03:00,059
<font color="#E5E5E5">very similar</font><font color="#CCCCCC"> to what what I</font><font color="#E5E5E5"> just showed</font>

1736
01:03:00,059 --> 01:03:01,949
you<font color="#E5E5E5"> in the previous slide and you avoid</font>

1737
01:03:01,949 --> 01:03:03,660
this<font color="#E5E5E5"> order dependency that we saw</font>

1738
01:03:03,660 --> 01:03:06,089
<font color="#E5E5E5">because marginalization this integration</font>

1739
01:03:06,089 --> 01:03:07,380
of the latent variables induces

1740
01:03:07,380 --> 01:03:10,529
dependencies<font color="#E5E5E5"> and latent variables have a</font>

1741
01:03:10,529 --> 01:03:12,599
different interpretation if you are

1742
01:03:12,599 --> 01:03:14,489
thinking putting<font color="#E5E5E5"> your hat</font><font color="#CCCCCC"> on from</font>

1743
01:03:14,489 --> 01:03:16,739
<font color="#E5E5E5">information theory or from compression</font>

1744
01:03:16,739 --> 01:03:19,019
<font color="#E5E5E5">theory as a compression or</font>

1745
01:03:19,019 --> 01:03:22,049
representation of the<font color="#CCCCCC"> data and I guess</font>

1746
01:03:22,049 --> 01:03:23,430
one of the important things is<font color="#CCCCCC"> that we</font>

1747
01:03:23,430 --> 01:03:25,559
always<font color="#CCCCCC"> want to do</font><font color="#E5E5E5"> model scoring we want</font>

1748
01:03:25,559 --> 01:03:27,269
to do model comparison we want to choose

1749
01:03:27,269 --> 01:03:29,519
<font color="#CCCCCC">the best</font><font color="#E5E5E5"> model</font><font color="#CCCCCC"> for the problem</font><font color="#E5E5E5"> that we</font>

1750
01:03:29,519 --> 01:03:31,619
have and being<font color="#CCCCCC"> able to</font><font color="#E5E5E5"> compute the</font>

1751
01:03:31,619 --> 01:03:33,660
<font color="#CCCCCC">marginalize likelihood is what we can do</font>

1752
01:03:33,660 --> 01:03:35,519
in latent variable models but what is

1753
01:03:35,519 --> 01:03:37,259
difficult<font color="#E5E5E5"> is that</font><font color="#CCCCCC"> you need</font><font color="#E5E5E5"> to know these</font>

1754
01:03:37,259 --> 01:03:40,319
<font color="#E5E5E5">latent</font><font color="#CCCCCC"> variables and to</font><font color="#E5E5E5"> be able to do</font>

1755
01:03:40,319 --> 01:03:41,789
any<font color="#E5E5E5"> of these</font><font color="#CCCCCC"> things and that inversion</font>

1756
01:03:41,789 --> 01:03:44,549
<font color="#CCCCCC">process is</font><font color="#E5E5E5"> very hard it can</font><font color="#CCCCCC"> be difficult</font>

1757
01:03:44,549 --> 01:03:46,289
to<font color="#E5E5E5"> compute the marginalize likelihood</font>

1758
01:03:46,289 --> 01:03:48,059
which is why we need<font color="#CCCCCC"> all the tricks from</font>

1759
01:03:48,059 --> 01:03:51,299
part<font color="#E5E5E5"> one and it may not be easy to</font>

1760
01:03:51,299 --> 01:03:53,489
<font color="#CCCCCC">specify</font><font color="#E5E5E5"> because</font><font color="#CCCCCC"> you need to know these</font>

1761
01:03:53,489 --> 01:03:54,989
sort of<font color="#E5E5E5"> going</font><font color="#CCCCCC"> back to this inversion</font>

1762
01:03:54,989 --> 01:03:57,239
<font color="#E5E5E5">process you may have to choose the kind</font>

1763
01:03:57,239 --> 01:03:59,099
<font color="#E5E5E5">of family</font><font color="#CCCCCC"> of approximations</font><font color="#E5E5E5"> and that can</font>

1764
01:03:59,099 --> 01:04:00,989
be<font color="#E5E5E5"> hard to do but in when you do that</font>

1765
01:04:00,989 --> 01:04:03,779
you can build very flexible and powerful

1766
01:04:03,779 --> 01:04:05,819
latent variable models of images for

1767
01:04:05,819 --> 01:04:08,180
example<font color="#CCCCCC"> and</font><font color="#E5E5E5"> this is a model called</font><font color="#CCCCCC"> draw</font>

1768
01:04:08,180 --> 01:04:11,459
so we'll just<font color="#E5E5E5"> quickly look at this one</font>

1769
01:04:11,459 --> 01:04:12,989
but<font color="#CCCCCC"> again there</font><font color="#E5E5E5"> are lots of different</font>

1770
01:04:12,989 --> 01:04:14,400
dimensions<font color="#CCCCCC"> for</font><font color="#E5E5E5"> which you can build a</font>

1771
01:04:14,400 --> 01:04:16,949
model<font color="#E5E5E5"> you can choose linear models</font>

1772
01:04:16,949 --> 01:04:18,569
versus deep models you can choose

1773
01:04:18,569 --> 01:04:20,789
parametric models versus non parametric

1774
01:04:20,789 --> 01:04:23,069
models you can<font color="#E5E5E5"> choose continuous latent</font>

1775
01:04:23,069 --> 01:04:24,410
variables versus discretely

1776
01:04:24,410 --> 01:04:26,780
<font color="#E5E5E5">variable and then</font><font color="#CCCCCC"> you can build sort</font><font color="#E5E5E5"> of</font>

1777
01:04:26,780 --> 01:04:27,980
lots of<font color="#CCCCCC"> different models in</font><font color="#E5E5E5"> different</font>

1778
01:04:27,980 --> 01:04:31,370
<font color="#CCCCCC">cases so maybe something you</font><font color="#E5E5E5"> haven't</font>

1779
01:04:31,370 --> 01:04:33,410
<font color="#E5E5E5">thought</font><font color="#CCCCCC"> of or seen before these deep</font>

1780
01:04:33,410 --> 01:04:35,690
<font color="#E5E5E5">nonparametric and discrete models so</font>

1781
01:04:35,690 --> 01:04:36,710
there are lots of models<font color="#CCCCCC"> in this</font><font color="#E5E5E5"> case</font>

1782
01:04:36,710 --> 01:04:39,980
<font color="#E5E5E5">one example</font><font color="#CCCCCC"> what they call cascaded</font>

1783
01:04:39,980 --> 01:04:42,050
Indian buffet processes which are<font color="#CCCCCC"> now a</font>

1784
01:04:42,050 --> 01:04:44,510
sequence<font color="#CCCCCC"> basically of discrete infinite</font>

1785
01:04:44,510 --> 01:04:46,820
<font color="#E5E5E5">dimensional distributions of a binary</font>

1786
01:04:46,820 --> 01:04:48,680
<font color="#E5E5E5">object or if you've heard of a something</font>

1787
01:04:48,680 --> 01:04:50,660
<font color="#E5E5E5">called Jewish</font><font color="#CCCCCC"> lay process then you can</font>

1788
01:04:50,660 --> 01:04:52,340
<font color="#E5E5E5">build a hierarchical darshan a process</font>

1789
01:04:52,340 --> 01:04:54,470
which<font color="#E5E5E5"> is the nonparametric extension of</font>

1790
01:04:54,470 --> 01:04:56,720
a model<font color="#CCCCCC"> call Lda</font><font color="#E5E5E5"> that lives in</font><font color="#CCCCCC"> that</font>

1791
01:04:56,720 --> 01:04:58,820
corner or<font color="#CCCCCC"> the Mun we are actually going</font>

1792
01:04:58,820 --> 01:05:01,040
<font color="#E5E5E5">to look at today are deep parametric and</font>

1793
01:05:01,040 --> 01:05:03,110
continuous latent variables<font color="#E5E5E5"> nonlinear</font>

1794
01:05:03,110 --> 01:05:05,060
factor analysis<font color="#CCCCCC"> nonlinear Gaussian</font>

1795
01:05:05,060 --> 01:05:07,010
belief networks<font color="#E5E5E5"> and all these deep</font>

1796
01:05:07,010 --> 01:05:09,530
latent<font color="#CCCCCC"> gaussian models like</font><font color="#E5E5E5"> v AE</font><font color="#CCCCCC"> the v</font>

1797
01:05:09,530 --> 01:05:11,090
AE algorithm<font color="#E5E5E5"> in draw so we're going to</font>

1798
01:05:11,090 --> 01:05:13,100
look<font color="#E5E5E5"> at that but lots of other models to</font>

1799
01:05:13,100 --> 01:05:15,460
look<font color="#E5E5E5"> at and</font><font color="#CCCCCC"> I</font><font color="#E5E5E5"> just want</font><font color="#CCCCCC"> to highlight</font>

1800
01:05:15,460 --> 01:05:17,270
<font color="#E5E5E5">separately from the latent variable</font>

1801
01:05:17,270 --> 01:05:18,980
<font color="#CCCCCC">models which I just described to these</font>

1802
01:05:18,980 --> 01:05:21,380
implicit models implicit models are

1803
01:05:21,380 --> 01:05:24,770
<font color="#E5E5E5">simulators they transform an unobserved</font>

1804
01:05:24,770 --> 01:05:27,740
<font color="#CCCCCC">source of noise into</font><font color="#E5E5E5"> data using a</font>

1805
01:05:27,740 --> 01:05:30,100
<font color="#E5E5E5">parametrized function f so</font><font color="#CCCCCC"> we saw this</font>

1806
01:05:30,100 --> 01:05:32,390
picture<font color="#CCCCCC"> before when we looked at</font><font color="#E5E5E5"> their</font>

1807
01:05:32,390 --> 01:05:34,130
path wise<font color="#CCCCCC"> gradients in</font><font color="#E5E5E5"> the pathways</font>

1808
01:05:34,130 --> 01:05:37,010
estimation this is exactly<font color="#E5E5E5"> of that form</font>

1809
01:05:37,010 --> 01:05:39,230
we choose some source of<font color="#E5E5E5"> noise and then</font>

1810
01:05:39,230 --> 01:05:40,820
we choose a path<font color="#E5E5E5"> through which</font><font color="#CCCCCC"> to</font>

1811
01:05:40,820 --> 01:05:42,980
transform<font color="#E5E5E5"> that noise</font><font color="#CCCCCC"> and because we will</font>

1812
01:05:42,980 --> 01:05:45,110
<font color="#E5E5E5">know F we're going</font><font color="#CCCCCC"> to</font><font color="#E5E5E5"> manipulate F and</font>

1813
01:05:45,110 --> 01:05:46,820
use the knowledge<font color="#E5E5E5"> that we have to learn</font>

1814
01:05:46,820 --> 01:05:48,260
its parameters<font color="#E5E5E5"> and learn a generative</font>

1815
01:05:48,260 --> 01:05:50,420
model<font color="#CCCCCC"> in</font><font color="#E5E5E5"> this way and again we're going</font>

1816
01:05:50,420 --> 01:05:52,280
<font color="#CCCCCC">to</font><font color="#E5E5E5"> use the change of</font><font color="#CCCCCC"> variables will be</font>

1817
01:05:52,280 --> 01:05:54,680
the central quantity that<font color="#CCCCCC"> will exploit</font>

1818
01:05:54,680 --> 01:05:58,220
<font color="#CCCCCC">when we do this</font><font color="#E5E5E5"> and the model that most</font>

1819
01:05:58,220 --> 01:05:59,960
people see today is this<font color="#CCCCCC"> one</font><font color="#E5E5E5"> here</font><font color="#CCCCCC"> which</font>

1820
01:05:59,960 --> 01:06:03,710
<font color="#CCCCCC">is</font><font color="#E5E5E5"> the generator</font><font color="#CCCCCC"> network from DC</font><font color="#E5E5E5"> Gann</font>

1821
01:06:03,710 --> 01:06:06,290
and which just<font color="#CCCCCC"> simply starts is choosing</font>

1822
01:06:06,290 --> 01:06:07,940
<font color="#CCCCCC">a Gaussian latent variable and then you</font>

1823
01:06:07,940 --> 01:06:10,040
<font color="#CCCCCC">create a function</font><font color="#E5E5E5"> f which</font><font color="#CCCCCC"> is</font><font color="#E5E5E5"> this deep</font>

1824
01:06:10,040 --> 01:06:12,650
continent which actually<font color="#E5E5E5"> grows art and</font>

1825
01:06:12,650 --> 01:06:15,440
image but the function f can literally

1826
01:06:15,440 --> 01:06:17,540
be anything can be a linear<font color="#CCCCCC"> function can</font>

1827
01:06:17,540 --> 01:06:18,860
<font color="#CCCCCC">be a deep neural network can be</font><font color="#E5E5E5"> a</font>

1828
01:06:18,860 --> 01:06:22,010
recurrent model like<font color="#E5E5E5"> in</font><font color="#CCCCCC"> El STM</font><font color="#E5E5E5"> it can be</font>

1829
01:06:22,010 --> 01:06:23,570
a non<font color="#E5E5E5"> parametric function like a</font>

1830
01:06:23,570 --> 01:06:26,240
sequence of Gaussian processes<font color="#E5E5E5"> lots of</font>

1831
01:06:26,240 --> 01:06:28,820
different<font color="#E5E5E5"> things</font><font color="#CCCCCC"> he can</font><font color="#E5E5E5"> use implicit</font>

1832
01:06:28,820 --> 01:06:30,710
<font color="#E5E5E5">models have different</font><font color="#CCCCCC"> properties</font><font color="#E5E5E5"> and</font>

1833
01:06:30,710 --> 01:06:32,840
<font color="#CCCCCC">some they are also easy to sample and</font>

1834
01:06:32,840 --> 01:06:34,250
easy to specify because<font color="#E5E5E5"> you can just</font>

1835
01:06:34,250 --> 01:06:36,500
<font color="#E5E5E5">write out this function f it's very easy</font>

1836
01:06:36,500 --> 01:06:38,210
to<font color="#E5E5E5"> compute the expectations we</font>

1837
01:06:38,210 --> 01:06:39,109
because all you<font color="#CCCCCC"> need to do is just</font>

1838
01:06:39,109 --> 01:06:41,390
sample<font color="#E5E5E5"> from the noise and generate from</font>

1839
01:06:41,390 --> 01:06:43,339
the function<font color="#E5E5E5"> and then you basically can</font>

1840
01:06:43,339 --> 01:06:46,520
take averages<font color="#CCCCCC"> over</font><font color="#E5E5E5"> the samples and it's</font>

1841
01:06:46,520 --> 01:06:48,680
very<font color="#E5E5E5"> easy to exploit</font><font color="#CCCCCC"> large scale</font>

1842
01:06:48,680 --> 01:06:50,180
classifiers<font color="#E5E5E5"> in confidence when you</font>

1843
01:06:50,180 --> 01:06:52,640
design those functions<font color="#E5E5E5"> but if you had</font>

1844
01:06:52,640 --> 01:06:55,580
any constraints like these functions if

1845
01:06:55,580 --> 01:06:57,080
<font color="#CCCCCC">they needed</font><font color="#E5E5E5"> to be invertible if there</font>

1846
01:06:57,080 --> 01:06:58,430
were other kinds<font color="#E5E5E5"> of constraints you</font>

1847
01:06:58,430 --> 01:07:00,500
needed to do<font color="#E5E5E5"> then an optimization can</font><font color="#CCCCCC"> be</font>

1848
01:07:00,500 --> 01:07:02,480
very difficult<font color="#E5E5E5"> and the invertibility may</font>

1849
01:07:02,480 --> 01:07:05,330
be hard to maintain<font color="#E5E5E5"> there isn't this</font>

1850
01:07:05,330 --> 01:07:07,010
likelihood<font color="#E5E5E5"> which seems to</font><font color="#CCCCCC"> be an</font>

1851
01:07:07,010 --> 01:07:08,359
advantage<font color="#E5E5E5"> but is sometimes</font><font color="#CCCCCC"> a</font>

1852
01:07:08,359 --> 01:07:10,369
disadvantage because<font color="#E5E5E5"> not</font><font color="#CCCCCC"> having a</font>

1853
01:07:10,369 --> 01:07:12,440
<font color="#E5E5E5">likelihood model is what causes you to</font>

1854
01:07:12,440 --> 01:07:14,510
be<font color="#E5E5E5"> unstable during optimization and to</font>

1855
01:07:14,510 --> 01:07:15,619
not learn the<font color="#E5E5E5"> correct probability</font>

1856
01:07:15,619 --> 01:07:19,369
<font color="#E5E5E5">density</font><font color="#CCCCCC"> but</font><font color="#E5E5E5"> the main</font><font color="#CCCCCC"> reason is</font><font color="#E5E5E5"> that you</font>

1857
01:07:19,369 --> 01:07:21,560
can't extend<font color="#E5E5E5"> to generic data types if</font>

1858
01:07:21,560 --> 01:07:23,270
your<font color="#CCCCCC"> function is continuous the data you</font>

1859
01:07:23,270 --> 01:07:25,160
generate is continuous if your function

1860
01:07:25,160 --> 01:07:26,810
<font color="#E5E5E5">is discrete you will generate discrete</font>

1861
01:07:26,810 --> 01:07:29,270
data but<font color="#CCCCCC"> well you won't be able to</font>

1862
01:07:29,270 --> 01:07:31,550
<font color="#E5E5E5">handle discrete data and it's very hard</font>

1863
01:07:31,550 --> 01:07:33,380
<font color="#E5E5E5">to compute the marginalization and your</font>

1864
01:07:33,380 --> 01:07:35,330
model scoring<font color="#E5E5E5"> in this case but again</font>

1865
01:07:35,330 --> 01:07:37,940
<font color="#CCCCCC">lots of different things</font><font color="#E5E5E5"> to consider</font><font color="#CCCCCC"> the</font>

1866
01:07:37,940 --> 01:07:39,380
one<font color="#CCCCCC"> we are going</font><font color="#E5E5E5"> to look at on this time</font>

1867
01:07:39,380 --> 01:07:41,060
we're<font color="#E5E5E5"> going</font><font color="#CCCCCC"> to look at functions that</font>

1868
01:07:41,060 --> 01:07:43,280
operate in discrete time<font color="#E5E5E5"> but you could</font>

1869
01:07:43,280 --> 01:07:44,869
also<font color="#CCCCCC"> easily look at</font><font color="#E5E5E5"> these kind of</font>

1870
01:07:44,869 --> 01:07:46,940
implicit models which are diffusions

1871
01:07:46,940 --> 01:07:48,920
based in continuous time<font color="#E5E5E5"> so we're going</font>

1872
01:07:48,920 --> 01:07:50,450
to<font color="#E5E5E5"> look at one line sampling</font>

1873
01:07:50,450 --> 01:07:51,890
transformations which we looked at<font color="#E5E5E5"> in</font>

1874
01:07:51,890 --> 01:07:54,710
the<font color="#CCCCCC"> earlier trick normalizing flows fits</font>

1875
01:07:54,710 --> 01:07:56,450
into this which<font color="#CCCCCC"> we won't</font><font color="#E5E5E5"> discuss today</font>

1876
01:07:56,450 --> 01:07:59,210
<font color="#E5E5E5">and generator networks in</font><font color="#CCCCCC"> ganz and</font>

1877
01:07:59,210 --> 01:08:00,609
volume and non volume preserving

1878
01:08:00,609 --> 01:08:02,480
optimizations but you've already seen

1879
01:08:02,480 --> 01:08:03,920
<font color="#CCCCCC">some</font><font color="#E5E5E5"> of these if you looked at the</font>

1880
01:08:03,920 --> 01:08:05,839
launch of<font color="#CCCCCC"> is de or Hamiltonian</font>

1881
01:08:05,839 --> 01:08:09,290
<font color="#CCCCCC">Montecarlo or</font><font color="#E5E5E5"> you know simple physical</font>

1882
01:08:09,290 --> 01:08:12,560
sarcastic differential equations<font color="#CCCCCC"> okay</font><font color="#E5E5E5"> I</font>

1883
01:08:12,560 --> 01:08:14,230
want<font color="#E5E5E5"> to talk about influence in</font>

1884
01:08:14,230 --> 01:08:16,850
<font color="#E5E5E5">prescribe models so these are models</font>

1885
01:08:16,850 --> 01:08:19,700
with latent variable with the likelihood

1886
01:08:19,700 --> 01:08:23,149
<font color="#CCCCCC">function</font><font color="#E5E5E5"> so the model evidence are the</font>

1887
01:08:23,149 --> 01:08:24,770
<font color="#E5E5E5">marginal likelihood or</font><font color="#CCCCCC"> the partition</font>

1888
01:08:24,770 --> 01:08:26,839
function<font color="#E5E5E5"> is the key</font><font color="#CCCCCC"> quantity we</font><font color="#E5E5E5"> will be</font>

1889
01:08:26,839 --> 01:08:28,250
<font color="#E5E5E5">interested in and</font><font color="#CCCCCC"> that means</font><font color="#E5E5E5"> we want to</font>

1890
01:08:28,250 --> 01:08:31,430
integrate out<font color="#E5E5E5"> Z to know P of x1 and the</font>

1891
01:08:31,430 --> 01:08:33,500
<font color="#E5E5E5">learning principle of knowing the</font>

1892
01:08:33,500 --> 01:08:35,540
<font color="#CCCCCC">bayesian model</font><font color="#E5E5E5"> evidence is that all we</font>

1893
01:08:35,540 --> 01:08:37,100
want<font color="#CCCCCC"> to do is at every</font><font color="#E5E5E5"> step of</font>

1894
01:08:37,100 --> 01:08:39,350
optimization<font color="#E5E5E5"> every time we look at our</font>

1895
01:08:39,350 --> 01:08:41,089
data we<font color="#E5E5E5"> want to make sure that this</font>

1896
01:08:41,089 --> 01:08:43,700
model evidence becomes maximized we want

1897
01:08:43,700 --> 01:08:45,649
the maximum<font color="#E5E5E5"> evidence the highest</font>

1898
01:08:45,649 --> 01:08:47,359
probability of data that we<font color="#E5E5E5"> can possibly</font>

1899
01:08:47,359 --> 01:08:49,969
<font color="#E5E5E5">have</font><font color="#CCCCCC"> and that's why optimizing the model</font>

1900
01:08:49,969 --> 01:08:51,770
evidence is the principle<font color="#E5E5E5"> for</font><font color="#CCCCCC"> learning</font>

1901
01:08:51,770 --> 01:08:54,439
and so that's the principle<font color="#E5E5E5"> we if we</font>

1902
01:08:54,439 --> 01:08:55,850
have these latent variables we<font color="#E5E5E5"> want to</font>

1903
01:08:55,850 --> 01:08:58,310
integrate said to know P of X and<font color="#E5E5E5"> once</font>

1904
01:08:58,310 --> 01:08:59,839
we<font color="#E5E5E5"> know P of X we're</font><font color="#CCCCCC"> going</font><font color="#E5E5E5"> to maximize</font>

1905
01:08:59,839 --> 01:09:03,439
<font color="#E5E5E5">it to try and</font><font color="#CCCCCC"> get to</font><font color="#E5E5E5"> learn the best</font>

1906
01:09:03,439 --> 01:09:05,839
model possible<font color="#E5E5E5"> of course there's an</font>

1907
01:09:05,839 --> 01:09:07,490
integral which<font color="#E5E5E5"> is very difficult</font><font color="#CCCCCC"> to</font>

1908
01:09:07,490 --> 01:09:09,260
compute so maybe some<font color="#CCCCCC"> of our tricks will</font>

1909
01:09:09,260 --> 01:09:12,500
be useful here<font color="#CCCCCC"> and the basic idea is</font><font color="#E5E5E5"> to</font>

1910
01:09:12,500 --> 01:09:14,779
<font color="#E5E5E5">transform this integral which</font><font color="#CCCCCC"> is an</font>

1911
01:09:14,779 --> 01:09:17,120
integral of an expectation of some

1912
01:09:17,120 --> 01:09:19,010
distribution<font color="#CCCCCC"> into an expectation of</font><font color="#E5E5E5"> a</font>

1913
01:09:19,010 --> 01:09:21,260
<font color="#CCCCCC">distribution that we choose and once we</font>

1914
01:09:21,260 --> 01:09:23,000
can choose<font color="#E5E5E5"> that</font><font color="#CCCCCC"> distribution then we can</font>

1915
01:09:23,000 --> 01:09:24,830
be more<font color="#CCCCCC"> flexible all right so we've seen</font>

1916
01:09:24,830 --> 01:09:27,020
that<font color="#CCCCCC"> before we use the identity trick</font>

1917
01:09:27,020 --> 01:09:29,479
<font color="#CCCCCC">and Jensen's inequality</font><font color="#E5E5E5"> to derive this</font>

1918
01:09:29,479 --> 01:09:31,250
lower bound<font color="#CCCCCC"> on the marginal likelihood</font>

1919
01:09:31,250 --> 01:09:33,290
<font color="#E5E5E5">and as I</font><font color="#CCCCCC"> said this marginal likelihood</font>

1920
01:09:33,290 --> 01:09:35,479
is called the<font color="#E5E5E5"> variational free-energy</font>

1921
01:09:35,479 --> 01:09:37,549
<font color="#E5E5E5">and it's the basis of what we will call</font>

1922
01:09:37,549 --> 01:09:39,319
variational inference which is one<font color="#E5E5E5"> of</font>

1923
01:09:39,319 --> 01:09:41,330
the most popular<font color="#E5E5E5"> methods for doing</font>

1924
01:09:41,330 --> 01:09:43,370
inference<font color="#CCCCCC"> in latent variable models and</font>

1925
01:09:43,370 --> 01:09:45,260
it<font color="#E5E5E5"> is called the variational free-energy</font>

1926
01:09:45,260 --> 01:09:47,600
because I chose to<font color="#E5E5E5"> introduce this</font>

1927
01:09:47,600 --> 01:09:49,549
distribution<font color="#CCCCCC"> Q and I'm free to choose</font>

1928
01:09:49,549 --> 01:09:52,220
<font color="#CCCCCC">the</font><font color="#E5E5E5"> distribution Q that allows me to</font>

1929
01:09:52,220 --> 01:09:55,970
best match the model likelihood<font color="#E5E5E5"> and so</font>

1930
01:09:55,970 --> 01:09:57,830
what I'm<font color="#CCCCCC"> gonna</font><font color="#E5E5E5"> have</font><font color="#CCCCCC"> to do here is learn</font>

1931
01:09:57,830 --> 01:10:00,050
something about Q but I also have<font color="#E5E5E5"> to</font>

1932
01:10:00,050 --> 01:10:02,840
learn<font color="#CCCCCC"> what</font><font color="#E5E5E5"> about P where's the model the</font>

1933
01:10:02,840 --> 01:10:04,220
model<font color="#CCCCCC"> that I'm actually interested in</font>

1934
01:10:04,220 --> 01:10:06,650
and as I said it's sometimes called<font color="#E5E5E5"> the</font>

1935
01:10:06,650 --> 01:10:08,330
evidence lower bound<font color="#E5E5E5"> because it is a</font>

1936
01:10:08,330 --> 01:10:10,400
bound on the model or<font color="#E5E5E5"> the data</font><font color="#CCCCCC"> evidence</font>

1937
01:10:10,400 --> 01:10:13,070
<font color="#E5E5E5">and I said we need to choose true in the</font>

1938
01:10:13,070 --> 01:10:14,690
two tricks we used<font color="#E5E5E5"> here with identity</font>

1939
01:10:14,690 --> 01:10:18,260
trick<font color="#E5E5E5"> and the bounding trick so this</font>

1940
01:10:18,260 --> 01:10:20,180
<font color="#E5E5E5">just lets me allow you to explain in</font>

1941
01:10:20,180 --> 01:10:22,430
<font color="#E5E5E5">general what</font><font color="#CCCCCC"> the variational principle</font>

1942
01:10:22,430 --> 01:10:24,920
is a variational principle is just a

1943
01:10:24,920 --> 01:10:27,010
<font color="#E5E5E5">family of methods that allows you to</font>

1944
01:10:27,010 --> 01:10:29,150
approximate something difficult<font color="#E5E5E5"> with</font>

1945
01:10:29,150 --> 01:10:31,160
something simple<font color="#E5E5E5"> right and by being</font>

1946
01:10:31,160 --> 01:10:33,440
variational that word variation you<font color="#CCCCCC"> can</font>

1947
01:10:33,440 --> 01:10:35,150
substitute it with<font color="#E5E5E5"> the word functional</font>

1948
01:10:35,150 --> 01:10:37,430
<font color="#E5E5E5">it is</font><font color="#CCCCCC"> an optimization in a functional</font>

1949
01:10:37,430 --> 01:10:39,650
space<font color="#E5E5E5"> right so here some complicated</font>

1950
01:10:39,650 --> 01:10:41,840
distribution this distribution is the

1951
01:10:41,840 --> 01:10:43,330
density<font color="#CCCCCC"> and</font><font color="#E5E5E5"> the density is a function</font>

1952
01:10:43,330 --> 01:10:46,070
<font color="#E5E5E5">special kind of function and so this is</font>

1953
01:10:46,070 --> 01:10:47,690
a variational problem<font color="#E5E5E5"> because I'm going</font>

1954
01:10:47,690 --> 01:10:49,850
<font color="#E5E5E5">to choose an approximating class of</font>

1955
01:10:49,850 --> 01:10:53,000
functions<font color="#E5E5E5"> and it is variational because</font>

1956
01:10:53,000 --> 01:10:54,260
I'm<font color="#E5E5E5"> going to try and find the</font><font color="#CCCCCC"> best</font>

1957
01:10:54,260 --> 01:10:56,630
function<font color="#E5E5E5"> to match that function and you</font>

1958
01:10:56,630 --> 01:10:58,400
<font color="#CCCCCC">can do this directly using functional</font>

1959
01:10:58,400 --> 01:11:00,290
gradient descent which is what we call

1960
01:11:00,290 --> 01:11:02,840
the variational calculus<font color="#CCCCCC"> but we're going</font>

1961
01:11:02,840 --> 01:11:03,950
<font color="#E5E5E5">to try and</font><font color="#CCCCCC"> avoid</font><font color="#E5E5E5"> the variational</font>

1962
01:11:03,950 --> 01:11:05,570
calculus because<font color="#CCCCCC"> we can actually put</font>

1963
01:11:05,570 --> 01:11:07,460
ramit<font color="#CCCCCC"> rise these distributions through</font>

1964
01:11:07,460 --> 01:11:09,590
other parameters<font color="#E5E5E5"> file and that</font><font color="#CCCCCC"> means we</font>

1965
01:11:09,590 --> 01:11:11,480
can do parametric optimization or

1966
01:11:11,480 --> 01:11:13,699
standard<font color="#CCCCCC"> optimization and the parameters</font>

1967
01:11:13,699 --> 01:11:16,429
Phi<font color="#E5E5E5"> right so this</font><font color="#CCCCCC"> is in</font><font color="#E5E5E5"> general which</font><font color="#CCCCCC"> is</font>

1968
01:11:16,429 --> 01:11:17,900
why even<font color="#CCCCCC"> though this is called</font>

1969
01:11:17,900 --> 01:11:20,389
variational inference<font color="#E5E5E5"> there are lots of</font>

1970
01:11:20,389 --> 01:11:22,040
things which are variational methods in

1971
01:11:22,040 --> 01:11:23,810
fact reinforcement learning is itself<font color="#CCCCCC"> a</font>

1972
01:11:23,810 --> 01:11:26,989
variational method because the policies

1973
01:11:26,989 --> 01:11:28,909
<font color="#E5E5E5">PI are these functions which are doing a</font>

1974
01:11:28,909 --> 01:11:31,070
functional descent over<font color="#E5E5E5"> or you know</font>

1975
01:11:31,070 --> 01:11:32,750
<font color="#CCCCCC">they're many all of information theory</font>

1976
01:11:32,750 --> 01:11:34,219
fits into this kind of<font color="#E5E5E5"> thing and</font>

1977
01:11:34,219 --> 01:11:35,960
building lots of other bounds<font color="#E5E5E5"> lets you</font>

1978
01:11:35,960 --> 01:11:38,210
build variational<font color="#CCCCCC"> methods so just be</font>

1979
01:11:38,210 --> 01:11:39,530
flexible<font color="#CCCCCC"> there are lots of very few</font>

1980
01:11:39,530 --> 01:11:42,020
methods<font color="#CCCCCC"> that exist out there and we</font><font color="#E5E5E5"> want</font>

1981
01:11:42,020 --> 01:11:43,880
to fit these variational parameters Phi

1982
01:11:43,880 --> 01:11:46,190
so in very tional inference there are

1983
01:11:46,190 --> 01:11:48,349
two terms there's this first term which

1984
01:11:48,349 --> 01:11:50,570
is<font color="#CCCCCC"> a</font><font color="#E5E5E5"> log</font><font color="#CCCCCC"> likelihood of</font><font color="#E5E5E5"> log P of x given</font>

1985
01:11:50,570 --> 01:11:52,849
<font color="#E5E5E5">that this</font><font color="#CCCCCC"> is the</font><font color="#E5E5E5"> model</font><font color="#CCCCCC"> you</font><font color="#E5E5E5"> get to choose</font>

1986
01:11:52,849 --> 01:11:54,679
so this is for<font color="#E5E5E5"> example a Gaussian</font>

1987
01:11:54,679 --> 01:11:56,810
<font color="#E5E5E5">distribution</font><font color="#CCCCCC"> in which case this</font><font color="#E5E5E5"> is an l2</font>

1988
01:11:56,810 --> 01:11:59,060
loss and then you get a penalty term

1989
01:11:59,060 --> 01:12:00,860
which is going<font color="#E5E5E5"> to say that this</font>

1990
01:12:00,860 --> 01:12:02,960
<font color="#CCCCCC">distribution Q that I have chosen</font><font color="#E5E5E5"> should</font>

1991
01:12:02,960 --> 01:12:05,840
be something<font color="#E5E5E5"> close to my prior</font><font color="#CCCCCC"> PI and</font>

1992
01:12:05,840 --> 01:12:07,760
this<font color="#E5E5E5"> is</font><font color="#CCCCCC"> good because this</font><font color="#E5E5E5"> is</font><font color="#CCCCCC"> a penalized</font>

1993
01:12:07,760 --> 01:12:10,460
way of doing<font color="#E5E5E5"> learning and we always want</font>

1994
01:12:10,460 --> 01:12:12,860
to have regularizes and the other thing

1995
01:12:12,860 --> 01:12:14,270
that's<font color="#E5E5E5"> useful is</font><font color="#CCCCCC"> that you didn't need to</font>

1996
01:12:14,270 --> 01:12:16,579
design<font color="#E5E5E5"> the regularizer</font><font color="#CCCCCC"> just by</font><font color="#E5E5E5"> applying</font>

1997
01:12:16,579 --> 01:12:18,530
the variational principle and<font color="#CCCCCC"> applying</font>

1998
01:12:18,530 --> 01:12:20,409
<font color="#CCCCCC">these two tricks I likelihood a</font>

1999
01:12:20,409 --> 01:12:22,579
reconstruction term appeared but also

2000
01:12:22,579 --> 01:12:24,949
<font color="#E5E5E5">the correct and you know</font>

2001
01:12:24,949 --> 01:12:27,349
regularization<font color="#E5E5E5"> appeared and this</font>

2002
01:12:27,349 --> 01:12:29,420
distribution Q that we introduced which

2003
01:12:29,420 --> 01:12:31,400
was when we did<font color="#E5E5E5"> importance sampling was</font>

2004
01:12:31,400 --> 01:12:33,230
just some<font color="#CCCCCC"> generic distribution was a</font>

2005
01:12:33,230 --> 01:12:35,659
proposal<font color="#CCCCCC"> now this distribution Q has a</font>

2006
01:12:35,659 --> 01:12:37,730
meaning it is<font color="#E5E5E5"> a posterior distribution</font>

2007
01:12:37,730 --> 01:12:40,520
<font color="#E5E5E5">it is something</font><font color="#CCCCCC"> that tries</font><font color="#E5E5E5"> to invert the</font>

2008
01:12:40,520 --> 01:12:41,960
probability it tries<font color="#CCCCCC"> to give you the</font>

2009
01:12:41,960 --> 01:12:45,409
probability<font color="#CCCCCC"> of Z</font><font color="#E5E5E5"> given X so in all the</font>

2010
01:12:45,409 --> 01:12:47,119
slides that's<font color="#E5E5E5"> here before the notation</font>

2011
01:12:47,119 --> 01:12:48,079
<font color="#E5E5E5">is a bit sloppy</font>

2012
01:12:48,079 --> 01:12:50,449
you should read Zed given X here<font color="#CCCCCC"> and</font>

2013
01:12:50,449 --> 01:12:52,730
I'll try<font color="#E5E5E5"> and fix them when I put that on</font>

2014
01:12:52,730 --> 01:12:56,599
<font color="#E5E5E5">line so again Q of Z tries to match the</font>

2015
01:12:56,599 --> 01:13:00,980
true posterior<font color="#E5E5E5"> P of Z given</font><font color="#CCCCCC"> Y and P of</font>

2016
01:13:00,980 --> 01:13:02,840
knowing P of Z<font color="#CCCCCC"> given Y</font><font color="#E5E5E5"> is one of the</font>

2017
01:13:02,840 --> 01:13:04,940
<font color="#E5E5E5">useful information quantities and then</font>

2018
01:13:04,940 --> 01:13:06,409
you have a reconstruction cost which I

2019
01:13:06,409 --> 01:13:08,659
talked about<font color="#E5E5E5"> and then a natural</font><font color="#CCCCCC"> penalty</font>

2020
01:13:08,659 --> 01:13:11,170
<font color="#E5E5E5">which is the</font><font color="#CCCCCC"> mechanism for Occam's razor</font>

2021
01:13:11,170 --> 01:13:13,670
<font color="#E5E5E5">now just some comments on this</font>

2022
01:13:13,670 --> 01:13:16,699
distribution<font color="#E5E5E5"> Q by having this problem</font><font color="#CCCCCC"> Y</font>

2023
01:13:16,699 --> 01:13:19,430
was an ugly integral is now enough to

2024
01:13:19,430 --> 01:13:22,160
problem because we<font color="#E5E5E5"> can just</font><font color="#CCCCCC"> how use</font><font color="#E5E5E5"> this</font>

2025
01:13:22,160 --> 01:13:24,020
is the last function<font color="#E5E5E5"> and there are two</font>

2026
01:13:24,020 --> 01:13:25,370
<font color="#E5E5E5">types of parameters</font><font color="#CCCCCC"> there are parameters</font>

2027
01:13:25,370 --> 01:13:27,680
<font color="#E5E5E5">theta which live</font><font color="#CCCCCC"> in P and there are</font>

2028
01:13:27,680 --> 01:13:29,930
<font color="#E5E5E5">parameters Phi which live in Q and these</font>

2029
01:13:29,930 --> 01:13:31,340
are the two things<font color="#E5E5E5"> you need</font><font color="#CCCCCC"> to</font><font color="#E5E5E5"> optimize</font>

2030
01:13:31,340 --> 01:13:34,010
<font color="#E5E5E5">and so this is</font><font color="#CCCCCC"> now much easier</font><font color="#E5E5E5"> problem</font>

2031
01:13:34,010 --> 01:13:36,950
<font color="#CCCCCC">to do</font><font color="#E5E5E5"> as I said</font><font color="#CCCCCC"> I've been</font><font color="#E5E5E5"> writing Q of Z</font>

2032
01:13:36,950 --> 01:13:39,230
but<font color="#E5E5E5"> it actually depends on the data and</font>

2033
01:13:39,230 --> 01:13:41,330
<font color="#E5E5E5">sometimes I'm</font><font color="#CCCCCC"> using</font><font color="#E5E5E5"> X and sometimes I'm</font>

2034
01:13:41,330 --> 01:13:45,080
using<font color="#E5E5E5"> Y sorry for that and</font><font color="#CCCCCC"> you have easy</font>

2035
01:13:45,080 --> 01:13:46,940
convergence assessments because<font color="#E5E5E5"> this is</font>

2036
01:13:46,940 --> 01:13:49,310
a bound<font color="#E5E5E5"> every time you do an improvement</font>

2037
01:13:49,310 --> 01:13:51,170
<font color="#CCCCCC">the bound can only go in one</font><font color="#E5E5E5"> direction</font>

2038
01:13:51,170 --> 01:13:54,560
it<font color="#E5E5E5"> must go up</font><font color="#CCCCCC"> and so you'll be able</font><font color="#E5E5E5"> to</font>

2039
01:13:54,560 --> 01:13:56,270
this<font color="#E5E5E5"> is how you can actually plot while</font>

2040
01:13:56,270 --> 01:13:58,460
you are testing this<font color="#E5E5E5"> and</font><font color="#CCCCCC"> then you</font><font color="#E5E5E5"> have</font>

2041
01:13:58,460 --> 01:14:04,520
<font color="#CCCCCC">purel</font><font color="#E5E5E5"> parameters of cubes in okay so the</font>

2042
01:14:04,520 --> 01:14:06,350
key thing<font color="#CCCCCC"> here I want</font><font color="#E5E5E5"> to just switch to</font>

2043
01:14:06,350 --> 01:14:08,390
<font color="#E5E5E5">talking</font><font color="#CCCCCC"> about</font><font color="#E5E5E5"> this distribution</font><font color="#CCCCCC"> Q of Z</font>

2044
01:14:08,390 --> 01:14:10,250
so cubes there was something<font color="#E5E5E5"> you had</font><font color="#CCCCCC"> to</font>

2045
01:14:10,250 --> 01:14:12,500
choose and as I said<font color="#CCCCCC"> it is something</font>

2046
01:14:12,500 --> 01:14:13,850
that's trying to<font color="#E5E5E5"> match the true</font>

2047
01:14:13,850 --> 01:14:15,770
posterior distribution which is<font color="#CCCCCC"> said</font>

2048
01:14:15,770 --> 01:14:18,560
<font color="#E5E5E5">given X this probability of</font><font color="#CCCCCC"> Z given X so</font>

2049
01:14:18,560 --> 01:14:20,090
what do real-world distributions

2050
01:14:20,090 --> 01:14:22,040
posterior distributions look like so

2051
01:14:22,040 --> 01:14:23,600
here's an example<font color="#CCCCCC"> that was made on</font><font color="#E5E5E5"> M</font>

2052
01:14:23,600 --> 01:14:25,820
<font color="#E5E5E5">NIST in two dimensional latent variables</font>

2053
01:14:25,820 --> 01:14:28,550
and by enumeration<font color="#CCCCCC"> I'm just going to</font>

2054
01:14:28,550 --> 01:14:30,080
show you what the real posterior

2055
01:14:30,080 --> 01:14:32,330
<font color="#E5E5E5">distribution for certain kinds</font><font color="#CCCCCC"> of simple</font>

2056
01:14:32,330 --> 01:14:34,340
models look like so here's a simple

2057
01:14:34,340 --> 01:14:36,200
<font color="#E5E5E5">model it</font><font color="#CCCCCC"> has two latent variables it's</font>

2058
01:14:36,200 --> 01:14:38,960
simple one layer<font color="#E5E5E5"> with a Gaussian output</font>

2059
01:14:38,960 --> 01:14:42,620
and the what is<font color="#CCCCCC"> the real distribution is</font>

2060
01:14:42,620 --> 01:14:44,060
the<font color="#E5E5E5"> gray thing so you just need to look</font>

2061
01:14:44,060 --> 01:14:45,740
at the gray<font color="#CCCCCC"> thing it's the same in each</font>

2062
01:14:45,740 --> 01:14:47,210
kernel<font color="#E5E5E5"> it's just a different zoom levels</font>

2063
01:14:47,210 --> 01:14:49,850
<font color="#E5E5E5">and what</font><font color="#CCCCCC"> are blue are samples from</font><font color="#E5E5E5"> this</font>

2064
01:14:49,850 --> 01:14:52,040
<font color="#CCCCCC">Q distribution which we sample from</font>

2065
01:14:52,040 --> 01:14:53,960
after training<font color="#E5E5E5"> this method so you can</font>

2066
01:14:53,960 --> 01:14:55,970
see in some<font color="#CCCCCC"> cases you can't see</font><font color="#E5E5E5"> because</font>

2067
01:14:55,970 --> 01:14:58,520
of the light<font color="#CCCCCC"> but</font><font color="#E5E5E5"> some cases</font><font color="#CCCCCC"> you</font><font color="#E5E5E5"> can do</font>

2068
01:14:58,520 --> 01:15:00,560
really<font color="#E5E5E5"> well the blue can overlap the</font>

2069
01:15:00,560 --> 01:15:02,480
white<font color="#E5E5E5"> really well you</font><font color="#CCCCCC"> can actually</font><font color="#E5E5E5"> learn</font>

2070
01:15:02,480 --> 01:15:04,580
<font color="#CCCCCC">the real distribution</font><font color="#E5E5E5"> and in fact you</font>

2071
01:15:04,580 --> 01:15:06,230
can<font color="#CCCCCC"> even learn</font><font color="#E5E5E5"> it when it has</font><font color="#CCCCCC"> the strong</font>

2072
01:15:06,230 --> 01:15:08,300
correlation structure<font color="#CCCCCC"> you can do that</font>

2073
01:15:08,300 --> 01:15:10,730
but this is<font color="#CCCCCC"> the model I think with the</font>

2074
01:15:10,730 --> 01:15:13,310
sigmoid non-linearity<font color="#E5E5E5"> in</font><font color="#CCCCCC"> there in the</font>

2075
01:15:13,310 --> 01:15:15,290
network<font color="#CCCCCC"> but</font><font color="#E5E5E5"> when you start doing</font><font color="#CCCCCC"> other</font>

2076
01:15:15,290 --> 01:15:17,510
kinds of<font color="#CCCCCC"> things you get weird blobs like</font>

2077
01:15:17,510 --> 01:15:20,330
this or you get this one has<font color="#CCCCCC"> at an</font><font color="#E5E5E5"> age</font>

2078
01:15:20,330 --> 01:15:22,640
<font color="#E5E5E5">it's a surface</font><font color="#CCCCCC"> that has a sharp thing</font>

2079
01:15:22,640 --> 01:15:24,860
like this<font color="#E5E5E5"> and then carries on or you</font>

2080
01:15:24,860 --> 01:15:26,570
have<font color="#E5E5E5"> something that looks</font><font color="#CCCCCC"> like this</font>

2081
01:15:26,570 --> 01:15:28,760
which has<font color="#E5E5E5"> a period</font><font color="#CCCCCC"> of</font><font color="#E5E5E5"> very high mass and</font>

2082
01:15:28,760 --> 01:15:31,940
then a very<font color="#E5E5E5"> long tail now if you were to</font>

2083
01:15:31,940 --> 01:15:32,900
choose a<font color="#E5E5E5"> queue</font>

2084
01:15:32,900 --> 01:15:34,699
as a<font color="#E5E5E5"> Gaussian</font><font color="#CCCCCC"> it won't be able to</font><font color="#E5E5E5"> learn</font>

2085
01:15:34,699 --> 01:15:36,469
this this this this<font color="#CCCCCC"> basically it's going</font>

2086
01:15:36,469 --> 01:15:39,650
to struggle<font color="#CCCCCC"> and you can sort</font><font color="#E5E5E5"> of hear it</font>

2087
01:15:39,650 --> 01:15:41,270
does well<font color="#E5E5E5"> because you can do</font><font color="#CCCCCC"> Gaussian</font>

2088
01:15:41,270 --> 01:15:43,880
<font color="#E5E5E5">ish things but it's cut off on</font><font color="#CCCCCC"> the other</font>

2089
01:15:43,880 --> 01:15:47,330
<font color="#E5E5E5">side and so real whopper</font><font color="#CCCCCC"> series even in</font>

2090
01:15:47,330 --> 01:15:49,250
simple<font color="#CCCCCC"> cases are complicated</font><font color="#E5E5E5"> and when we</font>

2091
01:15:49,250 --> 01:15:51,260
have high dimensional data<font color="#CCCCCC"> they're even</font>

2092
01:15:51,260 --> 01:15:52,820
<font color="#E5E5E5">more</font><font color="#CCCCCC"> complicated so then we need to have</font>

2093
01:15:52,820 --> 01:15:55,310
ways of designing<font color="#E5E5E5"> these</font><font color="#CCCCCC"> Q distributions</font>

2094
01:15:55,310 --> 01:15:57,469
that<font color="#E5E5E5"> are very flexible and efficient and</font>

2095
01:15:57,469 --> 01:15:59,150
<font color="#CCCCCC">that is one of the ongoing areas of</font>

2096
01:15:59,150 --> 01:16:02,330
research<font color="#CCCCCC"> and so you basically</font><font color="#E5E5E5"> have</font><font color="#CCCCCC"> a</font>

2097
01:16:02,330 --> 01:16:04,790
spectrum<font color="#E5E5E5"> of things to choose around this</font>

2098
01:16:04,790 --> 01:16:07,489
<font color="#E5E5E5">Q you</font><font color="#CCCCCC"> have on one side Q star</font><font color="#E5E5E5"> which is</font>

2099
01:16:07,489 --> 01:16:09,590
<font color="#E5E5E5">the optimal posterior distribution which</font>

2100
01:16:09,590 --> 01:16:11,570
<font color="#E5E5E5">is just the result of Bayes rule P of</font><font color="#CCCCCC"> X</font>

2101
01:16:11,570 --> 01:16:14,120
<font color="#E5E5E5">given Z times P</font><font color="#CCCCCC"> of Z you can never know</font>

2102
01:16:14,120 --> 01:16:15,710
this because<font color="#E5E5E5"> if</font><font color="#CCCCCC"> you knew the truth you</font>

2103
01:16:15,710 --> 01:16:18,080
wouldn't<font color="#CCCCCC"> do any of</font><font color="#E5E5E5"> this so this side you</font>

2104
01:16:18,080 --> 01:16:20,719
can't get to the exact opposite end is

2105
01:16:20,719 --> 01:16:22,070
where you choose<font color="#E5E5E5"> that everything is</font>

2106
01:16:22,070 --> 01:16:23,810
independent<font color="#CCCCCC"> and what we call the fully</font>

2107
01:16:23,810 --> 01:16:25,730
factorize this<font color="#E5E5E5"> is the least expressive</font>

2108
01:16:25,730 --> 01:16:27,949
way to design a<font color="#CCCCCC"> q function</font><font color="#E5E5E5"> but very</font>

2109
01:16:27,949 --> 01:16:29,750
<font color="#E5E5E5">popular so you can just do the product</font>

2110
01:16:29,750 --> 01:16:31,909
over<font color="#CCCCCC"> K dimensions of individual</font>

2111
01:16:31,909 --> 01:16:34,429
univariate gaussians for example right

2112
01:16:34,429 --> 01:16:36,050
<font color="#CCCCCC">and then in between</font><font color="#E5E5E5"> there's a lot of</font>

2113
01:16:36,050 --> 01:16:37,489
different things<font color="#E5E5E5"> so this is</font><font color="#CCCCCC"> where we</font>

2114
01:16:37,489 --> 01:16:40,820
want to<font color="#E5E5E5"> be able</font><font color="#CCCCCC"> to</font><font color="#E5E5E5"> build these deep rich</font>

2115
01:16:40,820 --> 01:16:42,920
distributions and<font color="#E5E5E5"> so what lives in</font>

2116
01:16:42,920 --> 01:16:44,120
between are things which are<font color="#E5E5E5"> called</font>

2117
01:16:44,120 --> 01:16:46,640
structured approximations<font color="#E5E5E5"> they introduce</font>

2118
01:16:46,640 --> 01:16:48,560
some<font color="#E5E5E5"> kind of dependency structures they</font>

2119
01:16:48,560 --> 01:16:50,810
can see Zed K is dependent<font color="#E5E5E5"> on some</font>

2120
01:16:50,810 --> 01:16:53,210
subset of<font color="#E5E5E5"> all others</font><font color="#CCCCCC"> ads in</font><font color="#E5E5E5"> the model</font>

2121
01:16:53,210 --> 01:16:55,070
and this is<font color="#E5E5E5"> where you can do a lot of</font>

2122
01:16:55,070 --> 01:16:56,390
<font color="#E5E5E5">things and</font><font color="#CCCCCC"> sometimes you'll hear people</font>

2123
01:16:56,390 --> 01:16:58,820
say structured mean<font color="#CCCCCC"> filled in the data</font>

2124
01:16:58,820 --> 01:17:01,640
and all the<font color="#E5E5E5"> times not so I thought we'd</font>

2125
01:17:01,640 --> 01:17:03,290
do a very simple<font color="#E5E5E5"> example so here's a</font>

2126
01:17:03,290 --> 01:17:05,300
<font color="#CCCCCC">model which has a</font><font color="#E5E5E5"> Gaussian latent</font>

2127
01:17:05,300 --> 01:17:08,239
variable said it has a likelihood

2128
01:17:08,239 --> 01:17:11,659
function which I'm leaving<font color="#E5E5E5"> generic but</font>

2129
01:17:11,659 --> 01:17:13,040
you can assume it's a Gaussian or

2130
01:17:13,040 --> 01:17:14,750
Bernoulli distribution<font color="#E5E5E5"> and then I'm</font>

2131
01:17:14,750 --> 01:17:16,460
choosing a<font color="#E5E5E5"> Q distribution which is</font><font color="#CCCCCC"> a</font>

2132
01:17:16,460 --> 01:17:18,530
<font color="#E5E5E5">product of univariate</font><font color="#CCCCCC"> gaussians said I</font>

2133
01:17:18,530 --> 01:17:21,560
given mu I<font color="#CCCCCC"> and Sigma I and so what we do</font>

2134
01:17:21,560 --> 01:17:23,090
is we<font color="#E5E5E5"> draw</font><font color="#CCCCCC"> it out a variational lower</font>

2135
01:17:23,090 --> 01:17:25,550
bound and then we would substitute<font color="#E5E5E5"> in</font>

2136
01:17:25,550 --> 01:17:28,190
<font color="#CCCCCC">here this care</font><font color="#E5E5E5"> because the products</font><font color="#CCCCCC"> over</font>

2137
01:17:28,190 --> 01:17:29,810
and<font color="#E5E5E5"> things actually becomes the sum of</font>

2138
01:17:29,810 --> 01:17:32,380
<font color="#CCCCCC">Cal's</font><font color="#E5E5E5"> for each of the individual</font><font color="#CCCCCC"> terms</font>

2139
01:17:32,380 --> 01:17:35,090
then you<font color="#E5E5E5"> can write out the KL between</font>

2140
01:17:35,090 --> 01:17:37,850
two gaussians<font color="#CCCCCC"> or the KL between any two</font>

2141
01:17:37,850 --> 01:17:39,530
<font color="#E5E5E5">exponential family distributions is</font>

2142
01:17:39,530 --> 01:17:41,570
actually known in closed<font color="#CCCCCC"> form</font><font color="#E5E5E5"> you can</font>

2143
01:17:41,570 --> 01:17:43,219
actually do this as an<font color="#E5E5E5"> exercise for</font>

2144
01:17:43,219 --> 01:17:45,860
<font color="#CCCCCC">yourself to derive the</font><font color="#E5E5E5"> kr between two</font>

2145
01:17:45,860 --> 01:17:46,520
<font color="#E5E5E5">distribution</font>

2146
01:17:46,520 --> 01:17:48,410
and you'll see it will<font color="#E5E5E5"> have a form like</font>

2147
01:17:48,410 --> 01:17:50,719
<font color="#CCCCCC">this it will be</font><font color="#E5E5E5"> a squared error</font>

2148
01:17:50,719 --> 01:17:53,750
reconstruction term and then we'll have

2149
01:17:53,750 --> 01:17:55,610
some<font color="#E5E5E5"> variance correction which</font><font color="#CCCCCC"> is based</font>

2150
01:17:55,610 --> 01:17:58,310
on<font color="#E5E5E5"> the on the log log</font><font color="#CCCCCC"> variance and then</font>

2151
01:17:58,310 --> 01:18:00,080
you can get<font color="#E5E5E5"> the log likelihood that you</font>

2152
01:18:00,080 --> 01:18:01,430
<font color="#E5E5E5">always expect so if this is a</font><font color="#CCCCCC"> Gaussian</font>

2153
01:18:01,430 --> 01:18:04,310
<font color="#CCCCCC">you get an l2 error</font><font color="#E5E5E5"> you get the l2 loss</font>

2154
01:18:04,310 --> 01:18:08,270
here so you can do lots of things<font color="#E5E5E5"> to</font>

2155
01:18:08,270 --> 01:18:10,340
choose the<font color="#CCCCCC"> Q</font><font color="#E5E5E5"> because even here you still</font>

2156
01:18:10,340 --> 01:18:12,980
I chose this Q as this<font color="#E5E5E5"> product of</font>

2157
01:18:12,980 --> 01:18:14,600
univariate<font color="#CCCCCC"> gaussians but</font><font color="#E5E5E5"> maybe that's</font>

2158
01:18:14,600 --> 01:18:16,400
not a<font color="#E5E5E5"> good thing</font><font color="#CCCCCC"> to do</font><font color="#E5E5E5"> so you</font><font color="#CCCCCC"> can do</font>

2159
01:18:16,400 --> 01:18:18,620
lots of things<font color="#E5E5E5"> in between you can do</font>

2160
01:18:18,620 --> 01:18:20,540
mixture models and mixture models is a

2161
01:18:20,540 --> 01:18:22,610
very popular<font color="#CCCCCC"> one or you</font><font color="#E5E5E5"> can just choose</font>

2162
01:18:22,610 --> 01:18:24,680
to build<font color="#E5E5E5"> gaussians with much more richer</font>

2163
01:18:24,680 --> 01:18:27,200
covariance function there's<font color="#E5E5E5"> many papers</font>

2164
01:18:27,200 --> 01:18:29,000
and building rich covariance models for

2165
01:18:29,000 --> 01:18:30,890
Gaussian distributions<font color="#E5E5E5"> you can build</font>

2166
01:18:30,890 --> 01:18:32,989
structured mean feel they can build<font color="#E5E5E5"> it</font>

2167
01:18:32,989 --> 01:18:34,820
basically<font color="#E5E5E5"> an auto regressive model</font><font color="#CCCCCC"> so</font>

2168
01:18:34,820 --> 01:18:36,140
all the<font color="#E5E5E5"> other models</font><font color="#CCCCCC"> that we learnt</font>

2169
01:18:36,140 --> 01:18:38,660
about<font color="#E5E5E5"> they can be used to design new</font>

2170
01:18:38,660 --> 01:18:40,580
posterior approximations<font color="#CCCCCC"> or you can</font>

2171
01:18:40,580 --> 01:18:42,350
create<font color="#E5E5E5"> other kinds</font><font color="#CCCCCC"> of</font><font color="#E5E5E5"> things these two</font>

2172
01:18:42,350 --> 01:18:44,000
are relation<font color="#E5E5E5"> of</font><font color="#CCCCCC"> Xillia variable models</font>

2173
01:18:44,000 --> 01:18:46,340
or normalizing flow methods and I have

2174
01:18:46,340 --> 01:18:48,920
lots of references<font color="#E5E5E5"> at the end if for</font>

2175
01:18:48,920 --> 01:18:51,560
anyone who wants to<font color="#E5E5E5"> look at them so the</font>

2176
01:18:51,560 --> 01:18:53,090
last bit is<font color="#E5E5E5"> just look at the</font>

2177
01:18:53,090 --> 01:18:55,130
optimization<font color="#E5E5E5"> of this kind of loss</font>

2178
01:18:55,130 --> 01:18:57,020
function<font color="#E5E5E5"> so there are many different</font>

2179
01:18:57,020 --> 01:18:58,880
<font color="#E5E5E5">ways of doing</font><font color="#CCCCCC"> the optimization the</font>

2180
01:18:58,880 --> 01:19:00,560
classical way was to do<font color="#E5E5E5"> what is called</font>

2181
01:19:00,560 --> 01:19:02,989
<font color="#E5E5E5">the variational</font><font color="#CCCCCC"> e/m algorithm and then</font>

2182
01:19:02,989 --> 01:19:05,000
to do stochastic versions of that<font color="#E5E5E5"> where</font>

2183
01:19:05,000 --> 01:19:07,040
we<font color="#E5E5E5"> can</font><font color="#CCCCCC"> subsample and use many</font><font color="#E5E5E5"> batches of</font>

2184
01:19:07,040 --> 01:19:10,310
data<font color="#E5E5E5"> instead then more</font><font color="#CCCCCC"> recently</font><font color="#E5E5E5"> what</font>

2185
01:19:10,310 --> 01:19:11,989
came was this idea of doubly stochastic

2186
01:19:11,989 --> 01:19:13,760
variational inference and<font color="#E5E5E5"> I'll explain</font>

2187
01:19:13,760 --> 01:19:16,190
<font color="#CCCCCC">the</font><font color="#E5E5E5"> couples</font><font color="#CCCCCC"> with</font><font color="#E5E5E5"> this idea of amortized</font>

2188
01:19:16,190 --> 01:19:18,080
<font color="#E5E5E5">inference which is the one that most</font>

2189
01:19:18,080 --> 01:19:22,100
people<font color="#CCCCCC"> use today</font><font color="#E5E5E5"> so in the variational</font>

2190
01:19:22,100 --> 01:19:24,380
problem<font color="#E5E5E5"> and in in an e/m algorithm if</font>

2191
01:19:24,380 --> 01:19:26,570
you recall what an e/m algorithm is

2192
01:19:26,570 --> 01:19:27,950
<font color="#E5E5E5">you're going to do an alternating</font>

2193
01:19:27,950 --> 01:19:30,469
optimization between model parameters

2194
01:19:30,469 --> 01:19:33,290
theta and variational parameters Phi

2195
01:19:33,290 --> 01:19:35,660
<font color="#CCCCCC">right so an e/m algorithm when you write</font>

2196
01:19:35,660 --> 01:19:37,760
it in code<font color="#E5E5E5"> always looks as</font><font color="#CCCCCC"> follows you</font>

2197
01:19:37,760 --> 01:19:39,770
write<font color="#CCCCCC"> a for loop for I equals 1 to</font><font color="#E5E5E5"> n</font>

2198
01:19:39,770 --> 01:19:42,080
then you<font color="#E5E5E5"> write a function for the</font><font color="#CCCCCC"> e step</font>

2199
01:19:42,080 --> 01:19:44,120
and the e step itself has a for<font color="#E5E5E5"> loop</font>

2200
01:19:44,120 --> 01:19:46,910
where you compute<font color="#E5E5E5"> you compute the</font>

2201
01:19:46,910 --> 01:19:48,410
gradient with respect<font color="#E5E5E5"> to all the</font>

2202
01:19:48,410 --> 01:19:50,900
variational parameters Phi<font color="#CCCCCC"> say e step is</font>

2203
01:19:50,900 --> 01:19:52,940
<font color="#E5E5E5">about expectation and the expectation</font>

2204
01:19:52,940 --> 01:19:54,980
<font color="#CCCCCC">you want is about</font><font color="#E5E5E5"> the Q distribution so</font>

2205
01:19:54,980 --> 01:19:56,630
these are the distributions are the

2206
01:19:56,630 --> 01:19:58,550
<font color="#E5E5E5">parameters of the Q distribution and</font>

2207
01:19:58,550 --> 01:20:00,000
then you come<font color="#CCCCCC"> to the M step</font>

2208
01:20:00,000 --> 01:20:01,770
<font color="#E5E5E5">as the optimization with respect to the</font>

2209
01:20:01,770 --> 01:20:04,520
<font color="#E5E5E5">model parameters which are</font><font color="#CCCCCC"> Tisa and</font>

2210
01:20:04,520 --> 01:20:07,080
because this<font color="#E5E5E5"> is</font><font color="#CCCCCC"> the bound like nem</font>

2211
01:20:07,080 --> 01:20:09,480
algorithm every<font color="#CCCCCC"> time you do</font><font color="#E5E5E5"> one setting</font>

2212
01:20:09,480 --> 01:20:11,970
of updating theta and Phi<font color="#E5E5E5"> you improve</font>

2213
01:20:11,970 --> 01:20:13,590
the bound until<font color="#CCCCCC"> you reach a point where</font>

2214
01:20:13,590 --> 01:20:15,330
you<font color="#E5E5E5"> can't do that anymore</font><font color="#CCCCCC"> and you</font>

2215
01:20:15,330 --> 01:20:17,850
inherit this by using variational

2216
01:20:17,850 --> 01:20:19,380
inference because<font color="#E5E5E5"> the variational bound</font>

2217
01:20:19,380 --> 01:20:22,410
<font color="#E5E5E5">is also a quantity</font><font color="#CCCCCC"> that is convergent</font>

2218
01:20:22,410 --> 01:20:24,660
<font color="#E5E5E5">this way and the idea of just</font><font color="#CCCCCC"> to</font><font color="#E5E5E5"> know</font>

2219
01:20:24,660 --> 01:20:27,030
this is that the classical idea of<font color="#CCCCCC"> e/m</font>

2220
01:20:27,030 --> 01:20:30,230
algorithm is<font color="#E5E5E5"> that you have a model</font><font color="#CCCCCC"> P and</font>

2221
01:20:30,230 --> 01:20:33,930
<font color="#E5E5E5">approximate distribution Q you go in and</font>

2222
01:20:33,930 --> 01:20:35,730
you do the key step the<font color="#CCCCCC"> East step which</font>

2223
01:20:35,730 --> 01:20:37,800
is to evaluate the expectation under

2224
01:20:37,800 --> 01:20:40,170
<font color="#E5E5E5">that distribution Q and</font><font color="#CCCCCC"> in</font><font color="#E5E5E5"> the classical</font>

2225
01:20:40,170 --> 01:20:41,850
way you always<font color="#E5E5E5"> assume you can compute</font>

2226
01:20:41,850 --> 01:20:44,970
this integral<font color="#E5E5E5"> exactly and analytically</font>

2227
01:20:44,970 --> 01:20:46,800
<font color="#E5E5E5">and if it's known to you analytically</font>

2228
01:20:46,800 --> 01:20:48,330
you use that result<font color="#CCCCCC"> to compute the</font>

2229
01:20:48,330 --> 01:20:50,700
gradient<font color="#E5E5E5"> so this is what every</font><font color="#CCCCCC"> ml Gotham</font>

2230
01:20:50,700 --> 01:20:53,250
looks like this is<font color="#CCCCCC"> difficult because now</font>

2231
01:20:53,250 --> 01:20:55,260
we're<font color="#E5E5E5"> gonna have to invent thousands of</font>

2232
01:20:55,260 --> 01:20:57,330
new tricks<font color="#CCCCCC"> just to solve this this</font>

2233
01:20:57,330 --> 01:21:00,420
<font color="#CCCCCC">integral here</font><font color="#E5E5E5"> and by inventing new more</font>

2234
01:21:00,420 --> 01:21:03,120
and more tricks<font color="#CCCCCC"> we get situations that</font>

2235
01:21:03,120 --> 01:21:04,950
are less and less generic<font color="#E5E5E5"> and much more</font>

2236
01:21:04,950 --> 01:21:07,290
specialized<font color="#E5E5E5"> so in some sense that's not</font>

2237
01:21:07,290 --> 01:21:09,270
<font color="#CCCCCC">the right direction to do we want to see</font>

2238
01:21:09,270 --> 01:21:10,920
if we<font color="#E5E5E5"> can create generic ways of doing</font>

2239
01:21:10,920 --> 01:21:13,140
that<font color="#E5E5E5"> and</font><font color="#CCCCCC"> just to jump to the end</font><font color="#E5E5E5"> we're</font>

2240
01:21:13,140 --> 01:21:14,610
<font color="#CCCCCC">gonna try and swap</font><font color="#E5E5E5"> these</font><font color="#CCCCCC"> two things</font>

2241
01:21:14,610 --> 01:21:17,780
using the two tricks that we had earlier

2242
01:21:17,780 --> 01:21:21,120
so let's talk about amortized<font color="#E5E5E5"> inference</font>

2243
01:21:21,120 --> 01:21:24,720
and just stay at<font color="#E5E5E5"> the e/m algorithm a</font>

2244
01:21:24,720 --> 01:21:26,220
<font color="#CCCCCC">little bit more</font><font color="#E5E5E5"> right</font><font color="#CCCCCC"> so the e/m Elger</font>

2245
01:21:26,220 --> 01:21:28,410
them again this is the algorithm<font color="#CCCCCC"> it has</font>

2246
01:21:28,410 --> 01:21:31,080
<font color="#CCCCCC">an e step for i equals</font><font color="#E5E5E5"> 1</font><font color="#CCCCCC"> to n so you go</font>

2247
01:21:31,080 --> 01:21:33,120
through<font color="#E5E5E5"> every data point and for every</font>

2248
01:21:33,120 --> 01:21:36,300
<font color="#E5E5E5">data point</font><font color="#CCCCCC"> you optimize this variational</font>

2249
01:21:36,300 --> 01:21:38,250
bound and<font color="#E5E5E5"> you solve for every data point</font>

2250
01:21:38,250 --> 01:21:41,190
<font color="#E5E5E5">and you find variational parameters Phi</font>

2251
01:21:41,190 --> 01:21:43,920
<font color="#CCCCCC">N and you do this</font><font color="#E5E5E5"> again</font><font color="#CCCCCC"> for</font><font color="#E5E5E5"> every data</font>

2252
01:21:43,920 --> 01:21:45,800
point<font color="#E5E5E5"> and then once you solve n</font>

2253
01:21:45,800 --> 01:21:47,850
optimizations and you have n sets of

2254
01:21:47,850 --> 01:21:49,950
parameters<font color="#CCCCCC"> then</font><font color="#E5E5E5"> you go to the M step and</font>

2255
01:21:49,950 --> 01:21:51,600
then<font color="#E5E5E5"> you compute this average which is</font>

2256
01:21:51,600 --> 01:21:55,140
<font color="#E5E5E5">an</font><font color="#CCCCCC"> average over</font><font color="#E5E5E5"> n data point</font><font color="#CCCCCC"> so the</font>

2257
01:21:55,140 --> 01:21:58,050
problem now<font color="#E5E5E5"> is that with this e step</font>

2258
01:21:58,050 --> 01:21:59,670
what<font color="#E5E5E5"> you have to do is you</font><font color="#CCCCCC"> are solving</font>

2259
01:21:59,670 --> 01:22:01,940
<font color="#E5E5E5">the optimization for every data point</font>

2260
01:22:01,940 --> 01:22:05,520
<font color="#E5E5E5">afresh</font><font color="#CCCCCC"> you never reuse what you just did</font>

2261
01:22:05,520 --> 01:22:07,740
<font color="#CCCCCC">for data point n minus 1 and you won't</font>

2262
01:22:07,740 --> 01:22:09,810
use<font color="#CCCCCC"> what you will do for data point</font><font color="#E5E5E5"> n</font>

2263
01:22:09,810 --> 01:22:12,830
plus 1<font color="#E5E5E5"> so this seems wasteful and</font>

2264
01:22:12,830 --> 01:22:14,290
some sense you can<font color="#CCCCCC"> think of</font><font color="#E5E5E5"> this</font>

2265
01:22:14,290 --> 01:22:16,460
classical<font color="#CCCCCC"> Estep is something that is</font>

2266
01:22:16,460 --> 01:22:18,470
like memory lists it doesn't use<font color="#CCCCCC"> the</font>

2267
01:22:18,470 --> 01:22:20,060
knowledge<font color="#E5E5E5"> of other kinds of</font><font color="#CCCCCC"> Estep</font>

2268
01:22:20,060 --> 01:22:22,370
computation<font color="#E5E5E5"> to inform the</font><font color="#CCCCCC"> estep</font>

2269
01:22:22,370 --> 01:22:24,560
computation<font color="#E5E5E5"> for data point</font><font color="#CCCCCC"> M and so what</font>

2270
01:22:24,560 --> 01:22:27,650
we want<font color="#E5E5E5"> to do is remove</font><font color="#CCCCCC"> that sort of</font>

2271
01:22:27,650 --> 01:22:29,840
deficiency in some sense and<font color="#E5E5E5"> introduce</font>

2272
01:22:29,840 --> 01:22:32,090
an idea<font color="#CCCCCC"> of memory</font><font color="#E5E5E5"> and this is where this</font>

2273
01:22:32,090 --> 01:22:33,920
idea of<font color="#E5E5E5"> introducing</font><font color="#CCCCCC"> what they call an</font>

2274
01:22:33,920 --> 01:22:36,050
inference<font color="#E5E5E5"> network or a recognition model</font>

2275
01:22:36,050 --> 01:22:38,360
<font color="#CCCCCC">is what we do is now we build</font><font color="#E5E5E5"> some new</font>

2276
01:22:38,360 --> 01:22:41,420
<font color="#CCCCCC">model</font><font color="#E5E5E5"> we'll call this thank you and it</font>

2277
01:22:41,420 --> 01:22:43,580
will have a<font color="#E5E5E5"> global set of parameters Phi</font>

2278
01:22:43,580 --> 01:22:47,030
<font color="#E5E5E5">it won't have parameters Phi n for every</font>

2279
01:22:47,030 --> 01:22:48,440
<font color="#CCCCCC">data point will</font><font color="#E5E5E5"> have one</font><font color="#CCCCCC"> set of</font>

2280
01:22:48,440 --> 01:22:50,060
parameters Phi that<font color="#E5E5E5"> will apply to all</font>

2281
01:22:50,060 --> 01:22:52,130
<font color="#E5E5E5">the</font><font color="#CCCCCC"> data points right and that's the</font>

2282
01:22:52,130 --> 01:22:54,440
point of<font color="#E5E5E5"> having</font><font color="#CCCCCC"> these inference networks</font>

2283
01:22:54,440 --> 01:22:57,470
and<font color="#E5E5E5"> the parameters of Q</font><font color="#CCCCCC"> are now no</font>

2284
01:22:57,470 --> 01:23:00,380
<font color="#E5E5E5">longer global per data point parameters</font>

2285
01:23:00,380 --> 01:23:02,000
that you have to do optimization for

2286
01:23:02,000 --> 01:23:03,680
there are actually a<font color="#CCCCCC"> global set of</font>

2287
01:23:03,680 --> 01:23:05,210
parameters that<font color="#CCCCCC"> are</font><font color="#E5E5E5"> applicable</font><font color="#CCCCCC"> to both</font>

2288
01:23:05,210 --> 01:23:07,700
training<font color="#E5E5E5"> and test time and the reason</font>

2289
01:23:07,700 --> 01:23:09,590
it's called an amortized<font color="#E5E5E5"> inference is</font>

2290
01:23:09,590 --> 01:23:11,570
<font color="#CCCCCC">that this new set of global parameters</font>

2291
01:23:11,570 --> 01:23:14,030
<font color="#E5E5E5">is what you</font><font color="#CCCCCC"> use to</font><font color="#E5E5E5"> spread the cost of</font>

2292
01:23:14,030 --> 01:23:15,740
the inference<font color="#E5E5E5"> over all the data points</font>

2293
01:23:15,740 --> 01:23:18,020
<font color="#CCCCCC">you get</font><font color="#E5E5E5"> to do sometimes</font><font color="#CCCCCC"> what they call</font>

2294
01:23:18,020 --> 01:23:20,570
sharing statistical<font color="#CCCCCC"> strengths</font><font color="#E5E5E5"> where you</font>

2295
01:23:20,570 --> 01:23:22,520
use all the<font color="#E5E5E5"> other data points inference</font>

2296
01:23:22,520 --> 01:23:25,310
to help inform your own inference and so

2297
01:23:25,310 --> 01:23:27,530
the joint<font color="#E5E5E5"> are and then by having</font><font color="#CCCCCC"> this</font>

2298
01:23:27,530 --> 01:23:29,510
kind of<font color="#E5E5E5"> inference</font><font color="#CCCCCC"> network</font><font color="#E5E5E5"> you no need to</font>

2299
01:23:29,510 --> 01:23:31,370
do an<font color="#CCCCCC"> EML</font><font color="#E5E5E5"> Gotham you don't need to</font>

2300
01:23:31,370 --> 01:23:34,420
alternate between model parameters and

2301
01:23:34,420 --> 01:23:36,470
<font color="#CCCCCC">variational parameters because there are</font>

2302
01:23:36,470 --> 01:23:38,270
only<font color="#E5E5E5"> global parameters you can do joint</font>

2303
01:23:38,270 --> 01:23:39,890
optimization of all of them

2304
01:23:39,890 --> 01:23:42,890
simultaneously<font color="#E5E5E5"> right so the</font><font color="#CCCCCC"> idea of</font>

2305
01:23:42,890 --> 01:23:45,410
inference networks or<font color="#CCCCCC"> anyplace</font><font color="#E5E5E5"> where you</font>

2306
01:23:45,410 --> 01:23:48,590
<font color="#E5E5E5">see this element of some kind of encoder</font>

2307
01:23:48,590 --> 01:23:50,660
<font color="#E5E5E5">appearing is that it gives you an</font>

2308
01:23:50,660 --> 01:23:52,970
efficient<font color="#CCCCCC"> mechanism to learn</font><font color="#E5E5E5"> kind of</font>

2309
01:23:52,970 --> 01:23:54,920
posterior distributions where you have

2310
01:23:54,920 --> 01:23:56,630
this<font color="#CCCCCC"> kind of</font><font color="#E5E5E5"> memory component where you</font>

2311
01:23:56,630 --> 01:23:59,060
<font color="#E5E5E5">can't share knowledge between different</font>

2312
01:23:59,060 --> 01:24:02,180
data<font color="#CCCCCC"> points</font><font color="#E5E5E5"> so</font><font color="#CCCCCC"> let's put very rational</font>

2313
01:24:02,180 --> 01:24:03,500
inference and amortize<font color="#E5E5E5"> inference</font>

2314
01:24:03,500 --> 01:24:05,900
together<font color="#E5E5E5"> and so we have this approximate</font>

2315
01:24:05,900 --> 01:24:07,610
posterior distribution<font color="#E5E5E5"> Q which we need</font>

2316
01:24:07,610 --> 01:24:09,710
<font color="#E5E5E5">to design in truth we have a</font>

2317
01:24:09,710 --> 01:24:11,390
reconstruction<font color="#E5E5E5"> term which is going to</font>

2318
01:24:11,390 --> 01:24:13,370
<font color="#E5E5E5">give us the fidelity of how well we are</font>

2319
01:24:13,370 --> 01:24:15,770
<font color="#E5E5E5">learning the data we see and we have the</font>

2320
01:24:15,770 --> 01:24:17,510
penalty<font color="#E5E5E5"> term to do Occam's razor</font>

2321
01:24:17,510 --> 01:24:20,390
now we implement a stochastic<font color="#CCCCCC"> encoder</font>

2322
01:24:20,390 --> 01:24:22,760
<font color="#CCCCCC">decoder system</font><font color="#E5E5E5"> so</font><font color="#CCCCCC"> our model</font><font color="#E5E5E5"> is</font>

2323
01:24:22,760 --> 01:24:25,280
effectively a decoder<font color="#E5E5E5"> it takes latent</font>

2324
01:24:25,280 --> 01:24:26,390
variable<font color="#CCCCCC"> Z</font>

2325
01:24:26,390 --> 01:24:28,670
through some kind<font color="#E5E5E5"> of model</font><font color="#CCCCCC"> and generates</font>

2326
01:24:28,670 --> 01:24:30,980
data<font color="#CCCCCC"> exa it</font><font color="#E5E5E5"> is a generative model</font><font color="#CCCCCC"> and</font>

2327
01:24:30,980 --> 01:24:32,930
then<font color="#E5E5E5"> we have a recognition model or an</font>

2328
01:24:32,930 --> 01:24:34,670
inference<font color="#CCCCCC"> Network which states data</font><font color="#E5E5E5"> X</font>

2329
01:24:34,670 --> 01:24:36,710
through another set<font color="#CCCCCC"> of global parameters</font>

2330
01:24:36,710 --> 01:24:39,230
and gives us<font color="#E5E5E5"> samples from</font><font color="#CCCCCC"> this set</font><font color="#E5E5E5"> Q in</font>

2331
01:24:39,230 --> 01:24:41,300
fact the two things that<font color="#CCCCCC"> this model</font>

2332
01:24:41,300 --> 01:24:42,830
<font color="#E5E5E5">needs to do it needs to be able to give</font>

2333
01:24:42,830 --> 01:24:44,600
you<font color="#CCCCCC"> samples and you need to tell you</font>

2334
01:24:44,600 --> 01:24:46,790
what the entropy<font color="#E5E5E5"> just what log Q is</font>

2335
01:24:46,790 --> 01:24:48,560
doesn't<font color="#E5E5E5"> need to do anything else so when</font>

2336
01:24:48,560 --> 01:24:50,420
you<font color="#CCCCCC"> code a</font><font color="#E5E5E5"> class for these kind of</font>

2337
01:24:50,420 --> 01:24:51,590
distributions as long as the

2338
01:24:51,590 --> 01:24:54,470
distribution<font color="#E5E5E5"> can have dot sample and dot</font>

2339
01:24:54,470 --> 01:24:56,540
entropy or dot log problem<font color="#CCCCCC"> that's all</font>

2340
01:24:56,540 --> 01:24:58,460
you need to basically<font color="#E5E5E5"> implement these</font>

2341
01:24:58,460 --> 01:25:00,320
two things<font color="#E5E5E5"> so again as I said the</font>

2342
01:25:00,320 --> 01:25:02,270
decoder is a<font color="#E5E5E5"> likelihood model the</font>

2343
01:25:02,270 --> 01:25:03,830
inference is now dealt with by this

2344
01:25:03,830 --> 01:25:07,090
encoder<font color="#CCCCCC"> or variational distribution and</font>

2345
01:25:07,090 --> 01:25:09,740
what this also does when you can see

2346
01:25:09,740 --> 01:25:11,600
this it transforms an auto encoder which

2347
01:25:11,600 --> 01:25:13,430
is a deterministic<font color="#CCCCCC"> function which is not</font>

2348
01:25:13,430 --> 01:25:15,590
<font color="#E5E5E5">probabilistic in this way into a</font>

2349
01:25:15,590 --> 01:25:17,270
generative<font color="#E5E5E5"> model because now you can</font>

2350
01:25:17,270 --> 01:25:19,190
actually sample from it by just sampling

2351
01:25:19,190 --> 01:25:21,320
<font color="#E5E5E5">from Z and then generating data right</font>

2352
01:25:21,320 --> 01:25:25,160
and<font color="#CCCCCC"> so this specific combination again</font>

2353
01:25:25,160 --> 01:25:26,960
going back to<font color="#E5E5E5"> that</font><font color="#CCCCCC"> principle of machine</font>

2354
01:25:26,960 --> 01:25:28,160
learning<font color="#CCCCCC"> that I asked</font><font color="#E5E5E5"> you to</font><font color="#CCCCCC"> think</font><font color="#E5E5E5"> about</font>

2355
01:25:28,160 --> 01:25:31,430
<font color="#E5E5E5">we chose a latent</font><font color="#CCCCCC"> variable model ik Zed</font>

2356
01:25:31,430 --> 01:25:33,950
and you chose a principle of inference

2357
01:25:33,950 --> 01:25:35,480
which is based on variational inference

2358
01:25:35,480 --> 01:25:37,910
<font color="#E5E5E5">and you implemented it using amortized</font>

2359
01:25:37,910 --> 01:25:39,920
inference<font color="#E5E5E5"> with the recognition</font><font color="#CCCCCC"> model</font>

2360
01:25:39,920 --> 01:25:42,650
that thing together<font color="#CCCCCC"> is an algorithm and</font>

2361
01:25:42,650 --> 01:25:44,240
<font color="#E5E5E5">that algorithm is today called</font><font color="#CCCCCC"> a</font>

2362
01:25:44,240 --> 01:25:46,400
variational auto encoder<font color="#CCCCCC"> please don't</font>

2363
01:25:46,400 --> 01:25:49,400
call it the<font color="#CCCCCC"> very short encoder model a</font>

2364
01:25:49,400 --> 01:25:51,590
<font color="#E5E5E5">little part of me will die so never do</font>

2365
01:25:51,590 --> 01:25:53,900
that that is<font color="#E5E5E5"> wrong</font><font color="#CCCCCC"> BAE</font>

2366
01:25:53,900 --> 01:25:55,910
is an algorithm because it defines a

2367
01:25:55,910 --> 01:25:57,890
full set of computation of computational

2368
01:25:57,890 --> 01:25:59,930
graph<font color="#E5E5E5"> for dealing with data learning its</font>

2369
01:25:59,930 --> 01:26:01,400
posterior<font color="#E5E5E5"> distribution and doing</font><font color="#CCCCCC"> its</font>

2370
01:26:01,400 --> 01:26:03,290
optimization together so never forget

2371
01:26:03,290 --> 01:26:05,300
<font color="#E5E5E5">what the model is it is a latent</font>

2372
01:26:05,300 --> 01:26:06,980
<font color="#E5E5E5">variable model what the principle of</font>

2373
01:26:06,980 --> 01:26:08,510
inference is you use variational

2374
01:26:08,510 --> 01:26:09,830
inference you can replace that<font color="#CCCCCC"> with</font>

2375
01:26:09,830 --> 01:26:11,510
<font color="#CCCCCC">almost</font><font color="#E5E5E5"> anything else that you like and</font>

2376
01:26:11,510 --> 01:26:13,100
we'll do<font color="#E5E5E5"> that now in the next</font><font color="#CCCCCC"> step and</font>

2377
01:26:13,100 --> 01:26:15,320
then how you put<font color="#CCCCCC"> it together</font><font color="#E5E5E5"> was by</font>

2378
01:26:15,320 --> 01:26:16,730
using<font color="#E5E5E5"> this inference network on</font>

2379
01:26:16,730 --> 01:26:18,260
amortized inference but<font color="#E5E5E5"> we could have</font>

2380
01:26:18,260 --> 01:26:20,840
easily solve this by variational<font color="#CCCCCC"> e/m or</font>

2381
01:26:20,840 --> 01:26:23,600
Monte Carlo<font color="#CCCCCC"> e/m or hybrid Monte Carlo</font>

2382
01:26:23,600 --> 01:26:28,880
sampling or several<font color="#E5E5E5"> other methods</font><font color="#CCCCCC"> so ok</font>

2383
01:26:28,880 --> 01:26:29,900
<font color="#E5E5E5">we are running out of time</font><font color="#CCCCCC"> so I'm just</font>

2384
01:26:29,900 --> 01:26:31,370
<font color="#CCCCCC">quickly going</font><font color="#E5E5E5"> to say a</font><font color="#CCCCCC"> little bit about</font>

2385
01:26:31,370 --> 01:26:33,620
the<font color="#E5E5E5"> stochastic gradient estimation again</font>

2386
01:26:33,620 --> 01:26:35,780
<font color="#E5E5E5">here's that famous problem that came</font>

2387
01:26:35,780 --> 01:26:37,460
where we had two tricks<font color="#E5E5E5"> earlier we need</font>

2388
01:26:37,460 --> 01:26:39,860
to compute<font color="#E5E5E5"> the gradient of an integrand</font>

2389
01:26:39,860 --> 01:26:41,600
with respect<font color="#CCCCCC"> to some function and</font><font color="#E5E5E5"> in the</font>

2390
01:26:41,600 --> 01:26:44,390
e/m algorithm we had first compute the

2391
01:26:44,390 --> 01:26:46,100
integral then compute<font color="#CCCCCC"> the gradient</font><font color="#E5E5E5"> and</font>

2392
01:26:46,100 --> 01:26:48,050
what we're<font color="#CCCCCC"> going to do now is</font><font color="#E5E5E5"> we're</font>

2393
01:26:48,050 --> 01:26:49,340
<font color="#CCCCCC">going to swap the gradient with the</font>

2394
01:26:49,340 --> 01:26:51,320
integral first<font color="#E5E5E5"> and the way we can do</font>

2395
01:26:51,320 --> 01:26:53,060
that<font color="#E5E5E5"> swapping of the greater</font><font color="#CCCCCC"> an integral</font>

2396
01:26:53,060 --> 01:26:55,310
is to exactly<font color="#E5E5E5"> use those two tricks that</font>

2397
01:26:55,310 --> 01:26:55,850
we had

2398
01:26:55,850 --> 01:26:57,830
you can either compute<font color="#E5E5E5"> this gradient by</font>

2399
01:26:57,830 --> 01:26:59,750
the pathways estimator using a<font color="#CCCCCC"> ripper</font>

2400
01:26:59,750 --> 01:27:01,670
<font color="#CCCCCC">ammeter ization trick</font><font color="#E5E5E5"> or you use the</font>

2401
01:27:01,670 --> 01:27:03,590
score function<font color="#CCCCCC"> estimator by applying the</font>

2402
01:27:03,590 --> 01:27:05,210
identity and<font color="#CCCCCC"> the</font><font color="#E5E5E5"> log derivative tricks</font>

2403
01:27:05,210 --> 01:27:07,610
<font color="#E5E5E5">right and so you</font><font color="#CCCCCC"> get these two ways</font><font color="#E5E5E5"> of</font>

2404
01:27:07,610 --> 01:27:09,650
doing estimation<font color="#E5E5E5"> both of them are</font>

2405
01:27:09,650 --> 01:27:14,000
equally valid<font color="#E5E5E5"> and what will inform</font><font color="#CCCCCC"> your</font>

2406
01:27:14,000 --> 01:27:16,400
choice will be again how you design your

2407
01:27:16,400 --> 01:27:18,650
model<font color="#E5E5E5"> f so for example if you were</font>

2408
01:27:18,650 --> 01:27:21,230
<font color="#E5E5E5">working computer graphics and your model</font>

2409
01:27:21,230 --> 01:27:22,700
you didn't want to<font color="#E5E5E5"> write</font><font color="#CCCCCC"> a</font><font color="#E5E5E5"> model</font><font color="#CCCCCC"> you</font>

2410
01:27:22,700 --> 01:27:24,110
<font color="#CCCCCC">actually wanted to put a renderer</font>

2411
01:27:24,110 --> 01:27:26,660
graphics render as the model then<font color="#CCCCCC"> you</font>

2412
01:27:26,660 --> 01:27:27,710
<font color="#E5E5E5">wouldn't be able to differentiate</font>

2413
01:27:27,710 --> 01:27:29,690
through the renderer<font color="#E5E5E5"> not in general</font>

2414
01:27:29,690 --> 01:27:31,340
<font color="#CCCCCC">though</font><font color="#E5E5E5"> they are differentiable renderers</font>

2415
01:27:31,340 --> 01:27:33,620
so then the<font color="#CCCCCC"> only thing</font><font color="#E5E5E5"> you could do is</font>

2416
01:27:33,620 --> 01:27:35,510
to do this model<font color="#E5E5E5"> but if you were doing</font><font color="#CCCCCC"> a</font>

2417
01:27:35,510 --> 01:27:38,090
more statistical approach where we were

2418
01:27:38,090 --> 01:27:40,970
understanding the biology<font color="#E5E5E5"> of the genetic</font>

2419
01:27:40,970 --> 01:27:42,680
tree and we actually<font color="#CCCCCC"> built</font><font color="#E5E5E5"> that model</font>

2420
01:27:42,680 --> 01:27:44,060
then we<font color="#E5E5E5"> would know about</font><font color="#CCCCCC"> f we could</font>

2421
01:27:44,060 --> 01:27:45,710
<font color="#E5E5E5">differentiate and then that would be</font>

2422
01:27:45,710 --> 01:27:48,160
<font color="#E5E5E5">maybe the better</font><font color="#CCCCCC"> approach to</font><font color="#E5E5E5"> take</font><font color="#CCCCCC"> so</font>

2423
01:27:48,160 --> 01:27:50,750
those are called doubly stochastic<font color="#CCCCCC"> so in</font>

2424
01:27:50,750 --> 01:27:52,040
the last<font color="#CCCCCC"> ten</font><font color="#E5E5E5"> minutes</font><font color="#CCCCCC"> I just want to</font>

2425
01:27:52,040 --> 01:27:53,330
<font color="#E5E5E5">quickly talk about estimation</font><font color="#CCCCCC"> about</font>

2426
01:27:53,330 --> 01:27:55,400
<font color="#E5E5E5">comparison and learning in implicit</font>

2427
01:27:55,400 --> 01:27:57,230
generative<font color="#E5E5E5"> model so in implicit</font>

2428
01:27:57,230 --> 01:27:58,850
generative models you<font color="#CCCCCC"> have a simulator</font>

2429
01:27:58,850 --> 01:28:01,220
from some data<font color="#E5E5E5"> points</font><font color="#CCCCCC"> from some random</font>

2430
01:28:01,220 --> 01:28:03,680
source said through this function<font color="#CCCCCC"> f and</font>

2431
01:28:03,680 --> 01:28:05,780
it<font color="#E5E5E5"> can just simulate data</font><font color="#CCCCCC"> X and the</font>

2432
01:28:05,780 --> 01:28:07,490
tasks that<font color="#E5E5E5"> you</font><font color="#CCCCCC"> have to do is compare</font>

2433
01:28:07,490 --> 01:28:10,250
<font color="#CCCCCC">samples from your data</font><font color="#E5E5E5"> model which I'm</font>

2434
01:28:10,250 --> 01:28:11,810
going<font color="#E5E5E5"> to call</font><font color="#CCCCCC"> Q with the true data</font>

2435
01:28:11,810 --> 01:28:14,210
distribution which is<font color="#E5E5E5"> P</font><font color="#CCCCCC"> star and</font><font color="#E5E5E5"> this is</font>

2436
01:28:14,210 --> 01:28:15,770
a classical problem<font color="#CCCCCC"> in statistic which</font>

2437
01:28:15,770 --> 01:28:17,390
is<font color="#E5E5E5"> called the two sample testing problem</font>

2438
01:28:17,390 --> 01:28:19,670
<font color="#CCCCCC">you have two sets</font><font color="#E5E5E5"> of samples you need to</font>

2439
01:28:19,670 --> 01:28:22,160
compare them<font color="#CCCCCC"> in some way and we</font><font color="#E5E5E5"> will</font>

2440
01:28:22,160 --> 01:28:24,020
<font color="#CCCCCC">actually want to do one</font><font color="#E5E5E5"> step more than</font>

2441
01:28:24,020 --> 01:28:25,850
what statisticians<font color="#E5E5E5"> we would do we also</font>

2442
01:28:25,850 --> 01:28:27,380
want to<font color="#E5E5E5"> do learning of our model</font>

2443
01:28:27,380 --> 01:28:29,240
parameter<font color="#E5E5E5"> so we're gonna do need to do a</font>

2444
01:28:29,240 --> 01:28:31,520
little bit<font color="#CCCCCC"> more and so all we</font><font color="#E5E5E5"> need to do</font>

2445
01:28:31,520 --> 01:28:33,830
is find ways<font color="#E5E5E5"> to compare distributions</font>

2446
01:28:33,830 --> 01:28:35,750
<font color="#E5E5E5">and there are either two things you can</font>

2447
01:28:35,750 --> 01:28:38,240
do you can<font color="#CCCCCC"> either compute P divided by</font><font color="#E5E5E5"> Q</font>

2448
01:28:38,240 --> 01:28:40,910
<font color="#CCCCCC">and if P divided by</font><font color="#E5E5E5"> Q</font><font color="#CCCCCC"> is</font><font color="#E5E5E5"> one that is the</font>

2449
01:28:40,910 --> 01:28:42,170
definition of<font color="#CCCCCC"> learning</font><font color="#E5E5E5"> because then</font>

2450
01:28:42,170 --> 01:28:43,940
you've learned<font color="#E5E5E5"> the data point or you can</font>

2451
01:28:43,940 --> 01:28:46,490
say P minus Q and if he<font color="#CCCCCC"> minus Q is zero</font>

2452
01:28:46,490 --> 01:28:48,230
then you've also<font color="#CCCCCC"> lunch right so these</font>

2453
01:28:48,230 --> 01:28:50,570
<font color="#CCCCCC">are</font><font color="#E5E5E5"> the two principles of learning</font>

2454
01:28:50,570 --> 01:28:52,370
<font color="#E5E5E5">involved here and you can do both of</font>

2455
01:28:52,370 --> 01:28:53,270
<font color="#E5E5E5">them</font>

2456
01:28:53,270 --> 01:28:54,860
if we had an hour we could<font color="#E5E5E5"> just talk</font>

2457
01:28:54,860 --> 01:28:56,480
about this one slide as different<font color="#CCCCCC"> ways</font>

2458
01:28:56,480 --> 01:28:58,850
of<font color="#CCCCCC"> do a density ratio versus density</font>

2459
01:28:58,850 --> 01:29:02,660
difference estimation so let's at the

2460
01:29:02,660 --> 01:29:04,940
<font color="#CCCCCC">high level</font><font color="#E5E5E5"> talk about this estimation by</font>

2461
01:29:04,940 --> 01:29:06,590
comparison always involves two steps

2462
01:29:06,590 --> 01:29:08,750
it's exactly like<font color="#E5E5E5"> the e/m algorithm</font>

2463
01:29:08,750 --> 01:29:11,360
before<font color="#E5E5E5"> you have a testing step or a</font>

2464
01:29:11,360 --> 01:29:13,100
comparison<font color="#CCCCCC"> step where you're first going</font>

2465
01:29:13,100 --> 01:29:16,040
<font color="#CCCCCC">to</font><font color="#E5E5E5"> find something some tools some trick</font>

2466
01:29:16,040 --> 01:29:18,440
that<font color="#CCCCCC"> helps you compare one sample</font><font color="#E5E5E5"> of</font>

2467
01:29:18,440 --> 01:29:20,240
<font color="#E5E5E5">data with</font><font color="#CCCCCC"> another and</font><font color="#E5E5E5"> to say how close</font>

2468
01:29:20,240 --> 01:29:22,370
<font color="#E5E5E5">it is or how different it is and once</font>

2469
01:29:22,370 --> 01:29:24,080
you have<font color="#CCCCCC"> that thing which tells you</font><font color="#E5E5E5"> how</font>

2470
01:29:24,080 --> 01:29:25,790
close<font color="#E5E5E5"> are different that's exactly like</font>

2471
01:29:25,790 --> 01:29:27,470
<font color="#E5E5E5">gradient and then you can use that to</font>

2472
01:29:27,470 --> 01:29:29,720
learn<font color="#E5E5E5"> so then you can do the learning</font>

2473
01:29:29,720 --> 01:29:33,110
<font color="#E5E5E5">step or the adjustment step</font><font color="#CCCCCC"> and so here</font>

2474
01:29:33,110 --> 01:29:34,760
<font color="#E5E5E5">you have the hypothesis is either that</font>

2475
01:29:34,760 --> 01:29:36,380
<font color="#E5E5E5">the two distributions</font><font color="#CCCCCC"> are the same or</font>

2476
01:29:36,380 --> 01:29:37,970
the two are different<font color="#CCCCCC"> and you're going</font>

2477
01:29:37,970 --> 01:29:39,710
<font color="#E5E5E5">to try and find some loss function f as</font>

2478
01:29:39,710 --> 01:29:41,570
I said you can do<font color="#E5E5E5"> this by density</font>

2479
01:29:41,570 --> 01:29:44,120
<font color="#E5E5E5">difference methods by looking at P minus</font>

2480
01:29:44,120 --> 01:29:46,430
Q<font color="#E5E5E5"> and lots of methods and the maximum</font>

2481
01:29:46,430 --> 01:29:48,500
mean discrepancy optimal transport

2482
01:29:48,500 --> 01:29:50,960
moment matching are going to fit into

2483
01:29:50,960 --> 01:29:52,190
this<font color="#CCCCCC"> category I'm not going to talk</font>

2484
01:29:52,190 --> 01:29:54,200
<font color="#CCCCCC">about that today</font><font color="#E5E5E5"> and then</font><font color="#CCCCCC"> you can either</font>

2485
01:29:54,200 --> 01:29:55,970
look<font color="#E5E5E5"> at P divided by Q which</font><font color="#CCCCCC"> is the</font>

2486
01:29:55,970 --> 01:29:58,370
density<font color="#E5E5E5"> ratio</font><font color="#CCCCCC"> method and we can do this</font>

2487
01:29:58,370 --> 01:30:00,290
either by<font color="#E5E5E5"> class probability estimation</font>

2488
01:30:00,290 --> 01:30:01,970
<font color="#E5E5E5">because we looked at that as a one</font>

2489
01:30:01,970 --> 01:30:03,590
specific<font color="#CCCCCC"> trick</font><font color="#E5E5E5"> but there are other</font>

2490
01:30:03,590 --> 01:30:05,390
methods based<font color="#E5E5E5"> and</font><font color="#CCCCCC"> Bragman</font><font color="#E5E5E5"> divergences</font>

2491
01:30:05,390 --> 01:30:07,430
<font color="#CCCCCC">and F divergences that we can solve</font><font color="#E5E5E5"> that</font>

2492
01:30:07,430 --> 01:30:10,070
instead so this is sort<font color="#E5E5E5"> of in</font><font color="#CCCCCC"> general</font>

2493
01:30:10,070 --> 01:30:12,410
view<font color="#E5E5E5"> of that landscape so let's just</font>

2494
01:30:12,410 --> 01:30:13,850
look at adversarial learning<font color="#E5E5E5"> in</font>

2495
01:30:13,850 --> 01:30:15,320
adversarial learning we're going to look

2496
01:30:15,320 --> 01:30:17,360
<font color="#E5E5E5">at this ratio</font><font color="#CCCCCC"> of</font><font color="#E5E5E5"> P divided by Q</font><font color="#CCCCCC"> and</font>

2497
01:30:17,360 --> 01:30:19,910
based on<font color="#E5E5E5"> the</font><font color="#CCCCCC"> density ratio trick</font><font color="#E5E5E5"> we know</font>

2498
01:30:19,910 --> 01:30:21,740
that we can solve ratios like<font color="#E5E5E5"> this by</font>

2499
01:30:21,740 --> 01:30:23,450
building classifiers instead our

2500
01:30:23,450 --> 01:30:25,460
generative model will be sampled from

2501
01:30:25,460 --> 01:30:27,530
some base distribution like a Gaussian

2502
01:30:27,530 --> 01:30:29,930
<font color="#CCCCCC">and generate this to some function f</font>

2503
01:30:29,930 --> 01:30:32,240
like a<font color="#E5E5E5"> conflict to generate samples and</font>

2504
01:30:32,240 --> 01:30:33,080
then we're going<font color="#CCCCCC"> to need</font><font color="#E5E5E5"> to do</font>

2505
01:30:33,080 --> 01:30:35,630
comparison right and so the comparison

2506
01:30:35,630 --> 01:30:38,240
<font color="#E5E5E5">is</font><font color="#CCCCCC"> to build this classifier that's what</font>

2507
01:30:38,240 --> 01:30:40,610
the density<font color="#E5E5E5"> ratio trick is so how do you</font>

2508
01:30:40,610 --> 01:30:43,130
actually compute<font color="#E5E5E5"> P of y equals 1 given</font><font color="#CCCCCC"> X</font>

2509
01:30:43,130 --> 01:30:45,020
you need<font color="#E5E5E5"> to</font><font color="#CCCCCC"> build a classifier</font><font color="#E5E5E5"> and</font>

2510
01:30:45,020 --> 01:30:47,180
that's in the statistical language

2511
01:30:47,180 --> 01:30:49,160
<font color="#E5E5E5">called building a scoring function so</font>

2512
01:30:49,160 --> 01:30:50,690
<font color="#CCCCCC">you're going to</font><font color="#E5E5E5"> build a scoring function</font>

2513
01:30:50,690 --> 01:30:52,340
which is going to tell you what the

2514
01:30:52,340 --> 01:30:54,650
probability<font color="#CCCCCC"> of</font><font color="#E5E5E5"> y equals plus</font><font color="#CCCCCC"> 1 is and</font>

2515
01:30:54,650 --> 01:30:56,780
the probability<font color="#CCCCCC"> of y equals minus 1 is</font>

2516
01:30:56,780 --> 01:30:58,580
just<font color="#CCCCCC"> 1</font><font color="#E5E5E5"> minus</font><font color="#CCCCCC"> that probability because</font>

2517
01:30:58,580 --> 01:31:01,460
they must sum<font color="#CCCCCC"> to</font><font color="#E5E5E5"> 1 and then now</font><font color="#CCCCCC"> that you</font>

2518
01:31:01,460 --> 01:31:03,830
have this you<font color="#E5E5E5"> can assign a</font><font color="#CCCCCC"> Bernoulli</font>

2519
01:31:03,830 --> 01:31:06,560
loss you can say I want<font color="#E5E5E5"> things</font>

2520
01:31:06,560 --> 01:31:10,040
<font color="#CCCCCC">I want data</font><font color="#E5E5E5"> that's under</font><font color="#CCCCCC"> queue to be</font>

2521
01:31:10,040 --> 01:31:11,840
seen as the<font color="#CCCCCC"> negative</font><font color="#E5E5E5"> class and data</font>

2522
01:31:11,840 --> 01:31:13,790
that's<font color="#CCCCCC"> frumpy to be seen as the positive</font>

2523
01:31:13,790 --> 01:31:15,140
<font color="#CCCCCC">class and that's how you'll actually</font>

2524
01:31:15,140 --> 01:31:16,880
learn this scoring function with that

2525
01:31:16,880 --> 01:31:19,370
<font color="#E5E5E5">and you can use any sort of scoring</font>

2526
01:31:19,370 --> 01:31:20,840
functions you don't<font color="#CCCCCC"> only</font><font color="#E5E5E5"> need to use the</font>

2527
01:31:20,840 --> 01:31:22,670
<font color="#CCCCCC">bernoulli you can replace this</font><font color="#E5E5E5"> with the</font>

2528
01:31:22,670 --> 01:31:24,260
squared loss in which case is called<font color="#CCCCCC"> a</font>

2529
01:31:24,260 --> 01:31:26,030
<font color="#E5E5E5">breyer loss and you can replace it with</font>

2530
01:31:26,030 --> 01:31:28,010
<font color="#E5E5E5">several other losses their entire papers</font>

2531
01:31:28,010 --> 01:31:30,230
<font color="#E5E5E5">just on choosing scoring functions and</font>

2532
01:31:30,230 --> 01:31:31,610
how you can choose the best<font color="#E5E5E5"> scoring</font>

2533
01:31:31,610 --> 01:31:34,280
function<font color="#CCCCCC"> depending</font><font color="#E5E5E5"> on the problem</font><font color="#CCCCCC"> you</font>

2534
01:31:34,280 --> 01:31:36,620
have<font color="#E5E5E5"> so this kind of idea has lots of</font>

2535
01:31:36,620 --> 01:31:37,730
different<font color="#CCCCCC"> names and</font><font color="#E5E5E5"> the most generic</font>

2536
01:31:37,730 --> 01:31:39,890
<font color="#E5E5E5">name it has been</font><font color="#CCCCCC"> given</font><font color="#E5E5E5"> it's called</font>

2537
01:31:39,890 --> 01:31:41,930
unsupervised<font color="#CCCCCC"> as supervised learning and</font>

2538
01:31:41,930 --> 01:31:43,640
<font color="#E5E5E5">you can see that we wanted to do this</font>

2539
01:31:43,640 --> 01:31:45,380
<font color="#E5E5E5">unsupervised learning and what we first</font>

2540
01:31:45,380 --> 01:31:46,910
had<font color="#CCCCCC"> to do was build this classifier so</font>

2541
01:31:46,910 --> 01:31:48,740
<font color="#E5E5E5">that is the supervised part and that's</font>

2542
01:31:48,740 --> 01:31:50,870
unsupervised supervised learning<font color="#CCCCCC"> I</font>

2543
01:31:50,870 --> 01:31:52,850
mentioned earlier<font color="#E5E5E5"> in the approximate</font>

2544
01:31:52,850 --> 01:31:54,290
Bayesian computation literature<font color="#CCCCCC"> they've</font>

2545
01:31:54,290 --> 01:31:56,150
also<font color="#E5E5E5"> had this</font><font color="#CCCCCC"> idea</font><font color="#E5E5E5"> of building a</font>

2546
01:31:56,150 --> 01:31:58,850
classifier<font color="#CCCCCC"> to decide moments or ways of</font>

2547
01:31:58,850 --> 01:32:00,740
comparing<font color="#CCCCCC"> two data points and then</font>

2548
01:32:00,740 --> 01:32:02,960
<font color="#CCCCCC">learning</font><font color="#E5E5E5"> a Monte Carlo sample that's</font>

2549
01:32:02,960 --> 01:32:05,600
called classify<font color="#E5E5E5"> ABC non-controlled</font><font color="#CCCCCC"> north</font>

2550
01:32:05,600 --> 01:32:07,580
contrastive estimation is a way<font color="#CCCCCC"> of doing</font>

2551
01:32:07,580 --> 01:32:09,710
non maximum likelihood estimation<font color="#E5E5E5"> like</font>

2552
01:32:09,710 --> 01:32:11,960
all of this in this section<font color="#E5E5E5"> is and so</font>

2553
01:32:11,960 --> 01:32:13,580
that<font color="#E5E5E5"> uses the same idea</font><font color="#CCCCCC"> and then</font>

2554
01:32:13,580 --> 01:32:15,620
adversarial learning<font color="#E5E5E5"> in particular</font><font color="#CCCCCC"> in</font>

2555
01:32:15,620 --> 01:32:18,020
the way it appears<font color="#E5E5E5"> in ganz uses also</font>

2556
01:32:18,020 --> 01:32:20,030
<font color="#E5E5E5">exactly this principle so we have again</font>

2557
01:32:20,030 --> 01:32:23,030
an alternating<font color="#CCCCCC"> optimization</font><font color="#E5E5E5"> between two</font>

2558
01:32:23,030 --> 01:32:25,040
types of<font color="#CCCCCC"> parameters you have model</font>

2559
01:32:25,040 --> 01:32:27,440
parameters Phi and then parameters of

2560
01:32:27,440 --> 01:32:30,050
<font color="#E5E5E5">the scoring functions D right and in</font>

2561
01:32:30,050 --> 01:32:32,150
adversarial networks you will<font color="#E5E5E5"> call this</font>

2562
01:32:32,150 --> 01:32:34,520
model<font color="#E5E5E5"> the generator and you will call</font>

2563
01:32:34,520 --> 01:32:37,040
this<font color="#E5E5E5"> scoring function the discriminator</font>

2564
01:32:37,040 --> 01:32:39,020
because it is a classifier and then you

2565
01:32:39,020 --> 01:32:40,070
have to just solve the optimization

2566
01:32:40,070 --> 01:32:42,770
between these two things<font color="#E5E5E5"> so then again</font>

2567
01:32:42,770 --> 01:32:44,210
you have<font color="#E5E5E5"> the comparison loss which is</font>

2568
01:32:44,210 --> 01:32:45,860
<font color="#E5E5E5">solve for</font><font color="#CCCCCC"> the</font><font color="#E5E5E5"> parameters of the</font>

2569
01:32:45,860 --> 01:32:47,600
discriminator<font color="#E5E5E5"> this is the thing</font><font color="#CCCCCC"> that</font>

2570
01:32:47,600 --> 01:32:49,520
tells<font color="#E5E5E5"> you how close or</font><font color="#CCCCCC"> how different</font>

2571
01:32:49,520 --> 01:32:51,830
<font color="#E5E5E5">these</font><font color="#CCCCCC"> two set of samples are and then</font>

2572
01:32:51,830 --> 01:32:53,120
you have the generative loss who says

2573
01:32:53,120 --> 01:32:54,710
well based on<font color="#E5E5E5"> the knowledge that I have</font>

2574
01:32:54,710 --> 01:32:56,960
<font color="#CCCCCC">of</font><font color="#E5E5E5"> D can I use</font><font color="#CCCCCC"> that to give me a</font>

2575
01:32:56,960 --> 01:32:58,460
<font color="#CCCCCC">gradient</font><font color="#E5E5E5"> and how you actually get this</font>

2576
01:32:58,460 --> 01:33:00,350
gradient<font color="#E5E5E5"> is by using the remote ization</font>

2577
01:33:00,350 --> 01:33:03,440
trick<font color="#E5E5E5"> okay so that brings me</font><font color="#CCCCCC"> to the end</font>

2578
01:33:03,440 --> 01:33:06,770
the summary of today was to<font color="#E5E5E5"> sort of give</font>

2579
01:33:06,770 --> 01:33:08,950
you<font color="#CCCCCC"> some of the tools to manipulate</font>

2580
01:33:08,950 --> 01:33:11,330
probabilities and<font color="#E5E5E5"> then use those tricks</font>

2581
01:33:11,330 --> 01:33:13,610
and tools<font color="#E5E5E5"> to manipulate probabilities to</font>

2582
01:33:13,610 --> 01:33:15,680
then<font color="#E5E5E5"> build all types of generative</font>

2583
01:33:15,680 --> 01:33:17,360
models that you could imagine<font color="#CCCCCC"> right and</font>

2584
01:33:17,360 --> 01:33:18,380
we looked at different<font color="#CCCCCC"> types</font><font color="#E5E5E5"> of</font>

2585
01:33:18,380 --> 01:33:19,940
<font color="#E5E5E5">generative models which were</font>

2586
01:33:19,940 --> 01:33:22,070
undirected models fully observed models

2587
01:33:22,070 --> 01:33:24,170
models with latent variables<font color="#CCCCCC"> I asked you</font>

2588
01:33:24,170 --> 01:33:25,910
<font color="#CCCCCC">always to think</font><font color="#E5E5E5"> about machine learning</font>

2589
01:33:25,910 --> 01:33:28,699
<font color="#E5E5E5">within this three three prong approach</font>

2590
01:33:28,699 --> 01:33:31,100
<font color="#E5E5E5">of thinking of models the corresponding</font>

2591
01:33:31,100 --> 01:33:33,199
<font color="#E5E5E5">learning principles and the joint idea</font>

2592
01:33:33,199 --> 01:33:35,449
of algorithms<font color="#E5E5E5"> and the two examples we</font>

2593
01:33:35,449 --> 01:33:37,850
used of that we said was<font color="#CCCCCC"> bae was one</font>

2594
01:33:37,850 --> 01:33:40,160
<font color="#CCCCCC">example</font><font color="#E5E5E5"> of a model</font><font color="#CCCCCC"> which was a latent</font>

2595
01:33:40,160 --> 01:33:42,080
<font color="#CCCCCC">proscribed latent variable model with</font>

2596
01:33:42,080 --> 01:33:43,610
variational inference which then gave us

2597
01:33:43,610 --> 01:33:46,100
a<font color="#E5E5E5"> VA</font><font color="#CCCCCC"> e</font><font color="#E5E5E5"> algorithm and the other algorithm</font>

2598
01:33:46,100 --> 01:33:48,430
we<font color="#CCCCCC"> learnt about was</font><font color="#E5E5E5"> ganz which use</font>

2599
01:33:48,430 --> 01:33:50,840
implicit<font color="#E5E5E5"> generative model as its model</font>

2600
01:33:50,840 --> 01:33:52,670
<font color="#CCCCCC">it used this principle of</font><font color="#E5E5E5"> two sample</font>

2601
01:33:52,670 --> 01:33:54,170
testing as its principle of inference

2602
01:33:54,170 --> 01:33:55,969
<font color="#E5E5E5">and we put it together by building an</font>

2603
01:33:55,969 --> 01:33:57,920
algorithm that<font color="#E5E5E5"> used a</font><font color="#CCCCCC"> repro motorisation</font>

2604
01:33:57,920 --> 01:33:59,239
trick<font color="#E5E5E5"> to</font><font color="#CCCCCC"> actually do the optimization</font>

2605
01:33:59,239 --> 01:34:01,430
<font color="#CCCCCC">and then we spoke about lots of other</font>

2606
01:34:01,430 --> 01:34:03,830
things<font color="#E5E5E5"> about</font><font color="#CCCCCC"> stochastic optimization how</font>

2607
01:34:03,830 --> 01:34:06,260
we can<font color="#E5E5E5"> manipulate those integrals to do</font>

2608
01:34:06,260 --> 01:34:08,090
a<font color="#E5E5E5"> lot more scalable inference we spoke</font>

2609
01:34:08,090 --> 01:34:10,430
about amortized inference<font color="#E5E5E5"> and how</font>

2610
01:34:10,430 --> 01:34:12,530
<font color="#E5E5E5">encoders typically</font><font color="#CCCCCC"> ampere machine</font>

2611
01:34:12,530 --> 01:34:14,239
learning<font color="#E5E5E5"> and then how we can</font><font color="#CCCCCC"> build lots</font>

2612
01:34:14,239 --> 01:34:17,120
of different complex distributions over

2613
01:34:17,120 --> 01:34:19,940
<font color="#E5E5E5">data there's lots more to do</font><font color="#CCCCCC"> almost</font>

2614
01:34:19,940 --> 01:34:22,190
every<font color="#E5E5E5"> one of these represents a</font>

2615
01:34:22,190 --> 01:34:24,350
different<font color="#E5E5E5"> topic for research whether it</font>

2616
01:34:24,350 --> 01:34:26,600
is the last function<font color="#E5E5E5"> itself whether it</font>

2617
01:34:26,600 --> 01:34:28,280
<font color="#E5E5E5">is how you build these</font><font color="#CCCCCC"> Q distributions</font>

2618
01:34:28,280 --> 01:34:30,140
<font color="#E5E5E5">other ways of computing gradient</font>

2619
01:34:30,140 --> 01:34:31,940
estimators understanding their<font color="#CCCCCC"> variance</font>

2620
01:34:31,940 --> 01:34:33,770
properties<font color="#E5E5E5"> what are other ways of doing</font>

2621
01:34:33,770 --> 01:34:36,980
non maximum likelihood learning<font color="#CCCCCC"> you know</font>

2622
01:34:36,980 --> 01:34:41,060
all of<font color="#E5E5E5"> these</font><font color="#CCCCCC"> represent</font><font color="#E5E5E5"> a place of more</font>

2623
01:34:41,060 --> 01:34:42,530
research which needs<font color="#E5E5E5"> to</font><font color="#CCCCCC"> be done</font><font color="#E5E5E5"> and I</font>

2624
01:34:42,530 --> 01:34:44,690
hope lots<font color="#E5E5E5"> of you all will</font><font color="#CCCCCC"> join us in</font>

2625
01:34:44,690 --> 01:34:45,949
<font color="#E5E5E5">that effort so thank you for your</font>

2626
01:34:45,949 --> 01:34:48,219
<font color="#E5E5E5">attention</font>

2627
01:34:51,050 --> 00:00:00,000
<font color="#E5E5E5">you</font>

