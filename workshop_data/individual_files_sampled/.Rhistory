scale_x_datetime(name = ''
, date_breaks = '6 months'
, date_labels = "%b %y"
, limits = c(as.POSIXct('2002-01-01'), as.POSIXct('2015-02-01'))) +
geom_line(data = fit_discrete_models[fit_discrete_models$Model == 'Observed',]
, aes(Time, Values
, color = factor(Model))
, size = 0.8) +
geom_line(data = fit_discrete_models[fit_discrete_models$Model == 'Fixed BPM',]
, aes(Time, Values
, color = factor(Model))
, size = 1.5) +
scale_color_manual(values = c("blue", "darkgray")) +
geom_segment(data = shades
, aes(x = bp
, y = bp_lim1
, xend = bp
, yend = bp_lim2)
, lty = 4) +
theme(legend.direction="horizontal"
, panel.border = element_blank()
, legend.title=element_blank()
, legend.position=c(.8, .2)
, legend.text=element_text(size=10)
, axis.text.x = element_text(angle = 45, hjust = 1)
, panel.grid.major = element_line(colour = "gray95")
)
p2
shades = data.frame(x1=c(as.POSIXct('2004-02-01') , as.POSIXct('2011-04-01'))
, x2=c(as.POSIXct('2004-06-01'), as.POSIXct('2011-07-01'))
, bp = c(as.POSIXct('2004-03-01'), as.POSIXct('2011-05-01'))
, bp_lim1 = rep(2, 2)
, bp_lim2 = rep(5, 2)
)
p2 = ggplot() +
geom_rect(data=shades
, aes(xmin = x1
, xmax = x2
, ymin = bp_lim1
, ymax = bp_lim2)
, alpha = 0.4
, fill = 'yellow') +
scale_y_continuous(name = 'Abusive language score\n'
, limits = c(2, 8)
, breaks=seq(2, 8, 0.05)) +
scale_x_datetime(name = ''
, date_breaks = '6 months'
, date_labels = "%b %y"
, limits = c(as.POSIXct('2002-01-01'), as.POSIXct('2015-02-01'))) +
geom_line(data = fit_discrete_models[fit_discrete_models$Model == 'Observed',]
, aes(Time, Values
, color = factor(Model))
, size = 0.8) +
geom_line(data = fit_discrete_models[fit_discrete_models$Model == 'Fixed BPM',]
, aes(Time, Values
, color = factor(Model))
, size = 1.5) +
scale_color_manual(values = c("blue", "darkgray")) +
geom_segment(data = shades
, aes(x = bp
, y = bp_lim1
, xend = bp
, yend = bp_lim2)
, lty = 4) +
theme(legend.direction="horizontal"
, panel.border = element_blank()
, legend.title=element_blank()
, legend.position=c(.8, .2)
, legend.text=element_text(size=10)
, axis.text.x = element_text(angle = 45, hjust = 1)
, panel.grid.major = element_line(colour = "gray95")
)
p2
bp_confint_bps2
shades = data.frame(x1=c(as.POSIXct('2003-12-01') , as.POSIXct('2005-11-01'), as.POSIXct('2009-04-01'), as.POSIXct('2011-01-01'))
, x2=c(as.POSIXct('2004-07-01'), as.POSIXct('2006-03-01'), as.POSIXct('2009-08-01'), as.POSIXct('2011-07-01'))
, bp = c(as.POSIXct('2004-01-01'), as.POSIXct('2006-01-01'), as.POSIXct('2009-06-01'), as.POSIXct('2011-06-01'))
, bp_lim1 = rep(2, 4)
, bp_lim2 = rep(5, 4)
)
p3 = ggplot() +
geom_rect(data=shades
, aes(xmin = x1
, xmax = x2
, ymin = bp_lim1
, ymax = bp_lim2)
, alpha = 0.4
, fill = 'yellow') +
scale_y_continuous(name = 'Abusive language score\n'
#, limits = c(0.36, .4)
, limits = c(2, 8)
, breaks=seq(2, 8, 0.05)) +
scale_x_datetime(name = '\n Time'
, date_breaks = '6 months'
, date_labels = "%b %y"
, limits = c(as.POSIXct('2002-01-01'), as.POSIXct('2015-02-01'))) +
geom_line(data = fit_discrete_models[fit_discrete_models$Model == 'Observed',]
, aes(Time, Values
, color = factor(Model))
, size = 0.8) +
geom_line(data = fit_discrete_models[fit_discrete_models$Model == 'Variable BPM',]
, aes(Time, Values
, color = factor(Model))
, size = 1.5) +
scale_color_manual(values = c("darkgray", "red")) +
geom_segment(data = shades
, aes(x = bp
, y = bp_lim1
, xend = bp
, yend = bp_lim2)
, lty = 4) +
theme(legend.direction="horizontal"
, panel.border = element_blank()
, legend.title=element_blank()
, legend.position=c(.8, .2)
, legend.text=element_text(size=10)
, axis.text.x = element_text(angle = 45, hjust = 1)
, panel.grid.major = element_line(colour = "gray95")
)
p3
## abusiveness per user
seed = 42
dt.abusive_bin = ltta.final[stormfront_user != "[]", sum(abusive_normal_bin), by = 'stormfront_user'][order(-V1)]
dt.abusive_bin[, prop_N := V1/sum(V1)][order(-prop_N)]
dt.abusive_bin[, cumprop_N := cumsum(V1/sum(V1))]
dt.abusive_bin[, prop_user := 1/.N]
dt.abusive_bin[, cumprop_user := cumsum(prop_user)]
plot(dt.abusive_bin$cumprop_user, dt.abusive_bin$cumprop_N)
seed = 42
set.seed(seed)
boot_n = 1000
boot = replicate(boot_n, Gini(sample(dt.abusive_bin$V1
, length(dt.abusive_bin$V1)
, replace=T), unbiased = T))
summary(boot)
mean(boot)
quantile(boot, probs = c(0.005, 0.995))
dt.abusive_bin[round(cumprop_user, 2) == .1, ]
dt.abusive_bin
dt.abusive_bin[round(cumprop_user, 2) == .1, V1]
bin_threshold = 8
dt.user_freq = ltta.final[month_date > '2001-12-01', .N, by = 'stormfront_user'][order(N)]
dt.user_freq
frequent_users = dt.user_freq[N >= 8, stormfront_user]
frequent_users
dt.user_freq
frequent_users = droplevels(frequent_users)
frequent_users
#PROLIFIC USERS
dt.abusive_bin[round(cumprop_N, 2) == .1, V1]
dt.abusive_bin[round(cumprop_N, 2) == .1, ]
dt.abusive_bin[round(cumprop_user, 2) == .9, ]
dt.abusive_bin[round(cumprop_user, 2) == .1, ]
prolific_users = dt.abusive_bin[round(cumprop_user, 2) == .1, stormfront_user]
prolific_users
prolific_users = droplevels(prolific_users)
prolific_users
ltta.user_level = ltta.final[month_date > '2001-12-01' & stormfront_user %in% prolific_users, ]
ltta.user_level
prolific_users
prolific_users
ltta.user_level[, .N, by='stormfront_user']
ltta.user_level[, .N, by='stormfront_user'][order(-N)]
ltta.example = ltta.user_level[stromfront_user == '85475', ]
ltta.example = ltta.user_level[stormfront_user == '85475', ]
ltta.example
ltta.example = ltta.user_level[stormfront_user == '85475', ][order(month_date)]
ltta.example
ts.user_example = ts(data = ltta.example$abusive_normal_cont
, start = c(2005, 12), frequency = 12)
plot(ts.user_example)
ts.user_example = zoo(ltta.example$abusive_normal_cont, ltta.example$month_date)
plot(ts.user_example)
ltta.user_level[, .N, by='stormfront_user'][order(-N)]
# exclude users with low freqs
dt.user_freq = ltta.final[month_date > '2001-12-01', .N, by = 'stormfront_user'][order(N)]
#statistical tests
##frequency
##time active
##average offensiveness
load('/Users/bennettkleinberg/Dropbox/research/ucl/abusive_language/stormfront_data/sf_kmeans_streak_aggr_8_meta_15112018.RData')
summary(aov(df.meta_kmeans$freq ~ df.meta_kmeans$cluster))
summary(aov(df.meta_kmeans$max_gap ~ df.meta_kmeans$cluster))
df.meta_kmeans
df.meta_kmeans = df.meta_kmeans[df.meta_kmeans$max_gap != Inf & df.meta_kmeans$max_gap != -Inf, ]
summary(aov(df.meta_kmeans$max_gap ~ df.meta_kmeans$cluster))
install.packages("cleanNLP")
df.meta_kmeans = df.meta_kmeans[df.meta_kmeans$max_gap != Inf & df.meta_kmeans$max_gap != -Inf, ]
tapply(df.meta_kmeans$avg_off, df.meta_kmeans$cluster, mean)
tapply(df.meta_kmeans$view_count_corrected, df.meta_kmeans$cluster, sd)
tapply(df.meta_kmeans$view_count_corrected, df.meta_kmeans$cluster, means)
tapply(df.meta_kmeans$view_count_corrected, df.meta_kmeans$cluster, mean)
tapply(df.meta_kmeans$max_gap, df.meta_kmeans$cluster, mean)
require(RCurl)
##############################################################################
##### EUROCSS 2018
##### LTTA workshop
##### dataset preprocessing
##### author: B Kleinberg https://github.com/ben-aaron188
###############################################################################
# PREPARATION
## clear ws
rm(list = ls())
## load deps
require(tm)
require(data.table)
require(RCurl)
r_source_path = getURL('https://github.com/ben-aaron188/r_helper_functions/blob/master/txt_df_from_dir.R', ssl.verifypeer = FALSE)
eval(parse(text = r_source_path))
##############################################################################
##### EUROCSS 2018
##### LTTA workshop
##### dataset preprocessing
##### author: B Kleinberg https://github.com/ben-aaron188
###############################################################################
# PREPARATION
## clear ws
rm(list = ls())
## load deps
require(tm)
require(data.table)
## set dir
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
## load function from GitHub repo
source('./r_deps/r_helper_functions/txt_df_from_dir.R')
## load function from GitHub repo
source('r_deps/r_helper_functions/txt_df_from_dir.R')
## load function from GitHub repo
source('./r_deps/txt_df_from_dir.R')
### left
setwd('./output_dir/left/parsed/')
#LOAD DATA
## read raw data
t1 = Sys.time()
df.full_corpus = txt_df_from_dir(dirpath = './'
, recursive = T
, to_lower = F
, include_processed = F)
t2 = Sys.time()
t2-t1
## set variable for merge with meta data
df.full_corpus$vlog_id = sub('(.*)\\..*','\\1', df.full_corpus$file_id)
df.full_corpus$channel_vlog_id = tolower(paste(df.full_corpus$file_parent, df.full_corpus$vlog_id, sep="_"))
df.full_corpus = df.full_corpus[nchar(df.full_corpus$text) > 0, ]
df.full_corpus = droplevels(df.full_corpus)
nrow(df.full_corpus)
df.meta = as.data.frame(fread('../overview_left.txt'
, sep = ","
, header=F))
names(df.meta) = c('channel_id'
, 'file_id'
, 'url'
, 'view_count'
, 'date_posted'
, 'landing_url'
, 'upvotes'
, 'downvotes')
df.meta$channel_id = sub('(.*)\\..*','\\1', df.meta$channel_id)
df.meta$channel_vlog_id = tolower(paste(df.meta$channel_id, df.meta$file_id, sep="_"))
df.meta$view_count = as.numeric(gsub("[.]", "", as.character(df.meta$view_count)))
df.meta$upvotes = as.numeric(gsub("[.]", "", as.character(df.meta$upvotes)))
df.meta$downvotes = as.numeric(gsub("[.]", "", as.character(df.meta$downvotes)))
df.meta$date_posted = as.Date(df.meta$date_posted)
reference_date = as.Date('2018-10-30')
df.meta$days_until_reference = as.numeric(reference_date - df.meta$date_posted)
df.meta$view_count_corrected = round(df.meta$view_count/df.meta$days_until_reference, 2)
df.meta = na.omit(df.meta)
## merge
df.data_left = merge(df.full_corpus, df.meta, by='channel_vlog_id', all.x = T)
df.data_left = na.omit(df.data_left)
df.meta = as.data.frame(fread('./overview_left.txt'
, sep = ","
, header=F))
df.meta = as.data.frame(fread('../overview_left.txt'
, sep = ","
, header=F))
df.meta = as.data.frame(fread('../../overview_left.txt'
, sep = ","
, header=F))
df.meta = as.data.frame(fread('../../../overview_left.txt'
, sep = ","
, header=F))
names(df.meta) = c('channel_id'
, 'file_id'
, 'url'
, 'view_count'
, 'date_posted'
, 'landing_url'
, 'upvotes'
, 'downvotes')
df.meta$channel_id = sub('(.*)\\..*','\\1', df.meta$channel_id)
df.meta$channel_vlog_id = tolower(paste(df.meta$channel_id, df.meta$file_id, sep="_"))
df.meta$view_count = as.numeric(gsub("[.]", "", as.character(df.meta$view_count)))
df.meta$upvotes = as.numeric(gsub("[.]", "", as.character(df.meta$upvotes)))
df.meta$downvotes = as.numeric(gsub("[.]", "", as.character(df.meta$downvotes)))
df.meta$date_posted = as.Date(df.meta$date_posted)
reference_date = as.Date('2018-10-30')
df.meta$days_until_reference = as.numeric(reference_date - df.meta$date_posted)
df.meta$view_count_corrected = round(df.meta$view_count/df.meta$days_until_reference, 2)
df.meta = na.omit(df.meta)
## merge
df.data_left = merge(df.full_corpus, df.meta, by='channel_vlog_id', all.x = T)
df.data_left = na.omit(df.data_left)
### right
setwd('../../right/parsed/')
t1 = Sys.time()
df.full_corpus = txt_df_from_dir(dirpath = './'
, recursive = T
, to_lower = F
, include_processed = F)
t2 = Sys.time()
t2-t1
## set variable for merge with meta data
df.full_corpus$vlog_id = sub('(.*)\\..*','\\1', df.full_corpus$file_id)
df.full_corpus$channel_vlog_id = tolower(paste(df.full_corpus$file_parent, df.full_corpus$vlog_id, sep="_"))
df.full_corpus = df.full_corpus[nchar(df.full_corpus$text) > 0, ]
df.full_corpus = droplevels(df.full_corpus)
nrow(df.full_corpus)
df.meta = as.data.frame(fread('../../../overview_right.txt'
, sep = ","
, header=F))
names(df.meta) = c('channel_id'
, 'file_id'
, 'url'
, 'view_count'
, 'date_posted'
, 'landing_url'
, 'upvotes'
, 'downvotes')
df.meta$channel_id = sub('(.*)\\..*','\\1', df.meta$channel_id)
df.meta$channel_vlog_id = tolower(paste(df.meta$channel_id, df.meta$file_id, sep="_"))
df.meta$view_count = as.numeric(gsub("[.]", "", as.character(df.meta$view_count)))
df.meta$upvotes = as.numeric(gsub("[.]", "", as.character(df.meta$upvotes)))
df.meta$downvotes = as.numeric(gsub("[.]", "", as.character(df.meta$downvotes)))
df.meta$date_posted = as.Date(df.meta$date_posted)
reference_date = as.Date('2018-10-30')
df.meta$days_until_reference = as.numeric(reference_date - df.meta$date_posted)
df.meta$view_count_corrected = round(df.meta$view_count/df.meta$days_until_reference, 2)
df.meta = na.omit(df.meta)
## merge
df.data_right = merge(df.full_corpus, df.meta, by='channel_vlog_id', all.x = T)
df.data_right = na.omit(df.data_right)
### CLEAN TRANSCRIPTS
df.data = rbind(df.data_left, df.data_right)
dt.data = setDT(df.data)
### sampling
dt.data
df.data_left$pol = 'l'
df.data_right$pol = 'r'
### CLEAN TRANSCRIPTS
df.data = rbind(df.data_left, df.data_right)
dt.data = setDT(df.data)
### sampling
dt.data
table(dt.data$pol)
library(splitstackshape)
install.packages("splitstackshape")
require(splitstackshape)
table(dt.data$channel_vlog_id, dt.data$pol)
table(dt.data$file_parent, dt.data$pol)
### only channels with at least 2000 videos
table(dt.data$file_parent, dt.data$pol)
dt.data[, .N, by=file_parent]
dt.data[, .N, by=file_parent][N > 2000]
dt.data[, .N, by=file_parent][N > 2000, file_parent]
### only channels with at least 2000 videos
selected_channels = dt.data[, .N, by=file_parent][N > 2000, file_parent]
dt.sub = dt.data[file_parent %in% selected_channels, ]
dt.sub
table(dt.sub$pol)
dt.sampled = stratified(dt.sub, c("file_parent", "pol"), 2000)
dt.sampled
table(dt.sub$pol)
table(dt.sampled$pol)
table(dt.sampled$pol, dt.sampled$file_parent)
table(dt.sampled$file_parent, dt.sampled$pol)
set.seed(123)
dt.sampled = stratified(dt.sub, c("file_parent", "pol"), 2000)
table(dt.sampled$file_parent, dt.sampled$pol)
unique(dt.sampled$file_parent[dt.sampled$pol == 'l'])
set.seed(123)
dt.sampled = stratified(dt.sub, c("file_parent", "pol"), 2000)
selected_channels_left = sample(x = unique(dt.sampled$file_parent[dt.sampled$pol == 'l'])
, size = 4
, replace = F
)
selected_channels_left
set.seed(123)
dt.sampled = stratified(dt.sub, c("file_parent", "pol"), 2000)
selected_channels_left = sample(x = unique(dt.sampled$file_parent[dt.sampled$pol == 'l'])
, size = 4
, replace = F
)
selected_channels_left
final_channels = c(unique(dt.sampled$file_parent[dt.sampled$pol == 'r'])
, selected_channels_left)
final_channels
dt.sampled_balanced = dt.sampled[file_parent %in% final_channels, ]
dt.sampled_balanced
table(dt.sampled_balanced$file_parent, dt.sampled_balanced$pol)
###fix channel names
levels(dt.sampled_balanced$file_parent)
###fix channel names
dt.sampled_balanced$channel_name = as.factor(dt.sampled_balanced$file_parent)
levels(dt.sampled_balanced$channel_name)
levels(dt.sampled_balanced$channel_name)[7:8]
###fix channel names
dt.sampled[file_parent == 'UCGy6uV7yqGWDeUWTZzT3ZE', ]
###fix channel names
dt.sampled[file_parent == 'UCGy6uV7yqGWDeUWTZzT3ZEg', ]
levels(dt.sampled_balanced$channel_name)[7:8]
###fix channel names
dt.sampled[file_parent == 'UCaeO5vkdj5xOQHp4UmIN6dw', ]
levels(dt.sampled_balanced$channel_name)[7:8] = c('thedailywire', 'rebelmedia')
levels(dt.sampled_balanced$channel_name)
table(dt.sampled_balanced$channel_name)
save(dt.sampled_balanced
, file='../../eurocss_ltta_workshop_data_sampled.RData')
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
load('eurocss_ltta_workshop_data_sampled.RData')
#set a new ID variable to merge LIWC attributes in later step
dt.sampled_balanced$new_id_runner = 1:nrow(dt.sampled_balanced)
dt.sampled_balanced
setwd('../individual_files_sampled')
setwd('./individual_files_sampled')
setwd('./individual_files_sampled')
#load sampled balanced data
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
setwd('./individual_files_sampled')
lapply(seq(nrow(dt.sampled_balanced)), function(i){
write.table(dt.sampled_balanced$text[i],
file = paste('./individual_files_sampled/'
, dt.sampled_balanced$new_id_runner[i]
, '.txt'
, sep=''),
col.names = FALSE,
row.names = FALSE,
append=F,
sep = '\t',
quote = FALSE)
})
#load sampled balanced data
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
#this chunk writes the text column to a .txt file with the file name set to its ID
?root.dir
setwd('./individual_files_sampled/')
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "")
getwd()
#load sampled balanced data
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
#this chunk writes the text column to a .txt file with the file name set to its ID
setwd('./individual_files_sampled/')
#this chunk writes the text column to a .txt file with the file name set to its ID
knitr::opts_knit$set(root.dir = "./individual_files_sampled/")
lapply(seq(nrow(dt.sampled_balanced)), function(i){
write.table(dt.sampled_balanced$text[i],
file = paste(dt.sampled_balanced$new_id_runner[i]
, '.txt'
, sep=''),
col.names = FALSE,
row.names = FALSE,
append=F,
sep = '\t',
quote = FALSE)
})
#load sampled balanced data
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
```{r message=FALSE, warning=FALSE}
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
load('eurocss_ltta_workshop_data_sampled.RData')
#set a new ID variable to merge LIWC attributes in later step
dt.sampled_balanced$new_id_runner = 1:nrow(dt.sampled_balanced)
#load sampled balanced data
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
#load sampled balanced data
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
#load sampled balanced data
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
load('eurocss_ltta_workshop_data_sampled.RData')
#load sampled balanced data
load('eurocss_ltta_workshop_data_sampled.RData')
## load data
setwd('/Users/bennettkleinberg/GitHub/ltta_workshop_data')
load('eurocss_ltta_workshop_data_sampled.RData')
setwd('individual_files_sampled')
lapply(seq(nrow(dt.sampled_balanced)), function(i){
write.table(dt.sampled_balanced$text[i],
file = paste(dt.sampled_balanced$new_id_runner[i]
, '.txt'
, sep=''),
col.names = FALSE,
row.names = FALSE,
append=F,
sep = '\t',
quote = FALSE)
})
getwd()
## write files
setwd('individual_files_sampled')
lapply(seq(nrow(dt.sampled_balanced)), function(i){
write.table(dt.sampled_balanced$text[i],
file = paste(dt.sampled_balanced$new_id_runner[i]
, '.txt'
, sep=''),
col.names = FALSE,
row.names = FALSE,
append=F,
sep = '\t',
quote = FALSE)
})
getwd()
